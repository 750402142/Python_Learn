{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66d45c3",
   "metadata": {},
   "source": [
    "# 模块7-机器学习基础-基础篇-04-从线性回归到神经网络之逻辑回归篇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2abc5b",
   "metadata": {},
   "source": [
    "![pic](./images/image-20241118120420276.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfdfdea",
   "metadata": {},
   "source": [
    "![pic](./images/image-20241118120436226.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f66fa",
   "metadata": {},
   "source": [
    "![pic](./images/image-20241118120450508.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86726210",
   "metadata": {},
   "source": [
    "## 名为回归的分类算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0bb0ee6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:21.638888Z",
     "start_time": "2024-11-19T06:35:20.723737Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa2946a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:21.656801Z",
     "start_time": "2024-11-19T06:35:21.642449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    165\n",
       "0    138\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/heart.csv')\n",
    "data['target'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38b792c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:22.016536Z",
     "start_time": "2024-11-19T06:35:22.012629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da743257",
   "metadata": {},
   "source": [
    "机器学习的分类过程,也就是确定某一个事物,隶属于某一个类别的可能性大小的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d87ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:03:03.438617Z",
     "start_time": "2024-11-19T01:03:03.428007Z"
    }
   },
   "source": [
    "1. 找到sigmoid函数: $$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "2. 找到线性回归模型店中间值: $$z(x)=w_1 x_1+w_2 x_2+\\ldots+w_{n-1} x_{n-1}+w_n x_n+b=W^TX$$\n",
    "\n",
    "3. 将1和2组合起来,将函数表示为假设函数形式:\n",
    "$$g(x) = \\frac{1}{1+e^{-(w_1 x_1+w_2 x_2+\\ldots+w_{n-1} x_{n-1}+w_n x_n+b)}}$$\n",
    "\n",
    "\n",
    "最终得到逻辑回归的假设函数\n",
    "$$h(x) = \\frac{1}{1+e^{-(W^TX)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6083e81d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:22.639493Z",
     "start_time": "2024-11-19T06:35:22.636614Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    y_hat = 1 / (1 + np.exp(-z))\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b2e59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:21:36.719224Z",
     "start_time": "2024-11-19T01:21:36.708711Z"
    }
   },
   "source": [
    "逻辑回归的损失函数\n",
    "\n",
    "$$L(w,b) = \\frac{1}{N} \\sum_{(x,y) \\in D} Loss(h(x), y ) = \\frac{1}{N} Loss(y^`,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224ba6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:25:02.853043Z",
     "start_time": "2024-11-19T01:25:02.848517Z"
    }
   },
   "source": [
    "$$\n",
    "y=\\left\\{\\begin{array}{l}\n",
    "1, \\operatorname{Loss}(h(x), y)=-\\log (h(x)) \\\\\n",
    "0, \\operatorname{Loss}(h(x), y)=-\\log (1-h(x))\n",
    "\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3b7a0",
   "metadata": {},
   "source": [
    "$$L(w,b) = \\frac{1}{N} \\sum_{(x,y) \\in D} [y*log(h(x)) + (1-y) \\times log(1-h(x))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088ac6aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:23.418755Z",
     "start_time": "2024-11-19T06:35:23.414816Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fuction(X,y,w,b):\n",
    "    y_hat = sigmoid(np.dot(X,w) +b) # 假设函数\n",
    "    loss = - (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    cost = np.sum(loss) / len(X)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a7f9f",
   "metadata": {},
   "source": [
    "### 逻辑回归的梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43054515",
   "metadata": {},
   "source": [
    "$$\\text{梯度} = h^{\\prime}(x)=\\frac{\\partial}{\\partial w} L(w, b)=\\frac{\\partial}{\\partial w}\\left\\{-\\frac{1}{N} \\sum_{(x, y) \\in D}[y * \\log (h(x))+(1-y) * \\log (1-h(x))]\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T02:02:21.012317Z",
     "start_time": "2024-11-19T02:02:21.007004Z"
    }
   },
   "source": [
    "$$\\text{梯度} = \\frac{1}{N} \\sum_{i=1}^N\\left(y^{(i)}-h\\left(x^{(i)}\\right)\\right) \\cdot x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f404b",
   "metadata": {},
   "source": [
    "$$\\frac{1}{N} \\sum_{i=1}^N\\left(y^{(i)}-h\\left(x^{(i)}\\right)\\right) \\cdot x^{(i)}$$\n",
    "\n",
    "\n",
    "$$w=w-\\frac{\\alpha}{N} \\sum_{j=1}^N\\left(y^{(i)}-\\left(w \\cdot x^{(i)}\\right)\\right) \\cdot x^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60817ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:24.513235Z",
     "start_time": "2024-11-19T06:35:24.507177Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,w,b,lr, iters): # 定义逻辑回归函数的梯度下降算法\n",
    "    l_history = np.zeros(iters) # 记录损失\n",
    "    w_history = np.zeros(iters) # 记录权重\n",
    "    b_history = np.zeros(iters) # 记录偏置\n",
    "    for i in range(iters):  # 迭代计算\n",
    "        y_hat = sigmoid(np.dot(X,w) + b) # 使用sigmoid函数+线性函数(wx + b)计算得到y`\n",
    "        deriivative_w = np.dot(X.T, (y_hat -y)) / X.shape[0]  # 给权重进行求导\n",
    "        deriivative_b = np.sum(y_hat - y) / X.shape[0]  # 给偏置进行求导\n",
    "        w = w - lr * deriivative_w  # 更新权重向量,lr是学习速率\n",
    "        b = b - lr * deriivative_b  # 更新偏置,\n",
    "        l_history[i] = loss_fuction(X, y, w, b) # 梯度下降过程中的损失,\n",
    "        print(f'当前是第{i+1}轮, 当前训练集损失为: {l_history[i]}')\n",
    "        w_history[i] = w  # 梯度下降过程中的权重历史,\n",
    "        b_history[i] = b  # 梯度下降算法中的偏置历史\n",
    "    return l_history,w_history, b_history  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd68f53",
   "metadata": {},
   "source": [
    "##  通过逻辑回归解决二元分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5398e3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:25.015661Z",
     "start_time": "2024-11-19T06:35:25.007543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:20:11) [MSC v.1938 64 bit (AMD64)]\n",
      "Pandas version: 2.2.2\n",
      "Numpy version: 1.26.4\n",
      "Matplotlib version: 3.8.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "print('Python version:', sys.version)\n",
    "print('Pandas version:', pd.__version__)\n",
    "print('Numpy version:', np.__version__)\n",
    "print('Matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e1d180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:25.398282Z",
     "start_time": "2024-11-19T06:35:25.384963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/heart.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fffdec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:30.043669Z",
     "start_time": "2024-11-19T06:35:25.668972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac155f05cda8487883cecefd5b4b2cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                             |                                                                …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report SWEETVIZ_REPORT.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "import sweetviz as sv\n",
    "report = sv.analyze(data)\n",
    "report.show_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "333256a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:30.050137Z",
     "start_time": "2024-11-19T06:35:30.046693Z"
    }
   },
   "outputs": [],
   "source": [
    "#  需要使用指定版本的python来进行实现\n",
    "# from dataprep.eda import create_report\n",
    "# import pandas as pd # 导入Pandas数据处理工具箱\n",
    "# data = pd.read_csv('./data/heart.csv')\n",
    "# report = create_report(data)\n",
    "# report.save('report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbfa037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:35.222945Z",
     "start_time": "2024-11-19T06:35:35.216923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    165\n",
       "0    138\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个步骤是必须的,如果某个类别的比例特别第 ,那这样的数据通过逻辑回归做分类是不合适的.\n",
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c46a3fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:35.665745Z",
     "start_time": "2024-11-19T06:35:35.662691Z"
    }
   },
   "outputs": [],
   "source": [
    "# 年龄/最大心率,是否患病之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4182b37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:36.047342Z",
     "start_time": "2024-11-19T06:35:36.030306Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = 'Times New Roman'\n",
    "import matplotx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5ab987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:35:36.748661Z",
     "start_time": "2024-11-19T06:35:36.629236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with plt.style.context(matplotx.styles.pitaya_smoothie['light']):\n",
    "    plt.figure(figsize=(10,5), dpi=100)\n",
    "    plt.scatter(x = data[data['target'] == 1]['age'],c='red',marker='o',\n",
    "                s=40,linewidths=.4,edgecolors='black',\n",
    "               y = data[data['target'] == 1]['thalach'],label='Disease',\n",
    "               )\n",
    "    plt.scatter(x = data[data['target'] == 0]['age'],marker='s',\n",
    "                s=40,linewidths=.5,edgecolors='black',\n",
    "               y = data[data['target'] == 0]['thalach'],label='No Disease' ,\n",
    "               )\n",
    "    plt.legend()\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Heart Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9920f9b",
   "metadata": {},
   "source": [
    "## 构建特征集和标签集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "011bd4c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:45:40.306861Z",
     "start_time": "2024-11-19T08:45:40.301137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征集形状: (303, 13)\n",
      "标签集形状: (303, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "y = data['target'].values\n",
    "y = y.reshape(-1,1)\n",
    "print(f'特征集形状: {X.shape}')\n",
    "print(f'标签集形状: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2732ec2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:45:41.984897Z",
     "start_time": "2024-11-19T08:45:41.977397Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bca0fcd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T07:35:38.517465Z",
     "start_time": "2024-11-19T07:35:38.514759Z"
    }
   },
   "outputs": [],
   "source": [
    "# report = sv.compare([pd.concat([X_train,y_train],axis=1),'训练集'],\n",
    "#                     [pd.concat([X_test,y_test],axis=1),'验证集'])\n",
    "# report.show_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026bfec",
   "metadata": {},
   "source": [
    "### 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "29a6a1b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:45:47.022634Z",
     "start_time": "2024-11-19T08:45:47.017260Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a70ba0",
   "metadata": {},
   "source": [
    "## 建立逻辑回归模型\n",
    "\n",
    "### 逻辑函数的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cd360716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:11:31.902050Z",
     "start_time": "2024-11-19T08:11:31.899009Z"
    }
   },
   "outputs": [],
   "source": [
    "# 首先定义一个sigmoid函数,输入Z,返回y`, y_hat\n",
    "def sigmoid(z):\n",
    "    y_hat = 1 / (1 + np.exp(-z))\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daeb405",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6e34267a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:14:23.649794Z",
     "start_time": "2024-11-19T08:14:23.644777Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(X,y,w,b):\n",
    "    y_hat = sigmoid(np.dot(X,w) +b) # 假设函数, sigmoid逻辑函数+线性函数(wX + b)得到的\n",
    "    loss =  - (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) # 计算损失\n",
    "    cost = np.sum(loss) / X.shape[0]  # 整个数据集的平局损失\n",
    "    return cost   # 返回平均损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bddfe",
   "metadata": {},
   "source": [
    "### 梯度下降的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e8b2e422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:14:25.924889Z",
     "start_time": "2024-11-19T08:14:25.918197Z"
    }
   },
   "outputs": [],
   "source": [
    "# 下面是逻辑回归的损失函数的 Python 实现 ：\n",
    "def gradient_descent(X, y, w, b, lr, iters) : # 定义逻辑回归梯度下降函数\n",
    "    l_history = np.zeros(iters) # 初始化记录梯度下降过程中误差值 ( 损失 ) 的数组\n",
    "    w_history = np.zeros((iters, w.shape[0], w.shape[1])) # 初始化记录梯度下降过程中权重的数组\n",
    "    b_history = np.zeros(iters) # 初始化记录梯度下降过程中偏置的数组 \n",
    "    for i in range(iters): # 进行机器训练的迭代\n",
    "        y_hat = sigmoid(np.dot(X, w) + b) #Sigmoid 逻辑函数 + 线性函数 (wX+b) 得到 y'\n",
    "        loss = -(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) # 计算损失\n",
    "        derivative_w = np.dot(X.T, ((y_hat-y)))/X.shape[0] # 给权重向量求导\n",
    "        derivative_b = np.sum(y_hat-y)/X.shape[0] # 给偏置求导\n",
    "        w = w - lr * derivative_w # 更新权重向量 , lr 即学习速率 alpha\n",
    "        b = b - lr * derivative_b # 更新偏置 , lr 即学习速率 alpha\n",
    "        l_history[i] = loss_function(X, y, w, b) # 梯度下降过程中的损失\n",
    "        print (\" 轮次 \", i+1 , \" 当前轮训练集损失 ：\", l_history[i]) \n",
    "        w_history[i] = w # 梯度下降过程中权重的历史记录，请注意 w_history 和 w 的形状\n",
    "        b_history[i] = b # 梯度下降过程中偏置的历史记录\n",
    "    return l_history, w_history, b_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b8bf4",
   "metadata": {},
   "source": [
    "### 预测函数的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bef62960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:14:28.095868Z",
     "start_time": "2024-11-19T08:14:28.091740Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, w, b): # 定义预测函数\n",
    "    z = np.dot(X, w) + b # 这是线性函数\n",
    "    y_hat = sigmoid(z) # 逻辑转换函数\n",
    "    y_pred = np.zeros((y_hat.shape[0],1))\n",
    "    for i in range(y_hat.shape[0]):\n",
    "        if y_hat[i,0] < 0.5:\n",
    "            y_pred[i,0] = 0  # 预测概率小于0.5,输出分类是0\n",
    "        else:\n",
    "            y_pred[i,0] = 1 # 预测概率大于0.5. 输出分类是1\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc12451",
   "metadata": {},
   "source": [
    "### 开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4e8b3755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:14:29.630164Z",
     "start_time": "2024-11-19T08:14:29.624125Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X,y,w,b,lr,iters):\n",
    "    l_history,w_history, b_history = gradient_descent(X,y,w,b,lr,iters) # 梯度下降求损失,权重,偏置\n",
    "    print(f'训练最终损失: {l_history[-1]}')  # d打印最终损失\n",
    "    y_pred = predict(X, w_history[-1],b_history[-1]) # 进行预测\n",
    "    trainning_acc = 100 - np.mean(np.abs(y_pred - y_train)) * 100 # 计算准确率\n",
    "    print(f'逻辑回归训练的准确率为: {trainning_acc: .2f}%')\n",
    "    return l_history, w_history, b_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e55ea",
   "metadata": {},
   "source": [
    "### 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f9743d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:44:06.482310Z",
     "start_time": "2024-11-19T08:44:06.479641Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "dimension = X.shape[1]\n",
    "weight = np.full((dimension,1),0.1)\n",
    "bias = 0\n",
    "# 初始化超参数\n",
    "alpha = 0.5 # 学习速率\n",
    "iterations = 1000   # 迭代次数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f141929",
   "metadata": {},
   "source": [
    "### 调用逻辑回归训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9da32043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:44:07.133196Z",
     "start_time": "2024-11-19T08:44:07.070308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 轮次  1  当前轮训练集损失 ： 0.6854176219094903\n",
      " 轮次  2  当前轮训练集损失 ： 0.6692134316680488\n",
      " 轮次  3  当前轮训练集损失 ： 0.6557837317504402\n",
      " 轮次  4  当前轮训练集损失 ： 0.6437244803630164\n",
      " 轮次  5  当前轮训练集损失 ： 0.6325832860506098\n",
      " 轮次  6  当前轮训练集损失 ： 0.6221924911818881\n",
      " 轮次  7  当前轮训练集损失 ： 0.612468713010015\n",
      " 轮次  8  当前轮训练集损失 ： 0.6033549049403493\n",
      " 轮次  9  当前轮训练集损失 ： 0.5948038179958415\n",
      " 轮次  10  当前轮训练集损失 ： 0.5867732303553008\n",
      " 轮次  11  当前轮训练集损失 ： 0.5792244891140443\n",
      " 轮次  12  当前轮训练集损失 ： 0.572121978803147\n",
      " 轮次  13  当前轮训练集损失 ： 0.5654328476858228\n",
      " 轮次  14  当前轮训练集损失 ： 0.5591268058524832\n",
      " 轮次  15  当前轮训练集损失 ： 0.5531759448189439\n",
      " 轮次  16  当前轮训练集损失 ： 0.5475545662318351\n",
      " 轮次  17  当前轮训练集损失 ： 0.5422390175875262\n",
      " 轮次  18  当前轮训练集损失 ： 0.5372075353870139\n",
      " 轮次  19  当前轮训练集损失 ： 0.5324400965071601\n",
      " 轮次  20  当前轮训练集损失 ： 0.527918278376447\n",
      " 轮次  21  当前轮训练集损失 ： 0.5236251282578686\n",
      " 轮次  22  当前轮训练集损失 ： 0.5195450416861217\n",
      " 轮次  23  当前轮训练集损失 ： 0.5156636499070419\n",
      " 轮次  24  当前轮训练集损失 ： 0.5119677160225001\n",
      " 轮次  25  当前轮训练集损失 ： 0.5084450394450776\n",
      " 轮次  26  当前轮训练集损失 ： 0.5050843682042426\n",
      " 轮次  27  当前轮训练集损失 ： 0.5018753186110445\n",
      " 轮次  28  当前轮训练集损失 ： 0.4988083017745016\n",
      " 轮次  29  当前轮训练集损失 ： 0.49587445646426564\n",
      " 轮次  30  当前轮训练集损失 ： 0.4930655878263644\n",
      " 轮次  31  当前轮训练集损失 ： 0.4903741114784384\n",
      " 轮次  32  当前轮训练集损失 ： 0.4877930025352807\n",
      " 轮次  33  当前轮训练集损失 ： 0.4853157491426883\n",
      " 轮次  34  当前轮训练集损失 ： 0.48293631012618765\n",
      " 轮次  35  当前轮训练集损失 ： 0.48064907639004456\n",
      " 轮次  36  当前轮训练集损失 ： 0.4784488357303504\n",
      " 轮次  37  当前轮训练集损失 ： 0.47633074075339005\n",
      " 轮次  38  当前轮训练集损失 ： 0.47429027961658304\n",
      " 轮次  39  当前轮训练集损失 ： 0.4723232493338572\n",
      " 轮次  40  当前轮训练集损失 ： 0.47042573141024774\n",
      " 轮次  41  当前轮训练集损失 ： 0.46859406959177746\n",
      " 轮次  42  当前轮训练集损失 ： 0.4668248495362835\n",
      " 轮次  43  当前轮训练集损失 ： 0.46511488022885666\n",
      " 轮次  44  当前轮训练集损失 ： 0.4634611769820251\n",
      " 轮次  45  当前轮训练集损失 ： 0.46186094587583254\n",
      " 轮次  46  当前轮训练集损失 ： 0.46031156950663016\n",
      " 轮次  47  当前轮训练集损失 ： 0.4588105939258028\n",
      " 轮次  48  当前轮训练集损失 ： 0.45735571666090996\n",
      " 轮次  49  当前轮训练集损失 ： 0.45594477572190406\n",
      " 轮次  50  当前轮训练集损失 ： 0.45457573950430974\n",
      " 轮次  51  当前轮训练集损失 ： 0.45324669750957697\n",
      " 轮次  52  当前轮训练集损失 ： 0.45195585181035175\n",
      " 轮次  53  当前轮训练集损失 ： 0.4507015091952012\n",
      " 轮次  54  当前轮训练集损失 ： 0.44948207393347234\n",
      " 轮次  55  当前轮训练集损失 ： 0.4482960411065018\n",
      " 轮次  56  当前轮训练集损失 ： 0.4471419904563937\n",
      " 轮次  57  当前轮训练集损失 ： 0.4460185807081019\n",
      " 轮次  58  当前轮训练集损失 ： 0.44492454432462586\n",
      " 轮次  59  当前轮训练集损失 ： 0.44385868265881073\n",
      " 轮次  60  当前轮训练集损失 ： 0.44281986146857244\n",
      " 轮次  61  当前轮训练集损失 ： 0.44180700676536605\n",
      " 轮次  62  当前轮训练集损失 ： 0.4408191009684382\n",
      " 轮次  63  当前轮训练集损失 ： 0.43985517933985946\n",
      " 轮次  64  当前轮训练集损失 ： 0.43891432667755215\n",
      " 轮次  65  当前轮训练集损失 ： 0.43799567424554536\n",
      " 轮次  66  当前轮训练集损失 ： 0.4370983969225066\n",
      " 轮次  67  当前轮训练集损失 ： 0.4362217105512543\n",
      " 轮次  68  当前轮训练集损失 ： 0.4353648694734492\n",
      " 轮次  69  当前轮训练集损失 ： 0.4345271642350214\n",
      " 轮次  70  当前轮训练集损失 ： 0.4337079194491232\n",
      " 轮次  71  当前轮训练集损失 ： 0.43290649180451457\n",
      " 轮次  72  当前轮训练集损失 ： 0.43212226820830935\n",
      " 轮次  73  当前轮训练集损失 ： 0.4313546640529298\n",
      " 轮次  74  当前轮训练集损失 ： 0.4306031215979633\n",
      " 轮次  75  当前轮训练集损失 ： 0.42986710845837856\n",
      " 轮次  76  当前轮训练集损失 ： 0.429146116191258\n",
      " 轮次  77  当前轮训练集损失 ： 0.42843965897383696\n",
      " 轮次  78  当前轮训练集损失 ： 0.4277472723662247\n",
      " 轮次  79  当前轮训练集损失 ： 0.4270685121527076\n",
      " 轮次  80  当前轮训练集损失 ： 0.4264029532560209\n",
      " 轮次  81  当前轮训练集损失 ： 0.42575018871941916\n",
      " 轮次  82  当前轮训练集损失 ： 0.42510982875177433\n",
      " 轮次  83  当前轮训练集损失 ： 0.4244814998313083\n",
      " 轮次  84  当前轮训练集损失 ： 0.42386484386389717\n",
      " 轮次  85  当前轮训练集损失 ： 0.4232595173922023\n",
      " 轮次  86  当前轮训练集损失 ： 0.4226651908521618\n",
      " 轮次  87  当前轮训练集损失 ： 0.4220815478736416\n",
      " 轮次  88  当前轮训练集损失 ： 0.42150828462228057\n",
      " 轮次  89  当前轮训练集损失 ： 0.42094510917978994\n",
      " 轮次  90  当前轮训练集损失 ： 0.42039174096016196\n",
      " 轮次  91  当前轮训练集损失 ： 0.419847910159437\n",
      " 轮次  92  当前轮训练集损失 ： 0.41931335723684193\n",
      " 轮次  93  当前轮训练集损失 ： 0.41878783242527484\n",
      " 轮次  94  当前轮训练集损失 ： 0.41827109526925577\n",
      " 轮次  95  当前轮训练集损失 ： 0.417762914188594\n",
      " 轮次  96  当前轮训练集损失 ： 0.41726306606614966\n",
      " 轮次  97  当前轮训练集损失 ： 0.41677133585817816\n",
      " 轮次  98  当前轮训练集损失 ： 0.41628751622585336\n",
      " 轮次  99  当前轮训练集损失 ： 0.41581140718666104\n",
      " 轮次  100  当前轮训练集损失 ： 0.41534281578444376\n",
      " 轮次  101  当前轮训练集损失 ： 0.41488155577696345\n",
      " 轮次  102  当前轮训练集损失 ： 0.4144274473399218\n",
      " 轮次  103  当前轮训练集损失 ： 0.4139803167864525\n",
      " 轮次  104  当前轮训练集损失 ： 0.4135399963011624\n",
      " 轮次  105  当前轮训练集损失 ： 0.41310632368786343\n",
      " 轮次  106  当前轮训练集损失 ： 0.41267914213018947\n",
      " 轮次  107  当前轮训练集损失 ： 0.41225829996434865\n",
      " 轮次  108  当前轮训练集损失 ： 0.41184365046330745\n",
      " 轮次  109  当前轮训练集损失 ： 0.41143505163175104\n",
      " 轮次  110  当前轮训练集损失 ： 0.41103236601120363\n",
      " 轮次  111  当前轮训练集损失 ： 0.4106354604947334\n",
      " 轮次  112  当前轮训练集损失 ： 0.41024420615070306\n",
      " 轮次  113  当前轮训练集损失 ： 0.4098584780550601\n",
      " 轮次  114  当前轮训练集损失 ： 0.40947815513169217\n",
      " 轮次  115  当前轮训练集损失 ： 0.40910312000040494\n",
      " 轮次  116  当前轮训练集损失 ： 0.4087332588321023\n",
      " 轮次  117  当前轮训练集损失 ： 0.4083684612107797\n",
      " 轮次  118  当前轮训练集损失 ： 0.4080086200019612\n",
      " 轮次  119  当前轮训练集损失 ： 0.40765363122723597\n",
      " 轮次  120  当前轮训练集损失 ： 0.40730339394456677\n",
      " 轮次  121  当前轮训练集损失 ： 0.40695781013406834\n",
      " 轮次  122  当前轮训练集损失 ： 0.4066167845889642\n",
      " 轮次  123  当前轮训练集损失 ： 0.4062802248114555\n",
      " 轮次  124  当前轮训练集损失 ： 0.4059480409132431\n",
      " 轮次  125  当前轮训练集损失 ： 0.40562014552046655\n",
      " 轮次  126  当前轮训练集损失 ： 0.4052964536828307\n",
      " 轮次  127  当前轮训练集损失 ： 0.40497688278670985\n",
      " 轮次  128  当前轮训练集损失 ： 0.40466135247202584\n",
      " 轮次  129  当前轮训练集损失 ： 0.40434978455271214\n",
      " 轮次  130  当前轮训练集损失 ： 0.404042102940585\n",
      " 轮次  131  当前轮训练集损失 ： 0.4037382335724512\n",
      " 轮次  132  当前轮训练集损失 ： 0.403438104340296\n",
      " 轮次  133  当前轮训练集损失 ： 0.403141645024397\n",
      " 轮次  134  当前轮训练集损失 ： 0.4028487872292247\n",
      " 轮次  135  当前轮训练集损失 ： 0.402559464321993\n",
      " 轮次  136  当前轮训练集损失 ： 0.40227361137373374\n",
      " 轮次  137  当前轮训练集损失 ： 0.40199116510277444\n",
      " 轮次  138  当前轮训练集损失 ： 0.4017120638205047\n",
      " 轮次  139  当前轮训练集损失 ： 0.4014362473793245\n",
      " 轮次  140  当前轮训练集损失 ： 0.4011636571226721\n",
      " 轮次  141  当前轮训练集损失 ： 0.4008942358370337\n",
      " 轮次  142  当前轮训练集损失 ： 0.40062792770584416\n",
      " 轮次  143  当前轮训练集损失 ： 0.40036467826519245\n",
      " 轮次  144  当前轮训练集损失 ： 0.40010443436124793\n",
      " 轮次  145  当前轮训练集损失 ： 0.3998471441093305\n",
      " 轮次  146  当前轮训练集损失 ： 0.3995927568545502\n",
      " 轮次  147  当前轮训练集损失 ： 0.39934122313394554\n",
      " 轮次  148  当前轮训练集损失 ： 0.39909249464005436\n",
      " 轮次  149  当前轮训练集损失 ： 0.3988465241858535\n",
      " 轮次  150  当前轮训练集损失 ： 0.3986032656710069\n",
      " 轮次  151  当前轮训练集损失 ： 0.39836267404936526\n",
      " 轮次  152  当前轮训练集损失 ： 0.3981247052976623\n",
      " 轮次  153  当前轮训练集损失 ： 0.3978893163853566\n",
      " 轮次  154  当前轮训练集损失 ： 0.39765646524556947\n",
      " 轮次  155  当前轮训练集损失 ： 0.3974261107470712\n",
      " 轮次  156  当前轮训练集损失 ： 0.3971982126672733\n",
      " 轮次  157  当前轮训练集损失 ： 0.39697273166618235\n",
      " 轮次  158  当前轮训练集损失 ： 0.3967496292612752\n",
      " 轮次  159  当前轮训练集损失 ： 0.3965288678032588\n",
      " 轮次  160  当前轮训练集损失 ： 0.3963104104526762\n",
      " 轮次  161  当前轮训练集损失 ： 0.39609422115732473\n",
      " 轮次  162  当前轮训练集损失 ： 0.3958802646304529\n",
      " 轮次  163  当前轮训练集损失 ： 0.395668506329704\n",
      " 轮次  164  当前轮训练集损失 ： 0.3954589124367778\n",
      " 轮次  165  当前轮训练集损失 ： 0.39525144983777777\n",
      " 轮次  166  当前轮训练集损失 ： 0.3950460861042215\n",
      " 轮次  167  当前轮训练集损失 ： 0.3948427894746832\n",
      " 轮次  168  当前轮训练集损失 ： 0.39464152883704634\n",
      " 轮次  169  当前轮训练集损失 ： 0.3944422737113413\n",
      " 轮次  170  当前轮训练集损失 ： 0.3942449942331448\n",
      " 轮次  171  当前轮训练集损失 ： 0.39404966113752066\n",
      " 轮次  172  当前轮训练集损失 ： 0.39385624574347944\n",
      " 轮次  173  当前轮训练集损失 ： 0.39366471993893754\n",
      " 轮次  174  当前轮训练集损失 ： 0.3934750561661577\n",
      " 轮次  175  当前轮训练集损失 ： 0.3932872274076508\n",
      " 轮次  176  当前轮训练集损失 ： 0.3931012071725238\n",
      " 轮次  177  当前轮训练集损失 ： 0.3929169694832538\n",
      " 轮次  178  当前轮训练集损失 ： 0.3927344888628765\n",
      " 轮次  179  当前轮训练集损失 ： 0.3925537403225698\n",
      " 轮次  180  当前轮训练集损失 ： 0.39237469934962105\n",
      " 轮次  181  当前轮训练集损失 ： 0.3921973418957612\n",
      " 轮次  182  当前轮训练集损失 ： 0.3920216443658546\n",
      " 轮次  183  当前轮训练集损失 ： 0.3918475836069305\n",
      " 轮次  184  当前轮训练集损失 ： 0.3916751368975437\n",
      " 轮次  185  当前轮训练集损失 ： 0.3915042819374538\n",
      " 轮次  186  当前轮训练集损失 ： 0.3913349968376104\n",
      " 轮次  187  当前轮训练集损失 ： 0.3911672601104334\n",
      " 轮次  188  当前轮训练集损失 ： 0.39100105066037955\n",
      " 轮次  189  当前轮训练集损失 ： 0.3908363477747829\n",
      " 轮次  190  当前轮训练集损失 ： 0.3906731311149617\n",
      " 轮次  191  当前轮训练集损失 ： 0.39051138070758135\n",
      " 轮次  192  当前轮训练集损失 ： 0.39035107693626486\n",
      " 轮次  193  当前轮训练集损失 ： 0.3901922005334419\n",
      " 轮次  194  当前轮训练集损失 ： 0.3900347325724295\n",
      " 轮次  195  当前轮训练集损失 ： 0.38987865445973496\n",
      " 轮次  196  当前轮训练集损失 ： 0.3897239479275753\n",
      " 轮次  197  当前轮训练集损失 ： 0.38957059502660324\n",
      " 轮次  198  当前轮训练集损失 ： 0.3894185781188367\n",
      " 轮次  199  当前轮训练集损失 ： 0.3892678798707809\n",
      " 轮次  200  当前轮训练集损失 ： 0.3891184832467401\n",
      " 轮次  201  当前轮训练集损失 ： 0.3889703715023106\n",
      " 轮次  202  当前轮训练集损失 ： 0.3888235281780504\n",
      " 轮次  203  当前轮训练集损失 ： 0.3886779370933184\n",
      " 轮次  204  当前轮训练集损失 ： 0.38853358234027924\n",
      " 轮次  205  当前轮训练集损失 ： 0.3883904482780677\n",
      " 轮次  206  当前轮训练集损失 ： 0.38824851952710676\n",
      " 轮次  207  当前轮训练集损失 ： 0.388107780963576\n",
      " 轮次  208  当前轮训练集损失 ： 0.38796821771402495\n",
      " 轮次  209  当前轮训练集损失 ： 0.38782981515012577\n",
      " 轮次  210  当前轮训练集损失 ： 0.38769255888356285\n",
      " 轮次  211  当前轮训练集损失 ： 0.3875564347610536\n",
      " 轮次  212  当前轮训练集损失 ： 0.38742142885949643\n",
      " 轮次  213  当前轮训练集损失 ： 0.38728752748124345\n",
      " 轮次  214  当前轮训练集损失 ： 0.3871547171494922\n",
      " 轮次  215  当前轮训练集损失 ： 0.3870229846037933\n",
      " 轮次  216  当前轮训练集损失 ： 0.3868923167956721\n",
      " 轮次  217  当前轮训练集损失 ： 0.3867627008843584\n",
      " 轮次  218  当前轮训练集损失 ： 0.38663412423262244\n",
      " 轮次  219  当前轮训练集损失 ： 0.38650657440271435\n",
      " 轮次  220  当前轮训练集损失 ： 0.38638003915240265\n",
      " 轮次  221  当前轮训练集损失 ： 0.38625450643111003\n",
      " 轮次  222  当前轮训练集损失 ： 0.3861299643761429\n",
      " 轮次  223  当前轮训练集损失 ： 0.3860064013090126\n",
      " 轮次  224  当前轮训练集损失 ： 0.38588380573184444\n",
      " 轮次  225  当前轮训练集损失 ： 0.38576216632387383\n",
      " 轮次  226  当前轮训练集损失 ： 0.3856414719380247\n",
      " 轮次  227  当前轮训练集损失 ： 0.3855217115975708\n",
      " 轮次  228  当前轮训练集损失 ： 0.3854028744928734\n",
      " 轮次  229  当前轮训练集损失 ： 0.38528494997819857\n",
      " 轮次  230  当前轮训练集损失 ： 0.3851679275686059\n",
      " 轮次  231  当前轮训练集损失 ： 0.3850517969369125\n",
      " 轮次  232  当前轮训练集损失 ： 0.3849365479107246\n",
      " 轮次  233  当前轮训练集损失 ： 0.3848221704695396\n",
      " 轮次  234  当前轮训练集损失 ： 0.3847086547419135\n",
      " 轮次  235  当前轮训练集损失 ： 0.38459599100269337\n",
      " 轮次  236  当前轮训练集损失 ： 0.3844841696703125\n",
      " 轮次  237  当前轮训练集损失 ： 0.3843731813041473\n",
      " 轮次  238  当前轮训练集损失 ： 0.3842630166019325\n",
      " 轮次  239  当前轮训练集损失 ： 0.38415366639723586\n",
      " 轮次  240  当前轮训练集损失 ： 0.3840451216569877\n",
      " 轮次  241  当前轮训练集损失 ： 0.38393737347906615\n",
      " 轮次  242  当前轮训练集损失 ： 0.38383041308993576\n",
      " 轮次  243  当前轮训练集损失 ： 0.38372423184233756\n",
      " 轮次  244  当前轮训练集损失 ： 0.3836188212130302\n",
      " 轮次  245  当前轮训练集损失 ： 0.38351417280058014\n",
      " 轮次  246  当前轮训练集损失 ： 0.3834102783231998\n",
      " 轮次  247  当前轮训练集损失 ： 0.3833071296166324\n",
      " 轮次  248  当前轮训练集损失 ： 0.3832047186320825\n",
      " 轮次  249  当前轮训练集损失 ： 0.3831030374341909\n",
      " 轮次  250  当前轮训练集损失 ： 0.38300207819905235\n",
      " 轮次  251  当前轮训练集损失 ： 0.38290183321227567\n",
      " 轮次  252  当前轮训练集损失 ： 0.38280229486708484\n",
      " 轮次  253  当前轮训练集损失 ： 0.3827034556624606\n",
      " 轮次  254  当前轮训练集损失 ： 0.3826053082013197\n",
      " 轮次  255  当前轮训练集损失 ： 0.38250784518873354\n",
      " 轮次  256  当前轮训练集损失 ： 0.38241105943018294\n",
      " 轮次  257  当前轮训练集损失 ： 0.38231494382985\n",
      " 轮次  258  当前轮训练集损失 ： 0.38221949138894407\n",
      " 轮次  259  当前轮训练集损失 ： 0.3821246952040628\n",
      " 轮次  260  当前轮训练集损失 ： 0.3820305484655866\n",
      " 轮次  261  当前轮训练集损失 ： 0.38193704445610605\n",
      " 轮次  262  当前轮训练集损失 ： 0.3818441765488803\n",
      " 轮次  263  当前轮训练集损失 ： 0.38175193820632775\n",
      " 轮次  264  当前轮训练集损失 ： 0.3816603229785466\n",
      " 轮次  265  当前轮训练集损失 ： 0.38156932450186504\n",
      " 轮次  266  当前轮训练集损失 ： 0.3814789364974203\n",
      " 轮次  267  当前轮训练集损失 ： 0.38138915276976637\n",
      " 轮次  268  当前轮训练集损失 ： 0.38129996720550885\n",
      " 轮次  269  当前轮训练集损失 ： 0.381211373771967\n",
      " 轮次  270  当前轮训练集损失 ： 0.3811233665158619\n",
      " 轮次  271  当前轮训练集损失 ： 0.3810359395620312\n",
      " 轮次  272  当前轮训练集损失 ： 0.380949087112167\n",
      " 轮次  273  当前轮训练集损失 ： 0.3808628034435804\n",
      " 轮次  274  当前轮训练集损失 ： 0.38077708290798895\n",
      " 轮次  275  当前轮训练集损失 ： 0.38069191993032697\n",
      " 轮次  276  当前轮训练集损失 ： 0.38060730900757983\n",
      " 轮次  277  当前轮训练集损失 ： 0.38052324470764043\n",
      " 轮次  278  当前轮训练集损失 ： 0.38043972166818646\n",
      " 轮次  279  当前轮训练集损失 ： 0.38035673459558017\n",
      " 轮次  280  当前轮训练集损失 ： 0.3802742782637882\n",
      " 轮次  281  当前轮训练集损失 ： 0.38019234751332226\n",
      " 轮次  282  当前轮训练集损失 ： 0.3801109372502\n",
      " 轮次  283  当前轮训练集损失 ： 0.38003004244492417\n",
      " 轮次  284  当前轮训练集损失 ： 0.3799496581314822\n",
      " 轮次  285  当前轮训练集损失 ： 0.3798697794063634\n",
      " 轮次  286  当前轮训练集损失 ： 0.3797904014275953\n",
      " 轮次  287  当前轮训练集损失 ： 0.3797115194137969\n",
      " 轮次  288  当前轮训练集损失 ： 0.3796331286432498\n",
      " 轮次  289  当前轮训练集损失 ： 0.37955522445298656\n",
      " 轮次  290  当前轮训练集损失 ： 0.37947780223789523\n",
      " 轮次  291  当前轮训练集损失 ： 0.37940085744984053\n",
      " 轮次  292  当前轮训练集损失 ： 0.3793243855968012\n",
      " 轮次  293  当前轮训练集损失 ： 0.37924838224202223\n",
      " 轮次  294  当前轮训练集损失 ： 0.37917284300318344\n",
      " 轮次  295  当前轮训练集损失 ： 0.3790977635515818\n",
      " 轮次  296  当前轮训练集损失 ： 0.37902313961132966\n",
      " 轮次  297  当前轮训练集损失 ： 0.37894896695856606\n",
      " 轮次  298  当前轮训练集损失 ： 0.37887524142068335\n",
      " 轮次  299  当前轮训练集损失 ： 0.37880195887556645\n",
      " 轮次  300  当前轮训练集损失 ： 0.3787291152508461\n",
      " 轮次  301  当前轮训练集损失 ： 0.3786567065231654\n",
      " 轮次  302  当前轮训练集损失 ： 0.37858472871745863\n",
      " 轮次  303  当前轮训练集损失 ： 0.37851317790624356\n",
      " 轮次  304  当前轮训练集损失 ： 0.37844205020892546\n",
      " 轮次  305  当前轮训练集损失 ： 0.37837134179111337\n",
      " 轮次  306  当前轮训练集损失 ： 0.37830104886394833\n",
      " 轮次  307  当前轮训练集损失 ： 0.3782311676834435\n",
      " 轮次  308  当前轮训练集损失 ： 0.37816169454983506\n",
      " 轮次  309  当前轮训练集损失 ： 0.37809262580694447\n",
      " 轮次  310  当前轮训练集损失 ： 0.37802395784155235\n",
      " 轮次  311  当前轮训练集损失 ： 0.3779556870827819\n",
      " 轮次  312  当前轮训练集损失 ： 0.3778878100014937\n",
      " 轮次  313  当前轮训练集损失 ： 0.37782032310969066\n",
      " 轮次  314  当前轮训练集损失 ： 0.3777532229599325\n",
      " 轮次  315  当前轮训练集损失 ： 0.37768650614476096\n",
      " 轮次  316  当前轮训练集损失 ： 0.3776201692961336\n",
      " 轮次  317  当前轮训练集损失 ： 0.3775542090848688\n",
      " 轮次  318  当前轮训练集损失 ： 0.37748862222009777\n",
      " 轮次  319  当前轮训练集损失 ： 0.3774234054487281\n",
      " 轮次  320  当前轮训练集损失 ： 0.37735855555491404\n",
      " 轮次  321  当前轮训练集损失 ： 0.37729406935953747\n",
      " 轮次  322  当前轮训练集损失 ： 0.37722994371969587\n",
      " 轮次  323  当前轮训练集损失 ： 0.3771661755282002\n",
      " 轮次  324  当前轮训练集损失 ： 0.3771027617130793\n",
      " 轮次  325  当前轮训练集损失 ： 0.3770396992370948\n",
      " 轮次  326  当前轮训练集损失 ： 0.37697698509726113\n",
      " 轮次  327  当前轮训练集损失 ： 0.3769146163243758\n",
      " 轮次  328  当前轮训练集损失 ： 0.3768525899825559\n",
      " 轮次  329  当前轮训练集损失 ： 0.3767909031687826\n",
      " 轮次  330  当前轮训练集损失 ： 0.37672955301245326\n",
      " 轮次  331  当前轮训练集损失 ： 0.37666853667494\n",
      " 轮次  332  当前轮训练集损失 ： 0.37660785134915625\n",
      " 轮次  333  当前轮训练集损失 ： 0.3765474942591294\n",
      " 轮次  334  当前轮训练集损失 ： 0.3764874626595814\n",
      " 轮次  335  当前轮训练集损失 ： 0.37642775383551463\n",
      " 轮次  336  当前轮训练集损失 ： 0.37636836510180594\n",
      " 轮次  337  当前轮训练集损失 ： 0.3763092938028053\n",
      " 轮次  338  当前轮训练集损失 ： 0.37625053731194275\n",
      " 轮次  339  当前轮训练集损失 ： 0.37619209303134027\n",
      " 轮次  340  当前轮训练集损失 ： 0.37613395839142977\n",
      " 轮次  341  当前轮训练集损失 ： 0.376076130850578\n",
      " 轮次  342  当前轮训练集损失 ： 0.37601860789471625\n",
      " 轮次  343  当前轮训练集损失 ： 0.3759613870369768\n",
      " 轮次  344  当前轮训练集损失 ： 0.37590446581733405\n",
      " 轮次  345  当前轮训练集损失 ： 0.3758478418022524\n",
      " 轮次  346  当前轮训练集损失 ： 0.37579151258433807\n",
      " 轮次  347  当前轮训练集损失 ： 0.37573547578199806\n",
      " 轮次  348  当前轮训练集损失 ： 0.3756797290391029\n",
      " 轮次  349  当前轮训练集损失 ： 0.3756242700246554\n",
      " 轮次  350  当前轮训练集损失 ： 0.3755690964324641\n",
      " 轮次  351  当前轮训练集损失 ： 0.3755142059808219\n",
      " 轮次  352  当前轮训练集损失 ： 0.37545959641219\n",
      " 轮次  353  当前轮训练集损失 ： 0.3754052654928852\n",
      " 轮次  354  当前轮训练集损失 ： 0.375351211012774\n",
      " 轮次  355  当前轮训练集损失 ： 0.37529743078497\n",
      " 轮次  356  当前轮训练集损失 ： 0.37524392264553563\n",
      " 轮次  357  当前轮训练集损失 ： 0.37519068445318965\n",
      " 轮次  358  当前轮训练集损失 ： 0.37513771408901797\n",
      " 轮次  359  当前轮训练集损失 ： 0.3750850094561891\n",
      " 轮次  360  当前轮训练集损失 ： 0.3750325684796742\n",
      " 轮次  361  当前轮训练集损失 ： 0.3749803891059707\n",
      " 轮次  362  当前轮训练集损失 ： 0.3749284693028305\n",
      " 轮次  363  当前轮训练集损失 ： 0.37487680705899235\n",
      " 轮次  364  当前轮训练集损失 ： 0.3748254003839174\n",
      " 轮次  365  当前轮训练集损失 ： 0.3747742473075294\n",
      " 轮次  366  当前轮训练集损失 ： 0.3747233458799588\n",
      " 轮次  367  当前轮训练集损失 ： 0.37467269417128996\n",
      " 轮次  368  当前轮训练集损失 ： 0.37462229027131266\n",
      " 轮次  369  当前轮训练集损失 ： 0.3745721322892767\n",
      " 轮次  370  当前轮训练集损失 ： 0.3745222183536512\n",
      " 轮次  371  当前轮训练集损失 ： 0.37447254661188556\n",
      " 轮次  372  当前轮训练集损失 ： 0.3744231152301759\n",
      " 轮次  373  当前轮训练集损失 ： 0.37437392239323336\n",
      " 轮次  374  当前轮训练集损失 ： 0.37432496630405665\n",
      " 轮次  375  当前轮训练集损失 ： 0.37427624518370767\n",
      " 轮次  376  当前轮训练集损失 ： 0.37422775727108987\n",
      " 轮次  377  当前轮训练集损失 ： 0.3741795008227309\n",
      " 轮次  378  当前轮训练集损失 ： 0.3741314741125667\n",
      " 轮次  379  当前轮训练集损失 ： 0.374083675431731\n",
      " 轮次  380  当前轮训练集损失 ： 0.37403610308834506\n",
      " 轮次  381  当前轮训练集损失 ： 0.37398875540731297\n",
      " 轮次  382  当前轮训练集损失 ： 0.3739416307301183\n",
      " 轮次  383  当前轮训练集损失 ： 0.373894727414624\n",
      " 轮次  384  当前轮训练集损失 ： 0.37384804383487547\n",
      " 轮次  385  当前轮训练集损失 ： 0.3738015783809058\n",
      " 轮次  386  当前轮训练集损失 ： 0.3737553294585444\n",
      " 轮次  387  当前轮训练集损失 ： 0.37370929548922804\n",
      " 轮次  388  当前轮训练集损失 ： 0.37366347490981455\n",
      " 轮次  389  当前轮训练集损失 ： 0.37361786617239867\n",
      " 轮次  390  当前轮训练集损失 ： 0.37357246774413144\n",
      " 轮次  391  当前轮训练集损失 ： 0.3735272781070415\n",
      " 轮次  392  当前轮训练集损失 ： 0.3734822957578587\n",
      " 轮次  393  当前轮训练集损失 ： 0.37343751920784063\n",
      " 轮次  394  当前轮训练集损失 ： 0.3733929469826013\n",
      " 轮次  395  当前轮训练集损失 ： 0.37334857762194223\n",
      " 轮次  396  当前轮训练集损失 ： 0.37330440967968603\n",
      " 轮次  397  当前轮训练集损失 ： 0.3732604417235116\n",
      " 轮次  398  当前轮训练集损失 ： 0.37321667233479283\n",
      " 轮次  399  当前轮训练集损失 ： 0.3731731001084381\n",
      " 轮次  400  当前轮训练集损失 ： 0.3731297236527334\n",
      " 轮次  401  当前轮训练集损失 ： 0.3730865415891861\n",
      " 轮次  402  当前轮训练集损失 ： 0.3730435525523718\n",
      " 轮次  403  当前轮训练集损失 ： 0.3730007551897838\n",
      " 轮次  404  当前轮训练集损失 ： 0.3729581481616829\n",
      " 轮次  405  当前轮训练集损失 ： 0.3729157301409506\n",
      " 轮次  406  当前轮训练集损失 ： 0.37287349981294393\n",
      " 轮次  407  当前轮训练集损失 ： 0.3728314558753524\n",
      " 轮次  408  当前轮训练集损失 ： 0.37278959703805603\n",
      " 轮次  409  当前轮训练集损失 ： 0.3727479220229868\n",
      " 轮次  410  当前轮训练集损失 ： 0.37270642956399025\n",
      " 轮次  411  当前轮训练集损失 ： 0.3726651184066905\n",
      " 轮次  412  当前轮训练集损失 ： 0.37262398730835594\n",
      " 轮次  413  当前轮训练集损失 ： 0.37258303503776685\n",
      " 轮次  414  当前轮训练集损失 ： 0.3725422603750859\n",
      " 轮次  415  当前轮训练集损失 ： 0.3725016621117285\n",
      " 轮次  416  当前轮训练集损失 ： 0.3724612390502367\n",
      " 轮次  417  当前轮训练集损失 ： 0.3724209900041533\n",
      " 轮次  418  当前轮训练集损失 ： 0.3723809137978988\n",
      " 轮次  419  当前轮训练集损失 ： 0.37234100926664904\n",
      " 轮次  420  当前轮训练集损失 ： 0.37230127525621487\n",
      " 轮次  421  当前轮训练集损失 ： 0.3722617106229234\n",
      " 轮次  422  当前轮训练集损失 ： 0.3722223142335011\n",
      " 轮次  423  当前轮训练集损失 ： 0.3721830849649574\n",
      " 轮次  424  当前轮训练集损失 ： 0.372144021704471\n",
      " 轮次  425  当前轮训练集损失 ： 0.372105123349277\n",
      " 轮次  426  当前轮训练集损失 ： 0.37206638880655574\n",
      " 轮次  427  当前轮训练集损失 ： 0.37202781699332277\n",
      " 轮次  428  当前轮训练集损失 ： 0.3719894068363206\n",
      " 轮次  429  当前轮训练集损失 ： 0.37195115727191175\n",
      " 轮次  430  当前轮训练集损失 ： 0.371913067245973\n",
      " 轮次  431  当前轮训练集损失 ： 0.3718751357137914\n",
      " 轮次  432  当前轮训练集损失 ： 0.3718373616399608\n",
      " 轮次  433  当前轮训练集损失 ： 0.37179974399828086\n",
      " 轮次  434  当前轮训练集损失 ： 0.3717622817716563\n",
      " 轮次  435  当前轮训练集损失 ： 0.3717249739519982\n",
      " 轮次  436  当前轮训练集损失 ： 0.3716878195401261\n",
      " 轮次  437  当前轮训练集损失 ： 0.3716508175456713\n",
      " 轮次  438  当前轮训练集损失 ： 0.37161396698698185\n",
      " 轮次  439  当前轮训练集损失 ： 0.3715772668910284\n",
      " 轮次  440  当前轮训练集损失 ： 0.37154071629331087\n",
      " 轮次  441  当前轮训练集损失 ： 0.3715043142377675\n",
      " 轮次  442  当前轮训练集损失 ： 0.37146805977668373\n",
      " 轮次  443  当前轮训练集损失 ： 0.3714319519706028\n",
      " 轮次  444  当前轮训练集损失 ： 0.3713959898882376\n",
      " 轮次  445  当前轮训练集损失 ： 0.3713601726063832\n",
      " 轮次  446  当前轮训练集损失 ： 0.3713244992098312\n",
      " 轮次  447  当前轮训练集损失 ： 0.3712889687912842\n",
      " 轮次  448  当前轮训练集损失 ： 0.371253580451272\n",
      " 轮次  449  当前轮训练集损失 ： 0.37121833329806875\n",
      " 轮次  450  当前轮训练集损失 ： 0.3711832264476109\n",
      " 轮次  451  当前轮训练集损失 ： 0.3711482590234163\n",
      " 轮次  452  当前轮训练集损失 ： 0.3711134301565044\n",
      " 轮次  453  当前轮训练集损失 ： 0.37107873898531735\n",
      " 轮次  454  当前轮训练集损失 ： 0.3710441846556418\n",
      " 轮次  455  当前轮训练集损失 ： 0.37100976632053234\n",
      " 轮次  456  当前轮训练集损失 ： 0.37097548314023504\n",
      " 轮次  457  当前轮训练集损失 ： 0.37094133428211246\n",
      " 轮次  458  当前轮训练集损失 ： 0.3709073189205699\n",
      " 轮次  459  当前轮训练集损失 ： 0.3708734362369812\n",
      " 轮次  460  当前轮训练集损失 ： 0.3708396854196174\n",
      " 轮次  461  当前轮训练集损失 ： 0.3708060656635743\n",
      " 轮次  462  当前轮训练集损失 ： 0.37077257617070264\n",
      " 轮次  463  当前轮训练集损失 ： 0.3707392161495374\n",
      " 轮次  464  当前轮训练集损失 ： 0.3707059848152298\n",
      " 轮次  465  当前轮训练集损失 ： 0.37067288138947857\n",
      " 轮次  466  当前轮训练集损失 ： 0.3706399051004629\n",
      " 轮次  467  当前轮训练集损失 ： 0.3706070551827757\n",
      " 轮次  468  当前轮训练集损失 ： 0.37057433087735847\n",
      " 轮次  469  当前轮训练集损失 ： 0.3705417314314356\n",
      " 轮次  470  当前轮训练集损失 ： 0.37050925609845115\n",
      " 轮次  471  当前轮训练集损失 ： 0.37047690413800477\n",
      " 轮次  472  当前轮训练集损失 ： 0.3704446748157894\n",
      " 轮次  473  当前轮训练集损失 ： 0.3704125674035296\n",
      " 轮次  474  当前轮训练集损失 ： 0.37038058117892\n",
      " 轮次  475  当前轮训练集损失 ： 0.3703487154255653\n",
      " 轮次  476  当前轮训练集损失 ： 0.37031696943292036\n",
      " 轮次  477  当前轮训练集损失 ： 0.37028534249623146\n",
      " 轮次  478  当前轮训练集损失 ： 0.37025383391647765\n",
      " 轮次  479  当前轮训练集损失 ： 0.37022244300031354\n",
      " 轮次  480  当前轮训练集损失 ： 0.3701911690600122\n",
      " 轮次  481  当前轮训练集损失 ： 0.3701600114134089\n",
      " 轮次  482  当前轮训练集损失 ： 0.37012896938384576\n",
      " 轮次  483  当前轮训练集损失 ： 0.37009804230011656\n",
      " 轮次  484  当前轮训练集损失 ： 0.3700672294964124\n",
      " 轮次  485  当前轮训练集损失 ： 0.37003653031226835\n",
      " 轮次  486  当前轮训练集损失 ： 0.3700059440925103\n",
      " 轮次  487  当前轮训练集损失 ： 0.3699754701872023\n",
      " 轮次  488  当前轮训练集损失 ： 0.36994510795159496\n",
      " 轮次  489  当前轮训练集损失 ： 0.3699148567460744\n",
      " 轮次  490  当前轮训练集损失 ： 0.3698847159361114\n",
      " 轮次  491  当前轮训练集损失 ： 0.3698546848922113\n",
      " 轮次  492  当前轮训练集损失 ： 0.3698247629898649\n",
      " 轮次  493  当前轮训练集损失 ： 0.3697949496094992\n",
      " 轮次  494  当前轮训练集损失 ： 0.3697652441364292\n",
      " 轮次  495  当前轮训练集损失 ： 0.3697356459608102\n",
      " 轮次  496  当前轮训练集损失 ： 0.3697061544775907\n",
      " 轮次  497  当前轮训练集损失 ： 0.3696767690864654\n",
      " 轮次  498  当前轮训练集损失 ： 0.36964748919182916\n",
      " 轮次  499  当前轮训练集损失 ： 0.3696183142027316\n",
      " 轮次  500  当前轮训练集损失 ： 0.3695892435328317\n",
      " 轮次  501  当前轮训练集损失 ： 0.3695602766003533\n",
      " 轮次  502  当前轮训练集损失 ： 0.36953141282804114\n",
      " 轮次  503  当前轮训练集损失 ： 0.36950265164311713\n",
      " 轮次  504  当前轮训练集损失 ： 0.36947399247723733\n",
      " 轮次  505  当前轮训练集损失 ： 0.3694454347664492\n",
      " 轮次  506  当前轮训练集损失 ： 0.3694169779511497\n",
      " 轮次  507  当前轮训练集损失 ： 0.3693886214760434\n",
      " 轮次  508  当前轮训练集损失 ： 0.3693603647901016\n",
      " 轮次  509  当前轮训练集损失 ： 0.36933220734652117\n",
      " 轮次  510  当前轮训练集损失 ： 0.36930414860268473\n",
      " 轮次  511  当前轮训练集损失 ： 0.3692761880201204\n",
      " 轮次  512  当前轮训练集损失 ： 0.3692483250644629\n",
      " 轮次  513  当前轮训练集损失 ： 0.3692205592054141\n",
      " 轮次  514  当前轮训练集损失 ： 0.3691928899167049\n",
      " 轮次  515  当前轮训练集损失 ： 0.36916531667605695\n",
      " 轮次  516  当前轮训练集损失 ： 0.36913783896514535\n",
      " 轮次  517  当前轮训练集损失 ： 0.3691104562695606\n",
      " 轮次  518  当前轮训练集损失 ： 0.3690831680787728\n",
      " 轮次  519  当前轮训练集损失 ： 0.36905597388609435\n",
      " 轮次  520  当前轮训练集损失 ： 0.3690288731886446\n",
      " 轮次  521  当前轮训练集损失 ： 0.3690018654873137\n",
      " 轮次  522  当前轮训练集损失 ： 0.36897495028672767\n",
      " 轮次  523  当前轮训练集损失 ： 0.3689481270952137\n",
      " 轮次  524  当前轮训练集损失 ： 0.3689213954247651\n",
      " 轮次  525  当前轮训练集损失 ： 0.3688947547910077\n",
      " 轮次  526  当前轮训练集损失 ： 0.3688682047131662\n",
      " 轮次  527  当前轮训练集损失 ： 0.36884174471403053\n",
      " 轮次  528  当前轮训练集损失 ： 0.36881537431992273\n",
      " 轮次  529  当前轮训练集损失 ： 0.36878909306066476\n",
      " 轮次  530  当前轮训练集损失 ： 0.3687629004695461\n",
      " 轮次  531  当前轮训练集损失 ： 0.36873679608329146\n",
      " 轮次  532  当前轮训练集损失 ： 0.3687107794420298\n",
      " 轮次  533  当前轮训练集损失 ： 0.36868485008926255\n",
      " 轮次  534  当前轮训练集损失 ： 0.36865900757183284\n",
      " 轮次  535  当前轮训练集损失 ： 0.36863325143989534\n",
      " 轮次  536  当前轮训练集损失 ： 0.3686075812468854\n",
      " 轮次  537  当前轮训练集损失 ： 0.3685819965494895\n",
      " 轮次  538  当前轮训练集损失 ： 0.3685564969076155\n",
      " 轮次  539  当前轮训练集损失 ： 0.36853108188436356\n",
      " 轮次  540  当前轮训练集损失 ： 0.36850575104599703\n",
      " 轮次  541  当前轮训练集损失 ： 0.3684805039619137\n",
      " 轮次  542  当前轮训练集损失 ： 0.3684553402046174\n",
      " 轮次  543  当前轮训练集损失 ： 0.36843025934969054\n",
      " 轮次  544  当前轮训练集损失 ： 0.3684052609757653\n",
      " 轮次  545  当前轮训练集损失 ： 0.3683803446644974\n",
      " 轮次  546  当前轮训练集损失 ： 0.3683555100005376\n",
      " 轮次  547  当前轮训练集损失 ： 0.36833075657150605\n",
      " 轮次  548  当前轮训练集损失 ： 0.36830608396796505\n",
      " 轮次  549  当前轮训练集损失 ： 0.3682814917833925\n",
      " 轮次  550  当前轮训练集损失 ： 0.36825697961415643\n",
      " 轮次  551  当前轮训练集损失 ： 0.3682325470594889\n",
      " 轮次  552  当前轮训练集损失 ： 0.3682081937214608\n",
      " 轮次  553  当前轮训练集损失 ： 0.36818391920495597\n",
      " 轮次  554  当前轮训练集损失 ： 0.3681597231176467\n",
      " 轮次  555  当前轮训练集损失 ： 0.3681356050699693\n",
      " 轮次  556  当前轮训练集损失 ： 0.36811156467509915\n",
      " 轮次  557  当前轮训练集损失 ： 0.36808760154892645\n",
      " 轮次  558  当前轮训练集损失 ： 0.3680637153100326\n",
      " 轮次  559  当前轮训练集损失 ： 0.3680399055796665\n",
      " 轮次  560  当前轮训练集损失 ： 0.3680161719817208\n",
      " 轮次  561  当前轮训练集损失 ： 0.367992514142709\n",
      " 轮次  562  当前轮训练集损失 ： 0.3679689316917422\n",
      " 轮次  563  当前轮训练集损失 ： 0.36794542426050664\n",
      " 轮次  564  当前轮训练集损失 ： 0.3679219914832409\n",
      " 轮次  565  当前轮训练集损失 ： 0.3678986329967139\n",
      " 轮次  566  当前轮训练集损失 ： 0.36787534844020264\n",
      " 轮次  567  当前轮训练集损失 ： 0.36785213745547074\n",
      " 轮次  568  当前轮训练集损失 ： 0.3678289996867464\n",
      " 轮次  569  当前轮训练集损失 ： 0.3678059347807011\n",
      " 轮次  570  当前轮训练集损失 ： 0.3677829423864289\n",
      " 轮次  571  当前轮训练集损失 ： 0.3677600221554249\n",
      " 轮次  572  当前轮训练集损失 ： 0.36773717374156506\n",
      " 轮次  573  当前轮训练集损失 ： 0.3677143968010851\n",
      " 轮次  574  当前轮训练集损失 ： 0.36769169099256077\n",
      " 轮次  575  当前轮训练集损失 ： 0.3676690559768872\n",
      " 轮次  576  当前轮训练集损失 ： 0.3676464914172598\n",
      " 轮次  577  当前轮训练集损失 ： 0.36762399697915343\n",
      " 轮次  578  当前轮训练集损失 ： 0.367601572330304\n",
      " 轮次  579  当前轮训练集损失 ： 0.3675792171406884\n",
      " 轮次  580  当前轮训练集损失 ： 0.36755693108250587\n",
      " 轮次  581  当前轮训练集损失 ： 0.36753471383015873\n",
      " 轮次  582  当前轮训练集损失 ： 0.3675125650602342\n",
      " 轮次  583  当前轮训练集损失 ： 0.36749048445148497\n",
      " 轮次  584  当前轮训练集损失 ： 0.3674684716848117\n",
      " 轮次  585  当前轮训练集损失 ： 0.3674465264432444\n",
      " 轮次  586  当前轮训练集损失 ： 0.36742464841192457\n",
      " 轮次  587  当前轮训练集损失 ： 0.36740283727808737\n",
      " 轮次  588  当前轮训练集损失 ： 0.36738109273104397\n",
      " 轮次  589  当前轮训练集损失 ： 0.36735941446216425\n",
      " 轮次  590  当前轮训练集损失 ： 0.3673378021648593\n",
      " 轮次  591  当前轮训练集损失 ： 0.36731625553456415\n",
      " 轮次  592  当前轮训练集损失 ： 0.3672947742687214\n",
      " 轮次  593  当前轮训练集损失 ： 0.367273358066764\n",
      " 轮次  594  当前轮训练集损失 ： 0.36725200663009877\n",
      " 轮次  595  当前轮训练集损失 ： 0.3672307196620896\n",
      " 轮次  596  当前轮训练集损失 ： 0.367209496868042\n",
      " 轮次  597  当前轮训练集损失 ： 0.36718833795518624\n",
      " 轮次  598  当前轮训练集损失 ： 0.36716724263266143\n",
      " 轮次  599  当前轮训练集损失 ： 0.3671462106115001\n",
      " 轮次  600  当前轮训练集损失 ： 0.36712524160461213\n",
      " 轮次  601  当前轮训练集损失 ： 0.3671043353267697\n",
      " 轮次  602  当前轮训练集损失 ： 0.3670834914945915\n",
      " 轮次  603  当前轮训练集损失 ： 0.3670627098265277\n",
      " 轮次  604  当前轮训练集损失 ： 0.3670419900428448\n",
      " 轮次  605  当前轮训练集损失 ： 0.36702133186561103\n",
      " 轮次  606  当前轮训练集损失 ： 0.3670007350186813\n",
      " 轮次  607  当前轮训练集损失 ： 0.36698019922768244\n",
      " 轮次  608  当前轮训练集损失 ： 0.36695972421999923\n",
      " 轮次  609  当前轮训练集损失 ： 0.36693930972475947\n",
      " 轮次  610  当前轮训练集损失 ： 0.3669189554728202\n",
      " 轮次  611  当前轮训练集损失 ： 0.36689866119675346\n",
      " 轮次  612  当前轮训练集损失 ： 0.36687842663083237\n",
      " 轮次  613  当前轮训练集损失 ： 0.36685825151101736\n",
      " 轮次  614  当前轮训练集损失 ： 0.36683813557494244\n",
      " 轮次  615  当前轮训练集损失 ： 0.3668180785619017\n",
      " 轮次  616  当前轮训练集损失 ： 0.36679808021283605\n",
      " 轮次  617  当前轮训练集损失 ： 0.3667781402703197\n",
      " 轮次  618  当前轮训练集损失 ： 0.36675825847854704\n",
      " 轮次  619  当前轮训练集损失 ： 0.36673843458332\n",
      " 轮次  620  当前轮训练集损失 ： 0.3667186683320344\n",
      " 轮次  621  当前轮训练集损失 ： 0.36669895947366804\n",
      " 轮次  622  当前轮训练集损失 ： 0.3666793077587672\n",
      " 轮次  623  当前轮训练集损失 ： 0.36665971293943483\n",
      " 轮次  624  当前轮训练集损失 ： 0.3666401747693177\n",
      " 轮次  625  当前轮训练集损失 ： 0.36662069300359407\n",
      " 轮次  626  当前轮训练集损失 ： 0.366601267398962\n",
      " 轮次  627  当前轮训练集损失 ： 0.36658189771362654\n",
      " 轮次  628  当前轮训练集损失 ： 0.3665625837072883\n",
      " 轮次  629  当前轮训练集损失 ： 0.36654332514113147\n",
      " 轮次  630  当前轮训练集损失 ： 0.36652412177781196\n",
      " 轮次  631  当前轮训练集损失 ： 0.36650497338144616\n",
      " 轮次  632  当前轮训练集损失 ： 0.3664858797175987\n",
      " 轮次  633  当前轮训练集损失 ： 0.366466840553272\n",
      " 轮次  634  当前轮训练集损失 ： 0.3664478556568939\n",
      " 轮次  635  当前轮训练集损失 ： 0.3664289247983077\n",
      " 轮次  636  当前轮训练集损失 ： 0.36641004774875957\n",
      " 轮次  637  当前轮训练集损失 ： 0.3663912242808889\n",
      " 轮次  638  当前轮训练集损失 ： 0.3663724541687169\n",
      " 轮次  639  当前轮训练集损失 ： 0.3663537371876355\n",
      " 轮次  640  当前轮训练集损失 ： 0.36633507311439717\n",
      " 轮次  641  当前轮训练集损失 ： 0.36631646172710414\n",
      " 轮次  642  当前轮训练集损失 ： 0.36629790280519764\n",
      " 轮次  643  当前轮训练集损失 ： 0.36627939612944804\n",
      " 轮次  644  当前轮训练集损失 ： 0.3662609414819442\n",
      " 轮次  645  当前轮训练集损失 ： 0.366242538646083\n",
      " 轮次  646  当前轮训练集损失 ： 0.36622418740656015\n",
      " 轮次  647  当前轮训练集损失 ： 0.3662058875493591\n",
      " 轮次  648  当前轮训练集损失 ： 0.36618763886174177\n",
      " 轮次  649  当前轮训练集损失 ： 0.3661694411322385\n",
      " 轮次  650  当前轮训练集损失 ： 0.3661512941506383\n",
      " 轮次  651  当前轮训练集损失 ： 0.3661331977079795\n",
      " 轮次  652  当前轮训练集损失 ： 0.3661151515965395\n",
      " 轮次  653  当前轮训练集损失 ： 0.36609715560982586\n",
      " 轮次  654  当前轮训练集损失 ： 0.3660792095425672\n",
      " 轮次  655  当前轮训练集损失 ： 0.36606131319070256\n",
      " 轮次  656  当前轮训练集损失 ： 0.3660434663513738\n",
      " 轮次  657  当前轮训练集损失 ： 0.36602566882291554\n",
      " 轮次  658  当前轮训练集损失 ： 0.36600792040484587\n",
      " 轮次  659  当前轮训练集损失 ： 0.36599022089785826\n",
      " 轮次  660  当前轮训练集损失 ： 0.36597257010381207\n",
      " 轮次  661  当前轮训练集损失 ： 0.36595496782572356\n",
      " 轮次  662  当前轮训练集损失 ： 0.3659374138677578\n",
      " 轮次  663  当前轮训练集损失 ： 0.36591990803521934\n",
      " 轮次  664  当前轮训练集损失 ： 0.3659024501345442\n",
      " 轮次  665  当前轮训练集损失 ： 0.3658850399732907\n",
      " 轮次  666  当前轮训练集损失 ： 0.36586767736013187\n",
      " 轮次  667  当前轮训练集损失 ： 0.3658503621048462\n",
      " 轮次  668  当前轮训练集损失 ： 0.3658330940183104\n",
      " 轮次  669  当前轮训练集损失 ： 0.36581587291249007\n",
      " 轮次  670  当前轮训练集损失 ： 0.3657986986004322\n",
      " 轮次  671  当前轮训练集损失 ： 0.36578157089625724\n",
      " 轮次  672  当前轮训练集损失 ： 0.36576448961515057\n",
      " 轮次  673  当前轮训练集损失 ： 0.3657474545733552\n",
      " 轮次  674  当前轮训练集损失 ： 0.3657304655881634\n",
      " 轮次  675  当前轮训练集损失 ： 0.3657135224779093\n",
      " 轮次  676  当前轮训练集损失 ： 0.36569662506196077\n",
      " 轮次  677  当前轮训练集损失 ： 0.36567977316071226\n",
      " 轮次  678  当前轮训练集损失 ： 0.36566296659557695\n",
      " 轮次  679  当前轮训练集损失 ： 0.3656462051889792\n",
      " 轮次  680  当前轮训练集损失 ： 0.3656294887643472\n",
      " 轮次  681  当前轮训练集损失 ： 0.36561281714610555\n",
      " 轮次  682  当前轮训练集损失 ： 0.36559619015966827\n",
      " 轮次  683  当前轮训练集损失 ： 0.36557960763143077\n",
      " 轮次  684  当前轮训练集损失 ： 0.3655630693887636\n",
      " 轮次  685  当前轮训练集损失 ： 0.36554657526000456\n",
      " 轮次  686  当前轮训练集损失 ： 0.36553012507445226\n",
      " 轮次  687  当前轮训练集损失 ： 0.3655137186623586\n",
      " 轮次  688  当前轮训练集损失 ： 0.3654973558549221\n",
      " 轮次  689  当前轮训练集损失 ： 0.365481036484281\n",
      " 轮次  690  当前轮训练集损失 ： 0.36546476038350645\n",
      " 轮次  691  当前轮训练集损失 ： 0.36544852738659583\n",
      " 轮次  692  当前轮训练集损失 ： 0.36543233732846564\n",
      " 轮次  693  当前轮训练集损失 ： 0.3654161900449453\n",
      " 轮次  694  当前轮训练集损失 ： 0.36540008537277063\n",
      " 轮次  695  当前轮训练集损失 ： 0.3653840231495769\n",
      " 轮次  696  当前轮训练集损失 ： 0.36536800321389257\n",
      " 轮次  697  当前轮训练集损失 ： 0.36535202540513295\n",
      " 轮次  698  当前轮训练集损失 ： 0.36533608956359365\n",
      " 轮次  699  当前轮训练集损失 ： 0.36532019553044465\n",
      " 轮次  700  当前轮训练集损失 ： 0.3653043431477234\n",
      " 轮次  701  当前轮训练集损失 ： 0.3652885322583296\n",
      " 轮次  702  当前轮训练集损失 ： 0.3652727627060175\n",
      " 轮次  703  当前轮训练集损失 ： 0.36525703433539175\n",
      " 轮次  704  当前轮训练集损失 ： 0.3652413469918999\n",
      " 轮次  705  当前轮训练集损失 ： 0.36522570052182685\n",
      " 轮次  706  当前轮训练集损失 ： 0.36521009477228905\n",
      " 轮次  707  当前轮训练集损失 ： 0.3651945295912285\n",
      " 轮次  708  当前轮训练集损失 ： 0.36517900482740684\n",
      " 轮次  709  当前轮训练集损失 ： 0.36516352033039984\n",
      " 轮次  710  当前轮训练集损失 ： 0.3651480759505911\n",
      " 轮次  711  当前轮训练集损失 ： 0.365132671539167\n",
      " 轮次  712  当前轮训练集损失 ： 0.36511730694811073\n",
      " 轮次  713  当前轮训练集损失 ： 0.3651019820301965\n",
      " 轮次  714  当前轮训练集损失 ： 0.36508669663898446\n",
      " 轮次  715  当前轮训练集损失 ： 0.3650714506288149\n",
      " 轮次  716  当前轮训练集损失 ： 0.36505624385480256\n",
      " 轮次  717  当前轮训练集损失 ： 0.36504107617283227\n",
      " 轮次  718  当前轮训练集损失 ： 0.3650259474395517\n",
      " 轮次  719  当前轮训练集损失 ： 0.36501085751236817\n",
      " 轮次  720  当前轮训练集损失 ： 0.36499580624944183\n",
      " 轮次  721  当前轮训练集损失 ： 0.36498079350968105\n",
      " 轮次  722  当前轮训练集损失 ： 0.3649658191527372\n",
      " 轮次  723  当前轮训练集损失 ： 0.3649508830389993\n",
      " 轮次  724  当前轮训练集损失 ： 0.36493598502958924\n",
      " 轮次  725  当前轮训练集损失 ： 0.36492112498635654\n",
      " 轮次  726  当前轮训练集损失 ： 0.3649063027718731\n",
      " 轮次  727  当前轮训练集损失 ： 0.36489151824942867\n",
      " 轮次  728  当前轮训练集损失 ： 0.36487677128302576\n",
      " 轮次  729  当前轮训练集损失 ： 0.3648620617373742\n",
      " 轮次  730  当前轮训练集损失 ： 0.3648473894778875\n",
      " 轮次  731  当前轮训练集损失 ： 0.3648327543706765\n",
      " 轮次  732  当前轮训练集损失 ： 0.3648181562825463\n",
      " 轮次  733  当前轮训练集损失 ： 0.3648035950809897\n",
      " 轮次  734  当前轮训练集损失 ： 0.36478907063418414\n",
      " 轮次  735  当前轮训练集损失 ： 0.36477458281098585\n",
      " 轮次  736  当前轮训练集损失 ： 0.36476013148092595\n",
      " 轮次  737  当前轮训练集损失 ： 0.3647457165142055\n",
      " 轮次  738  当前轮训练集损失 ： 0.3647313377816913\n",
      " 轮次  739  当前轮训练集损失 ： 0.3647169951549108\n",
      " 轮次  740  当前轮训练集损失 ： 0.3647026885060484\n",
      " 轮次  741  当前轮训练集损失 ： 0.3646884177079402\n",
      " 轮次  742  当前轮训练集损失 ： 0.36467418263407014\n",
      " 轮次  743  当前轮训练集损失 ： 0.3646599831585658\n",
      " 轮次  744  当前轮训练集损失 ： 0.3646458191561934\n",
      " 轮次  745  当前轮训练集损失 ： 0.3646316905023538\n",
      " 轮次  746  当前轮训练集损失 ： 0.36461759707307884\n",
      " 轮次  747  当前轮训练集损失 ： 0.36460353874502593\n",
      " 轮次  748  当前轮训练集损失 ： 0.3645895153954753\n",
      " 轮次  749  当前轮训练集损失 ： 0.3645755269023243\n",
      " 轮次  750  当前轮训练集损失 ： 0.36456157314408466\n",
      " 轮次  751  当前轮训练集损失 ： 0.36454765399987726\n",
      " 轮次  752  当前轮训练集损失 ： 0.3645337693494292\n",
      " 轮次  753  当前轮训练集损失 ： 0.3645199190730688\n",
      " 轮次  754  当前轮训练集损失 ： 0.3645061030517219\n",
      " 轮次  755  当前轮训练集损失 ： 0.3644923211669083\n",
      " 轮次  756  当前轮训练集损失 ： 0.3644785733007373\n",
      " 轮次  757  当前轮训练集损失 ： 0.3644648593359042\n",
      " 轮次  758  当前轮训练集损失 ： 0.3644511791556862\n",
      " 轮次  759  当前轮训练集损失 ： 0.36443753264393847\n",
      " 轮次  760  当前轮训练集损失 ： 0.3644239196850909\n",
      " 轮次  761  当前轮训练集损失 ： 0.36441034016414353\n",
      " 轮次  762  当前轮训练集损失 ： 0.3643967939666633\n",
      " 轮次  763  当前轮训练集损失 ： 0.36438328097878053\n",
      " 轮次  764  当前轮训练集损失 ： 0.3643698010871846\n",
      " 轮次  765  当前轮训练集损失 ： 0.3643563541791207\n",
      " 轮次  766  当前轮训练集损失 ： 0.3643429401423865\n",
      " 轮次  767  当前轮训练集损失 ： 0.3643295588653277\n",
      " 轮次  768  当前轮训练集损失 ： 0.36431621023683536\n",
      " 轮次  769  当前轮训练集损失 ： 0.3643028941463417\n",
      " 轮次  770  当前轮训练集损失 ： 0.36428961048381703\n",
      " 轮次  771  当前轮训练集损失 ： 0.364276359139766\n",
      " 轮次  772  当前轮训练集损失 ： 0.3642631400052242\n",
      " 轮次  773  当前轮训练集损失 ： 0.3642499529717549\n",
      " 轮次  774  当前轮训练集损失 ： 0.3642367979314453\n",
      " 轮次  775  当前轮训练集损失 ： 0.3642236747769035\n",
      " 轮次  776  当前轮训练集损失 ： 0.36421058340125484\n",
      " 轮次  777  当前轮训练集损失 ： 0.364197523698139\n",
      " 轮次  778  当前轮训练集损失 ： 0.3641844955617064\n",
      " 轮次  779  当前轮训练集损失 ： 0.36417149888661454\n",
      " 轮次  780  当前轮训练集损失 ： 0.3641585335680256\n",
      " 轮次  781  当前轮训练集损失 ： 0.3641455995016027\n",
      " 轮次  782  当前轮训练集损失 ： 0.3641326965835067\n",
      " 轮次  783  当前轮训练集损失 ： 0.36411982471039334\n",
      " 轮次  784  当前轮训练集损失 ： 0.36410698377940964\n",
      " 轮次  785  当前轮训练集损失 ： 0.3640941736881911\n",
      " 轮次  786  当前轮训练集损失 ： 0.36408139433485853\n",
      " 轮次  787  当前轮训练集损失 ： 0.3640686456180149\n",
      " 轮次  788  当前轮训练集损失 ： 0.36405592743674237\n",
      " 轮次  789  当前轮训练集损失 ： 0.36404323969059915\n",
      " 轮次  790  当前轮训练集损失 ： 0.3640305822796168\n",
      " 轮次  791  当前轮训练集损失 ： 0.3640179551042968\n",
      " 轮次  792  当前轮训练集损失 ： 0.364005358065608\n",
      " 轮次  793  当前轮训练集损失 ： 0.36399279106498317\n",
      " 轮次  794  当前轮训练集损失 ： 0.36398025400431677\n",
      " 轮次  795  当前轮训练集损失 ： 0.36396774678596155\n",
      " 轮次  796  当前轮训练集损失 ： 0.36395526931272576\n",
      " 轮次  797  当前轮训练集损失 ： 0.36394282148787055\n",
      " 轮次  798  当前轮训练集损失 ： 0.36393040321510683\n",
      " 轮次  799  当前轮训练集损失 ： 0.3639180143985926\n",
      " 轮次  800  当前轮训练集损失 ： 0.3639056549429303\n",
      " 轮次  801  当前轮训练集损失 ： 0.3638933247531636\n",
      " 轮次  802  当前轮训练集损失 ： 0.3638810237347754\n",
      " 轮次  803  当前轮训练集损失 ： 0.3638687517936844\n",
      " 轮次  804  当前轮训练集损失 ： 0.3638565088362426\n",
      " 轮次  805  当前轮训练集损失 ： 0.3638442947692329\n",
      " 轮次  806  当前轮训练集损失 ： 0.3638321094998664\n",
      " 轮次  807  当前轮训练集损失 ： 0.36381995293577907\n",
      " 轮次  808  当前轮训练集损失 ： 0.3638078249850301\n",
      " 轮次  809  当前轮训练集损失 ： 0.3637957255560988\n",
      " 轮次  810  当前轮训练集损失 ： 0.3637836545578819\n",
      " 轮次  811  当前轮训练集损失 ： 0.3637716118996912\n",
      " 轮次  812  当前轮训练集损失 ： 0.36375959749125125\n",
      " 轮次  813  当前轮训练集损失 ： 0.36374761124269633\n",
      " 轮次  814  当前轮训练集损失 ： 0.36373565306456823\n",
      " 轮次  815  当前轮训练集损失 ： 0.36372372286781374\n",
      " 轮次  816  当前轮训练集损失 ： 0.36371182056378215\n",
      " 轮次  817  当前轮训练集损失 ： 0.3636999460642229\n",
      " 轮次  818  当前轮训练集损失 ： 0.3636880992812829\n",
      " 轮次  819  当前轮训练集损失 ： 0.3636762801275044\n",
      " 轮次  820  当前轮训练集损失 ： 0.3636644885158226\n",
      " 轮次  821  当前轮训练集损失 ： 0.3636527243595629\n",
      " 轮次  822  当前轮训练集损失 ： 0.3636409875724391\n",
      " 轮次  823  当前轮训练集损失 ： 0.3636292780685502\n",
      " 轮次  824  当前轮训练集损失 ： 0.3636175957623794\n",
      " 轮次  825  当前轮训练集损失 ： 0.3636059405687904\n",
      " 轮次  826  当前轮训练集损失 ： 0.3635943124030259\n",
      " 轮次  827  当前轮训练集损失 ： 0.3635827111807055\n",
      " 轮次  828  当前轮训练集损失 ： 0.3635711368178225\n",
      " 轮次  829  当前轮训练集损失 ： 0.3635595892307426\n",
      " 轮次  830  当前轮训练集损失 ： 0.36354806833620157\n",
      " 轮次  831  当前轮训练集损失 ： 0.3635365740513023\n",
      " 轮次  832  当前轮训练集损失 ： 0.3635251062935136\n",
      " 轮次  833  当前轮训练集损失 ： 0.3635136649806671\n",
      " 轮次  834  当前轮训练集损失 ： 0.3635022500309558\n",
      " 轮次  835  当前轮训练集损失 ： 0.3634908613629318\n",
      " 轮次  836  当前轮训练集损失 ： 0.36347949889550374\n",
      " 轮次  837  当前轮训练集损失 ： 0.36346816254793507\n",
      " 轮次  838  当前轮训练集损失 ： 0.36345685223984187\n",
      " 轮次  839  当前轮训练集损失 ： 0.363445567891191\n",
      " 轮次  840  当前轮训练集损失 ： 0.36343430942229743\n",
      " 轮次  841  当前轮训练集损失 ： 0.36342307675382285\n",
      " 轮次  842  当前轮训练集损失 ： 0.36341186980677304\n",
      " 轮次  843  当前轮训练集损失 ： 0.3634006885024963\n",
      " 轮次  844  当前轮训练集损失 ： 0.3633895327626816\n",
      " 轮次  845  当前轮训练集损失 ： 0.36337840250935566\n",
      " 轮次  846  当前轮训练集损失 ： 0.3633672976648821\n",
      " 轮次  847  当前轮训练集损失 ： 0.363356218151959\n",
      " 轮次  848  当前轮训练集损失 ： 0.3633451638936164\n",
      " 轮次  849  当前轮训练集损失 ： 0.3633341348132152\n",
      " 轮次  850  当前轮训练集损失 ： 0.3633231308344451\n",
      " 轮次  851  当前轮训练集损失 ： 0.3633121518813222\n",
      " 轮次  852  当前轮训练集损失 ： 0.36330119787818743\n",
      " 轮次  853  当前轮训练集损失 ： 0.3632902687497052\n",
      " 轮次  854  当前轮训练集损失 ： 0.36327936442086006\n",
      " 轮次  855  当前轮训练集损失 ： 0.36326848481695645\n",
      " 轮次  856  当前轮训练集损失 ： 0.36325762986361604\n",
      " 轮次  857  当前轮训练集损失 ： 0.36324679948677596\n",
      " 轮次  858  当前轮训练集损失 ： 0.3632359936126871\n",
      " 轮次  859  当前轮训练集损失 ： 0.36322521216791237\n",
      " 轮次  860  当前轮训练集损失 ： 0.3632144550793246\n",
      " 轮次  861  当前轮训练集损失 ： 0.3632037222741052\n",
      " 轮次  862  当前轮训练集损失 ： 0.363193013679742\n",
      " 轮次  863  当前轮训练集损失 ： 0.36318232922402793\n",
      " 轮次  864  当前轮训练集损失 ： 0.36317166883505875\n",
      " 轮次  865  当前轮训练集损失 ： 0.3631610324412317\n",
      " 轮次  866  当前轮训练集损失 ： 0.36315041997124387\n",
      " 轮次  867  当前轮训练集损失 ： 0.3631398313540899\n",
      " 轮次  868  当前轮训练集损失 ： 0.3631292665190611\n",
      " 轮次  869  当前轮训练集损失 ： 0.36311872539574297\n",
      " 轮次  870  当前轮训练集损失 ： 0.3631082079140147\n",
      " 轮次  871  当前轮训练集损失 ： 0.36309771400404567\n",
      " 轮次  872  当前轮训练集损失 ： 0.36308724359629585\n",
      " 轮次  873  当前轮训练集损失 ： 0.36307679662151254\n",
      " 轮次  874  当前轮训练集损失 ： 0.3630663730107298\n",
      " 轮次  875  当前轮训练集损失 ： 0.36305597269526646\n",
      " 轮次  876  当前轮训练集损失 ： 0.36304559560672406\n",
      " 轮次  877  当前轮训练集损失 ： 0.3630352416769863\n",
      " 轮次  878  当前轮训练集损失 ： 0.36302491083821653\n",
      " 轮次  879  当前轮训练集损失 ： 0.36301460302285676\n",
      " 轮次  880  当前轮训练集损失 ： 0.3630043181636256\n",
      " 轮次  881  当前轮训练集损失 ： 0.3629940561935174\n",
      " 轮次  882  当前轮训练集损失 ： 0.3629838170457997\n",
      " 轮次  883  当前轮训练集损失 ： 0.362973600654013\n",
      " 轮次  884  当前轮训练集损失 ： 0.3629634069519682\n",
      " 轮次  885  当前轮训练集损失 ： 0.36295323587374556\n",
      " 轮次  886  当前轮训练集损失 ： 0.36294308735369313\n",
      " 轮次  887  当前轮训练集损失 ： 0.36293296132642516\n",
      " 轮次  888  当前轮训练集损失 ： 0.3629228577268209\n",
      " 轮次  889  当前轮训练集损失 ： 0.36291277649002285\n",
      " 轮次  890  当前轮训练集损失 ： 0.3629027175514355\n",
      " 轮次  891  当前轮训练集损失 ： 0.3628926808467237\n",
      " 轮次  892  当前轮训练集损失 ： 0.3628826663118115\n",
      " 轮次  893  当前轮训练集损失 ： 0.36287267388288036\n",
      " 轮次  894  当前轮训练集损失 ： 0.36286270349636834\n",
      " 轮次  895  当前轮训练集损失 ： 0.3628527550889679\n",
      " 轮次  896  当前轮训练集损失 ： 0.3628428285976252\n",
      " 轮次  897  当前轮训练集损失 ： 0.3628329239595383\n",
      " 轮次  898  当前轮训练集损失 ： 0.3628230411121561\n",
      " 轮次  899  当前轮训练集损失 ： 0.3628131799931764\n",
      " 轮次  900  当前轮训练集损失 ： 0.3628033405405455\n",
      " 轮次  901  当前轮训练集损失 ： 0.3627935226924559\n",
      " 轮次  902  当前轮训练集损失 ： 0.3627837263873455\n",
      " 轮次  903  当前轮训练集损失 ： 0.3627739515638962\n",
      " 轮次  904  当前轮训练集损失 ： 0.3627641981610324\n",
      " 轮次  905  当前轮训练集损失 ： 0.36275446611792006\n",
      " 轮次  906  当前轮训练集损失 ： 0.3627447553739648\n",
      " 轮次  907  当前轮训练集损失 ： 0.36273506586881116\n",
      " 轮次  908  当前轮训练集损失 ： 0.3627253975423412\n",
      " 轮次  909  当前轮训练集损失 ： 0.362715750334673\n",
      " 轮次  910  当前轮训练集损失 ： 0.36270612418615955\n",
      " 轮次  911  当前轮训练集损失 ： 0.36269651903738753\n",
      " 轮次  912  当前轮训练集损失 ： 0.36268693482917624\n",
      " 轮次  913  当前轮训练集损失 ： 0.36267737150257556\n",
      " 轮次  914  当前轮训练集损失 ： 0.36266782899886574\n",
      " 轮次  915  当前轮训练集损失 ： 0.3626583072595558\n",
      " 轮次  916  当前轮训练集损失 ： 0.36264880622638185\n",
      " 轮次  917  当前轮训练集损失 ： 0.36263932584130654\n",
      " 轮次  918  当前轮训练集损失 ： 0.36262986604651776\n",
      " 轮次  919  当前轮训练集损失 ： 0.3626204267844268\n",
      " 轮次  920  当前轮训练集损失 ： 0.362611007997668\n",
      " 轮次  921  当前轮训练集损失 ： 0.36260160962909715\n",
      " 轮次  922  当前轮训练集损失 ： 0.36259223162179044\n",
      " 轮次  923  当前轮训练集损失 ： 0.36258287391904326\n",
      " 轮次  924  当前轮训练集损失 ： 0.3625735364643687\n",
      " 轮次  925  当前轮训练集损失 ： 0.36256421920149734\n",
      " 轮次  926  当前轮训练集损失 ： 0.3625549220743752\n",
      " 轮次  927  当前轮训练集损失 ： 0.3625456450271627\n",
      " 轮次  928  当前轮训练集损失 ： 0.36253638800423427\n",
      " 轮次  929  当前轮训练集损失 ： 0.36252715095017635\n",
      " 轮次  930  当前轮训练集损失 ： 0.36251793380978686\n",
      " 轮次  931  当前轮训练集损失 ： 0.36250873652807386\n",
      " 轮次  932  当前轮训练集损失 ： 0.3624995590502544\n",
      " 轮次  933  当前轮训练集损失 ： 0.3624904013217536\n",
      " 轮次  934  当前轮训练集损失 ： 0.36248126328820357\n",
      " 轮次  935  当前轮训练集损失 ： 0.36247214489544216\n",
      " 轮次  936  当前轮训练集损失 ： 0.3624630460895122\n",
      " 轮次  937  当前轮训练集损失 ： 0.36245396681666003\n",
      " 轮次  938  当前轮训练集损失 ： 0.3624449070233349\n",
      " 轮次  939  当前轮训练集损失 ： 0.36243586665618727\n",
      " 轮次  940  当前轮训练集损失 ： 0.36242684566206856\n",
      " 轮次  941  当前轮训练集损失 ： 0.36241784398802995\n",
      " 轮次  942  当前轮训练集损失 ： 0.3624088615813205\n",
      " 轮次  943  当前轮训练集损失 ： 0.3623998983893876\n",
      " 轮次  944  当前轮训练集损失 ： 0.3623909543598744\n",
      " 轮次  945  当前轮训练集损失 ： 0.36238202944062003\n",
      " 轮次  946  当前轮训练集损失 ： 0.362373123579658\n",
      " 轮次  947  当前轮训练集损失 ： 0.3623642367252152\n",
      " 轮次  948  当前轮训练集损失 ： 0.3623553688257114\n",
      " 轮次  949  当前轮训练集损失 ： 0.3623465198297576\n",
      " 轮次  950  当前轮训练集损失 ： 0.36233768968615543\n",
      " 轮次  951  当前轮训练集损失 ： 0.3623288783438965\n",
      " 轮次  952  当前轮训练集损失 ： 0.36232008575216035\n",
      " 轮次  953  当前轮训练集损失 ： 0.3623113118603149\n",
      " 轮次  954  当前轮训练集损失 ： 0.3623025566179146\n",
      " 轮次  955  当前轮训练集损失 ： 0.3622938199746996\n",
      " 轮次  956  当前轮训练集损失 ： 0.362285101880595\n",
      " 轮次  957  当前轮训练集损失 ： 0.36227640228571006\n",
      " 轮次  958  当前轮训练集损失 ： 0.3622677211403365\n",
      " 轮次  959  当前轮训练集损失 ： 0.3622590583949489\n",
      " 轮次  960  当前轮训练集损失 ： 0.3622504140002024\n",
      " 轮次  961  当前轮训练集损失 ： 0.3622417879069328\n",
      " 轮次  962  当前轮训练集损失 ： 0.3622331800661549\n",
      " 轮次  963  当前轮训练集损失 ： 0.3622245904290626\n",
      " 轮次  964  当前轮训练集损失 ： 0.36221601894702676\n",
      " 轮次  965  当前轮训练集损失 ： 0.3622074655715954\n",
      " 轮次  966  当前轮训练集损失 ： 0.36219893025449224\n",
      " 轮次  967  当前轮训练集损失 ： 0.36219041294761584\n",
      " 轮次  968  当前轮训练集损失 ： 0.36218191360303903\n",
      " 轮次  969  当前轮训练集损失 ： 0.36217343217300774\n",
      " 轮次  970  当前轮训练集损失 ： 0.36216496860994046\n",
      " 轮次  971  当前轮训练集损失 ： 0.36215652286642713\n",
      " 轮次  972  当前轮训练集损失 ： 0.36214809489522826\n",
      " 轮次  973  当前轮训练集损失 ： 0.3621396846492744\n",
      " 轮次  974  当前轮训练集损失 ： 0.36213129208166484\n",
      " 轮次  975  当前轮训练集损失 ： 0.3621229171456674\n",
      " 轮次  976  当前轮训练集损失 ： 0.3621145597947171\n",
      " 轮次  977  当前轮训练集损失 ： 0.3621062199824154\n",
      " 轮次  978  当前轮训练集损失 ： 0.36209789766252937\n",
      " 轮次  979  当前轮训练集损失 ： 0.3620895927889916\n",
      " 轮次  980  当前轮训练集损失 ： 0.36208130531589794\n",
      " 轮次  981  当前轮训练集损失 ： 0.36207303519750833\n",
      " 轮次  982  当前轮训练集损失 ： 0.3620647823882447\n",
      " 轮次  983  当前轮训练集损失 ： 0.36205654684269084\n",
      " 轮次  984  当前轮训练集损失 ： 0.3620483285155916\n",
      " 轮次  985  当前轮训练集损失 ： 0.362040127361852\n",
      " 轮次  986  当前轮训练集损失 ： 0.3620319433365362\n",
      " 轮次  987  当前轮训练集损失 ： 0.3620237763948672\n",
      " 轮次  988  当前轮训练集损失 ： 0.3620156264922259\n",
      " 轮次  989  当前轮训练集损失 ： 0.3620074935841501\n",
      " 轮次  990  当前轮训练集损失 ： 0.36199937762633383\n",
      " 轮次  991  当前轮训练集损失 ： 0.36199127857462715\n",
      " 轮次  992  当前轮训练集损失 ： 0.36198319638503473\n",
      " 轮次  993  当前轮训练集损失 ： 0.3619751310137151\n",
      " 轮次  994  当前轮训练集损失 ： 0.3619670824169805\n",
      " 轮次  995  当前轮训练集损失 ： 0.3619590505512956\n",
      " 轮次  996  当前轮训练集损失 ： 0.361951035373277\n",
      " 轮次  997  当前轮训练集损失 ： 0.36194303683969253\n",
      " 轮次  998  当前轮训练集损失 ： 0.3619350549074605\n",
      " 轮次  999  当前轮训练集损失 ： 0.3619270895336488\n",
      " 轮次  1000  当前轮训练集损失 ： 0.36191914067547437\n",
      "训练最终损失: 0.36191914067547437\n",
      "逻辑回归训练的准确率为:  83.88%\n"
     ]
    }
   ],
   "source": [
    "loss_history, weight_history, bias_histoay = \\\n",
    "    logistic_regression(X_train, y_train, weight, bias, alpha, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f2bc22",
   "metadata": {},
   "source": [
    "### 绘制损失曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1bc04e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:45:53.220566Z",
     "start_time": "2024-11-19T08:45:53.214095Z"
    }
   },
   "outputs": [],
   "source": [
    "# 归一化验证数据\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0cc10819",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:45:54.475490Z",
     "start_time": "2024-11-19T08:45:54.343185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "loss_history_test = np.zeros(iterations) # 训练集没有归一化\n",
    "for i in range(iterations):\n",
    "    loss_history_test[i] = loss_function(X_test,y_test,weight_history[i],bias_histoay[i])\n",
    "index = np.arange(0,iterations,1)\n",
    "plt.figure(figsize=(12,6), dpi=100)\n",
    "plt.plot(index, loss_history, c='blue', linestyle='dotted',label='Trainning Loss')\n",
    "plt.plot(index, loss_history_test, c='red', label='Test Loss')\n",
    "\n",
    "plt.xlabel('Number of Iteratrion')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaaa40b",
   "metadata": {},
   "source": [
    "## 哑变量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d644afe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:07:57.824323Z",
     "start_time": "2024-11-19T09:07:57.804183Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_0</th>\n",
       "      <th>cp_1</th>\n",
       "      <th>cp_2</th>\n",
       "      <th>cp_3</th>\n",
       "      <th>thal_0</th>\n",
       "      <th>thal_1</th>\n",
       "      <th>thal_2</th>\n",
       "      <th>thal_3</th>\n",
       "      <th>slope_0</th>\n",
       "      <th>slope_1</th>\n",
       "      <th>...</th>\n",
       "      <th>sex</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>ca</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cp_0   cp_1   cp_2   cp_3  thal_0  thal_1  thal_2  thal_3  slope_0  \\\n",
       "0  False  False  False   True   False    True   False   False     True   \n",
       "1  False  False   True  False   False   False    True   False     True   \n",
       "2  False   True  False  False   False   False    True   False    False   \n",
       "3  False   True  False  False   False   False    True   False    False   \n",
       "4   True  False  False  False   False   False    True   False    False   \n",
       "\n",
       "   slope_1  ...  sex  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0    False  ...    1       145   233    1        0      150      0      2.3   \n",
       "1    False  ...    1       130   250    0        1      187      0      3.5   \n",
       "2    False  ...    0       130   204    0        0      172      0      1.4   \n",
       "3    False  ...    1       120   236    0        1      178      0      0.8   \n",
       "4    False  ...    0       120   354    0        1      163      1      0.6   \n",
       "\n",
       "   ca  target  \n",
       "0   0       1  \n",
       "1   0       1  \n",
       "2   0       1  \n",
       "3   0       1  \n",
       "4   0       1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/heart.csv')\n",
    "a = pd.get_dummies(data['cp'],prefix='cp')\n",
    "b = pd.get_dummies(data['thal'], prefix='thal')\n",
    "c = pd.get_dummies(data['slope'], prefix='slope')\n",
    "frames = [a,b,c,data]\n",
    "data = pd.concat(frames,axis=1)\n",
    "data = data.drop(columns=['cp','thal','slope'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "3bd5ba57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:17:34.039448Z",
     "start_time": "2024-11-19T09:17:34.027057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征集形状: (303, 21)\n",
      "标签集形状: (303, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "y = data['target'].values\n",
    "y = y.reshape(-1,1)\n",
    "print(f'特征集形状: {X.shape}')\n",
    "print(f'标签集形状: {y.shape}')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "0aecffa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:17:43.407961Z",
     "start_time": "2024-11-19T09:17:43.404448Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "dimension = X.shape[1]\n",
    "weight = np.full((dimension,1),0.1)\n",
    "bias = 0\n",
    "# 初始化超参数\n",
    "alpha = 1 # 学习速率\n",
    "iterations = 3000   # 迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "cf2ae0bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:17:43.808110Z",
     "start_time": "2024-11-19T09:17:43.626072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 轮次  1  当前轮训练集损失 ： 0.6222467622787473\n",
      " 轮次  2  当前轮训练集损失 ： 0.5679450127855463\n",
      " 轮次  3  当前轮训练集损失 ： 0.5308013487322748\n",
      " 轮次  4  当前轮训练集损失 ： 0.5043965878693714\n",
      " 轮次  5  当前轮训练集损失 ： 0.4850272706593694\n",
      " 轮次  6  当前轮训练集损失 ： 0.47040417019318226\n",
      " 轮次  7  当前轮训练集损失 ： 0.4590781878636297\n",
      " 轮次  8  当前轮训练集损失 ： 0.4501057746298244\n",
      " 轮次  9  当前轮训练集损失 ： 0.4428548588411144\n",
      " 轮次  10  当前轮训练集损失 ： 0.4368905967069161\n",
      " 轮次  11  当前轮训练集损失 ： 0.4319064979082156\n",
      " 轮次  12  当前轮训练集损失 ： 0.42768179348834606\n",
      " 轮次  13  当前轮训练集损失 ： 0.42405434407808884\n",
      " 轮次  14  当前轮训练集损失 ： 0.4209029883377424\n",
      " 轮次  15  当前轮训练集损失 ： 0.41813577235349875\n",
      " 轮次  16  当前轮训练集损失 ： 0.41568193167574335\n",
      " 轮次  17  当前轮训练集损失 ： 0.41348632224634146\n",
      " 轮次  18  当前轮训练集损失 ： 0.41150548295290085\n",
      " 轮次  19  当前轮训练集损失 ： 0.4097048062978073\n",
      " 轮次  20  当前轮训练集损失 ： 0.40805647500660996\n",
      " 轮次  21  当前轮训练集损失 ： 0.4065379367021377\n",
      " 轮次  22  当前轮训练集损失 ： 0.40513076223937955\n",
      " 轮次  23  当前轮训练集损失 ： 0.40381978138700875\n",
      " 轮次  24  当前轮训练集损失 ： 0.4025924215590717\n",
      " 轮次  25  当前轮训练集损失 ： 0.40143819695604727\n",
      " 轮次  26  当前轮训练集损失 ： 0.400348310338514\n",
      " 轮次  27  当前轮训练集损失 ： 0.3993153399996969\n",
      " 轮次  28  当前轮训练集损失 ： 0.3983329917927679\n",
      " 轮次  29  当前轮训练集损失 ： 0.39739590126802354\n",
      " 轮次  30  当前轮训练集损失 ： 0.39649947472496155\n",
      " 轮次  31  当前轮训练集损失 ： 0.3956397607172539\n",
      " 轮次  32  当前轮训练集损失 ： 0.39481334556004777\n",
      " 轮次  33  当前轮训练集损失 ： 0.39401726788311237\n",
      " 轮次  34  当前轮训练集损失 ： 0.39324894839279206\n",
      " 轮次  35  当前轮训练集损失 ： 0.39250613185134625\n",
      " 轮次  36  当前轮训练集损失 ： 0.39178683892596494\n",
      " 轮次  37  当前轮训练集损失 ： 0.391089326053333\n",
      " 轮次  38  当前轮训练集损失 ： 0.3904120518467037\n",
      " 轮次  39  当前轮训练集损失 ： 0.38975364886859376\n",
      " 轮次  40  当前轮训练集损失 ： 0.3891128998237928\n",
      " 轮次  41  当前轮训练集损失 ： 0.3884887174095206\n",
      " 轮次  42  当前轮训练集损失 ： 0.3878801272036484\n",
      " 轮次  43  当前轮训练集损失 ： 0.3872862530864526\n",
      " 轮次  44  当前轮训练集损失 ： 0.38670630478293977\n",
      " 轮次  45  当前轮训练集损失 ： 0.38613956718630854\n",
      " 轮次  46  当前轮训练集损失 ： 0.3855853911824425\n",
      " 轮次  47  当前轮训练集损失 ： 0.3850431857434068\n",
      " 轮次  48  当前轮训练集损失 ： 0.38451241109704504\n",
      " 轮次  49  当前轮训练集损失 ： 0.3839925728117448\n",
      " 轮次  50  当前轮训练集损失 ： 0.38348321666166574\n",
      " 轮次  51  当前轮训练集损失 ： 0.38298392415931326\n",
      " 轮次  52  当前轮训练集损失 ： 0.3824943086601858\n",
      " 轮次  53  当前轮训练集损失 ： 0.38201401195901424\n",
      " 轮次  54  当前轮训练集损失 ： 0.3815427013094179\n",
      " 轮次  55  当前轮训练集损失 ： 0.3810800668090685\n",
      " 轮次  56  当前轮训练集损失 ： 0.3806258191010534\n",
      " 轮次  57  当前轮训练集损失 ： 0.38017968734933993\n",
      " 轮次  58  当前轮训练集损失 ： 0.37974141745232065\n",
      " 轮次  59  当前轮训练集损失 ： 0.37931077046354544\n",
      " 轮次  60  当前轮训练集损失 ： 0.37888752119308866\n",
      " 轮次  61  当前轮训练集损失 ： 0.37847145696667833\n",
      " 轮次  62  当前轮训练集损失 ： 0.37806237652285124\n",
      " 轮次  63  当前轮训练集损失 ： 0.37766008903106224\n",
      " 轮次  64  当前轮训练集损失 ： 0.37726441321596366\n",
      " 轮次  65  当前轮训练集损失 ： 0.3768751765750188\n",
      " 轮次  66  当前轮训练集损失 ： 0.3764922146782934\n",
      " 轮次  67  当前轮训练集损失 ： 0.376115370540709\n",
      " 轮次  68  当前轮训练集损失 ： 0.3757444940582837\n",
      " 轮次  69  当前轮训练集损失 ： 0.3753794415009563\n",
      " 轮次  70  当前轮训练集损失 ： 0.3750200750555198\n",
      " 轮次  71  当前轮训练集损失 ： 0.3746662624129878\n",
      " 轮次  72  当前轮训练集损失 ： 0.3743178763954168\n",
      " 轮次  73  当前轮训练集损失 ： 0.3739747946178111\n",
      " 轮次  74  当前轮训练集损失 ： 0.3736368991812635\n",
      " 轮次  75  当前轮训练集损失 ： 0.3733040763939438\n",
      " 轮次  76  当前轮训练集损失 ： 0.37297621651694624\n",
      " 轮次  77  当前轮训练集损失 ： 0.3726532135323596\n",
      " 轮次  78  当前轮训练集损失 ： 0.3723349649312272\n",
      " 轮次  79  当前轮训练集损失 ： 0.3720213715193332\n",
      " 轮次  80  当前轮训练集损失 ： 0.37171233723898817\n",
      " 轮次  81  当前轮训练集损失 ： 0.3714077690051914\n",
      " 轮次  82  当前轮训练集损失 ： 0.3711075765547321\n",
      " 轮次  83  当前轮训练集损失 ： 0.370811672306948\n",
      " 轮次  84  当前轮训练集损失 ： 0.3705199712350055\n",
      " 轮次  85  当前轮训练集损失 ： 0.37023239074668507\n",
      " 轮次  86  当前轮训练集损失 ： 0.3699488505737699\n",
      " 轮次  87  当前轮训练集损失 ： 0.3696692726692301\n",
      " 轮次  88  当前轮训练集损失 ： 0.36939358111148113\n",
      " 轮次  89  当前轮训练集损失 ： 0.36912170201507327\n",
      " 轮次  90  当前轮训练集损失 ： 0.368853563447233\n",
      " 轮次  91  当前轮训练集损失 ： 0.3685890953497415\n",
      " 轮次  92  当前轮训练集损失 ： 0.36832822946568305\n",
      " 轮次  93  当前轮训练集损失 ： 0.36807089927064895\n",
      " 轮次  94  当前轮训练集损失 ： 0.36781703990802117\n",
      " 轮次  95  当前轮训练集损失 ： 0.3675665881279991\n",
      " 轮次  96  当前轮训练集损失 ： 0.3673194822300634\n",
      " 轮次  97  当前轮训练集损失 ： 0.36707566200860553\n",
      " 轮次  98  当前轮训练集损失 ： 0.3668350687014736\n",
      " 轮次  99  当前轮训练集损失 ： 0.36659764494121017\n",
      " 轮次  100  当前轮训练集损失 ： 0.3663633347087809\n",
      " 轮次  101  当前轮训练集损失 ： 0.36613208328960883\n",
      " 轮次  102  当前轮训练集损失 ： 0.36590383723174813\n",
      " 轮次  103  当前轮训练集损失 ： 0.3656785443060462\n",
      " 轮次  104  当前轮训练集损失 ： 0.36545615346815524\n",
      " 轮次  105  当前轮训练集损失 ： 0.3652366148222683\n",
      " 轮次  106  当前轮训练集损失 ： 0.36501987958646587\n",
      " 轮次  107  当前轮训练集损失 ： 0.3648059000595668\n",
      " 轮次  108  当前轮训练集损失 ： 0.36459462958938954\n",
      " 轮次  109  当前轮训练集损失 ： 0.36438602254233515\n",
      " 轮次  110  当前轮训练集损失 ： 0.36418003427421114\n",
      " 轮次  111  当前轮训练集损失 ： 0.36397662110222384\n",
      " 轮次  112  当前轮训练集损失 ： 0.36377574027807014\n",
      " 轮次  113  当前轮训练集损失 ： 0.3635773499620657\n",
      " 轮次  114  当前轮训练集损失 ： 0.36338140919825407\n",
      " 轮次  115  当前轮训练集损失 ： 0.36318787789044044\n",
      " 轮次  116  当前轮训练集损失 ： 0.3629967167791031\n",
      " 轮次  117  当前轮训练集损失 ： 0.3628078874191355\n",
      " 轮次  118  当前轮训练集损失 ： 0.36262135215837676\n",
      " 轮次  119  当前轮训练集损失 ： 0.36243707411689247\n",
      " 轮次  120  当前轮训练集损失 ： 0.36225501716696673\n",
      " 轮次  121  当前轮训练集损失 ： 0.36207514591377365\n",
      " 轮次  122  当前轮训练集损失 ： 0.36189742567669486\n",
      " 轮次  123  当前轮训练集损失 ： 0.3617218224712538\n",
      " 轮次  124  当前轮训练集损失 ： 0.3615483029916388\n",
      " 轮次  125  当前轮训练集损失 ： 0.36137683459378966\n",
      " 轮次  126  当前轮训练集损失 ： 0.36120738527902124\n",
      " 轮次  127  当前轮训练集损失 ： 0.3610399236781626\n",
      " 轮次  128  当前轮训练集损失 ： 0.3608744190361909\n",
      " 轮次  129  当前轮训练集损失 ： 0.3607108411973361\n",
      " 轮次  130  当前轮训练集损失 ： 0.36054916059064174\n",
      " 轮次  131  当前轮训练集损失 ： 0.3603893482159602\n",
      " 轮次  132  当前轮训练集损失 ： 0.3602313756303665\n",
      " 轮次  133  当前轮训练集损失 ： 0.36007521493497524\n",
      " 轮次  134  当前轮训练集损失 ： 0.3599208387621432\n",
      " 轮次  135  当前轮训练集损失 ： 0.35976822026304545\n",
      " 轮次  136  当前轮训练集损失 ： 0.3596173330956088\n",
      " 轮次  137  当前轮训练集损失 ： 0.3594681514127914\n",
      " 轮次  138  当前轮训练集损失 ： 0.35932064985119444\n",
      " 轮次  139  当前轮训练集损失 ： 0.3591748035199951\n",
      " 轮次  140  当前轮训练集损失 ： 0.3590305879901887\n",
      " 轮次  141  当前轮训练集损失 ： 0.3588879792841295\n",
      " 轮次  142  当前轮训练集损失 ： 0.3587469538653602\n",
      " 轮次  143  当前轮训练集损失 ： 0.358607488628719\n",
      " 轮次  144  当前轮训练集损失 ： 0.3584695608907169\n",
      " 轮次  145  当前轮训练集损失 ： 0.3583331483801734\n",
      " 轮次  146  当前轮训练集损失 ： 0.3581982292291054\n",
      " 轮次  147  当前轮训练集损失 ： 0.35806478196385755\n",
      " 轮次  148  当前轮训练集损失 ： 0.3579327854964685\n",
      " 轮次  149  当前轮训练集损失 ： 0.35780221911626503\n",
      " 轮次  150  当前轮训练集损失 ： 0.35767306248167513\n",
      " 轮次  151  当前轮训练集损失 ： 0.35754529561225623\n",
      " 轮次  152  当前轮训练集损失 ： 0.35741889888092837\n",
      " 轮次  153  当前轮训练集损失 ： 0.3572938530064085\n",
      " 轮次  154  当前轮训练集损失 ： 0.35717013904583755\n",
      " 轮次  155  当前轮训练集损失 ： 0.357047738387597\n",
      " 轮次  156  当前轮训练集损失 ： 0.3569266327443063\n",
      " 轮次  157  当前轮训练集损失 ： 0.35680680414599697\n",
      " 轮次  158  当前轮训练集损失 ： 0.35668823493345925\n",
      " 轮次  159  当前轮训练集损失 ： 0.3565709077517534\n",
      " 轮次  160  当前轮训练集损失 ： 0.3564548055438833\n",
      " 轮次  161  当前轮训练集损失 ： 0.35633991154462613\n",
      " 轮次  162  当前轮训练集损失 ： 0.35622620927451376\n",
      " 轮次  163  当前轮训练集损失 ： 0.35611368253396203\n",
      " 轮次  164  当前轮训练集损失 ： 0.356002315397543\n",
      " 轮次  165  当前轮训练集损失 ： 0.35589209220839596\n",
      " 轮次  166  当前轮训练集损失 ： 0.35578299757277393\n",
      " 轮次  167  当前轮训练集损失 ： 0.3556750163547212\n",
      " 轮次  168  当前轮训练集损失 ： 0.35556813367087814\n",
      " 轮次  169  当前轮训练集损失 ： 0.35546233488541046\n",
      " 轮次  170  当前轮训练集损失 ： 0.35535760560505764\n",
      " 轮次  171  当前轮训练集损失 ： 0.3552539316743006\n",
      " 轮次  172  当前轮训练集损失 ： 0.3551512991706403\n",
      " 轮次  173  当前轮训练集损失 ： 0.35504969439998996\n",
      " 轮次  174  当前轮训练集损失 ： 0.35494910389217216\n",
      " 轮次  175  当前轮训练集损失 ： 0.35484951439652285\n",
      " 轮次  176  当前轮训练集损失 ： 0.35475091287759575\n",
      " 轮次  177  当前轮训练集损失 ： 0.3546532865109663\n",
      " 轮次  178  当前轮训练集损失 ： 0.3545566226791323\n",
      " 轮次  179  当前轮训练集损失 ： 0.3544609089675075\n",
      " 轮次  180  当前轮训练集损失 ： 0.3543661331605074\n",
      " 轮次  181  当前轮训练集损失 ： 0.35427228323772353\n",
      " 轮次  182  当前轮训练集损失 ： 0.3541793473701841\n",
      " 轮次  183  当前轮训练集损失 ： 0.3540873139166991\n",
      " 轮次  184  当前轮训练集损失 ： 0.3539961714202875\n",
      " 轮次  185  当前轮训练集损失 ： 0.35390590860468374\n",
      " 轮次  186  当前轮训练集损失 ： 0.3538165143709231\n",
      " 轮次  187  当前轮训练集损失 ： 0.35372797779400184\n",
      " 轮次  188  当前轮训练集损失 ： 0.3536402881196116\n",
      " 轮次  189  当前轮训练集损失 ： 0.35355343476094603\n",
      " 轮次  190  当前轮训练集损失 ： 0.35346740729557663\n",
      " 轮次  191  当前轮训练集损失 ： 0.3533821954623981\n",
      " 轮次  192  当前轮训练集损失 ： 0.3532977891586393\n",
      " 轮次  193  当前轮训练集损失 ： 0.35321417843693925\n",
      " 轮次  194  当前轮训练集损失 ： 0.3531313535024873\n",
      " 轮次  195  当前轮训练集损失 ： 0.3530493047102235\n",
      " 轮次  196  当前轮训练集损失 ： 0.3529680225621002\n",
      " 轮次  197  当前轮训练集损失 ： 0.35288749770440225\n",
      " 轮次  198  当前轮训练集损失 ： 0.35280772092512364\n",
      " 轮次  199  当前轮训练集损失 ： 0.3527286831514005\n",
      " 轮次  200  当前轮训练集损失 ： 0.3526503754469986\n",
      " 轮次  201  当前轮训练集损失 ： 0.3525727890098537\n",
      " 轮次  202  当前轮训练集损失 ： 0.3524959151696634\n",
      " 轮次  203  当前轮训练集损失 ： 0.35241974538553\n",
      " 轮次  204  当前轮训练集损失 ： 0.3523442712436526\n",
      " 轮次  205  当前轮训练集损失 ： 0.35226948445506656\n",
      " 轮次  206  当前轮训练集损失 ： 0.3521953768534315\n",
      " 轮次  207  当前轮训练集损失 ： 0.3521219403928632\n",
      " 轮次  208  当前轮训练集损失 ： 0.35204916714581147\n",
      " 轮次  209  当前轮训练集损失 ： 0.35197704930098167\n",
      " 轮次  210  当前轮训练集损失 ： 0.35190557916129794\n",
      " 轮次  211  当前轮训练集损失 ： 0.35183474914190926\n",
      " 轮次  212  当前轮训练集损失 ： 0.3517645517682351\n",
      " 轮次  213  当前轮训练集损失 ： 0.35169497967405167\n",
      " 轮次  214  当前轮训练集损失 ： 0.3516260255996158\n",
      " 轮次  215  当前轮训练集损失 ： 0.3515576823898282\n",
      " 轮次  216  当前轮训练集损失 ： 0.3514899429924321\n",
      " 轮次  217  当前轮训练集损失 ： 0.3514228004562489\n",
      " 轮次  218  当前轮训练集损失 ： 0.3513562479294496\n",
      " 轮次  219  当前轮训练集损失 ： 0.35129027865785895\n",
      " 轮次  220  当前轮训练集损失 ： 0.35122488598329554\n",
      " 轮次  221  当前轮训练集损失 ： 0.3511600633419427\n",
      " 轮次  222  当前轮训练集损失 ： 0.3510958042627532\n",
      " 轮次  223  当前轮训练集损失 ： 0.3510321023658845\n",
      " 轮次  224  当前轮训练集损失 ： 0.3509689513611648\n",
      " 轮次  225  当前轮训练集损失 ： 0.35090634504658946\n",
      " 轮次  226  当前轮训练集损失 ： 0.3508442773068463\n",
      " 轮次  227  当前轮训练集损失 ： 0.35078274211187044\n",
      " 轮次  228  当前轮训练集损失 ： 0.35072173351542596\n",
      " 轮次  229  当前轮训练集损失 ： 0.35066124565371626\n",
      " 轮次  230  当前轮训练集损失 ： 0.35060127274402025\n",
      " 轮次  231  当前轮训练集损失 ： 0.3505418090833554\n",
      " 轮次  232  当前轮训练集损失 ： 0.3504828490471662\n",
      " 轮次  233  当前轮训练集损失 ： 0.3504243870880375\n",
      " 轮次  234  当前轮训练集损失 ： 0.35036641773443317\n",
      " 轮次  235  当前轮训练集损失 ： 0.3503089355894574\n",
      " 轮次  236  当前轮训练集损失 ： 0.35025193532964133\n",
      " 轮次  237  当前轮训练集损失 ： 0.35019541170375046\n",
      " 轮次  238  当前轮训练集损失 ： 0.3501393595316167\n",
      " 轮次  239  当前轮训练集损失 ： 0.3500837737029906\n",
      " 轮次  240  当前轮训练集损失 ： 0.35002864917641674\n",
      " 轮次  241  当前轮训练集损失 ： 0.34997398097812826\n",
      " 轮次  242  当前轮训练集损失 ： 0.34991976420096393\n",
      " 轮次  243  当前轮训练集损失 ： 0.34986599400330426\n",
      " 轮次  244  当前轮训练集损失 ： 0.3498126656080274\n",
      " 轮次  245  当前轮训练集损失 ： 0.34975977430148436\n",
      " 轮次  246  当前轮训练集损失 ： 0.34970731543249395\n",
      " 轮次  247  当前轮训练集损失 ： 0.3496552844113553\n",
      " 轮次  248  当前轮训练集损失 ： 0.3496036767088783\n",
      " 轮次  249  当前轮训练集损失 ： 0.3495524878554329\n",
      " 轮次  250  当前轮训练集损失 ： 0.34950171344001485\n",
      " 轮次  251  当前轮训练集损失 ： 0.3494513491093288\n",
      " 轮次  252  当前轮训练集损失 ： 0.3494013905668878\n",
      " 轮次  253  当前轮训练集损失 ： 0.34935183357212973\n",
      " 轮次  254  当前轮训练集损失 ： 0.34930267393954867\n",
      " 轮次  255  当前轮训练集损失 ： 0.3492539075378428\n",
      " 轮次  256  当前轮训练集损失 ： 0.3492055302890773\n",
      " 轮次  257  当前轮训练集损失 ： 0.3491575381678623\n",
      " 轮次  258  当前轮训练集损失 ： 0.34910992720054523\n",
      " 轮次  259  当前轮训练集损失 ： 0.3490626934644181\n",
      " 轮次  260  当前轮训练集损失 ： 0.3490158330869382\n",
      " 轮次  261  当前轮训练集损失 ： 0.34896934224496373\n",
      " 轮次  262  当前轮训练集损失 ： 0.3489232171640013\n",
      " 轮次  263  当前轮训练集损失 ： 0.3488774541174685\n",
      " 轮次  264  当前轮训练集损失 ： 0.34883204942596807\n",
      " 轮次  265  当前轮训练集损失 ： 0.3487869994565754\n",
      " 轮次  266  当前轮训练集损失 ： 0.3487423006221386\n",
      " 轮次  267  当前轮训练集损失 ： 0.348697949380591\n",
      " 轮次  268  当前轮训练集损失 ： 0.3486539422342742\n",
      " 轮次  269  当前轮训练集损失 ： 0.3486102757292753\n",
      " 轮次  270  当前轮训练集损失 ： 0.3485669464547729\n",
      " 轮次  271  当前轮训练集损失 ： 0.3485239510423973\n",
      " 轮次  272  当前轮训练集损失 ： 0.348481286165599\n",
      " 轮次  273  当前轮训练集损失 ： 0.34843894853903007\n",
      " 轮次  274  当前轮训练集损失 ： 0.34839693491793494\n",
      " 轮次  275  当前轮训练集损失 ： 0.3483552420975522\n",
      " 轮次  276  当前轮训练集损失 ： 0.3483138669125267\n",
      " 轮次  277  当前轮训练集损失 ： 0.3482728062363311\n",
      " 轮次  278  当前轮训练集损失 ： 0.34823205698069803\n",
      " 轮次  279  当前轮训练集损失 ： 0.3481916160950612\n",
      " 轮次  280  当前轮训练集损失 ： 0.34815148056600603\n",
      " 轮次  281  当前轮训练集损失 ： 0.34811164741673034\n",
      " 轮次  282  当前轮训练集损失 ： 0.3480721137065132\n",
      " 轮次  283  当前轮训练集损失 ： 0.3480328765301928\n",
      " 轮次  284  当前轮训练集损失 ： 0.347993933017654\n",
      " 轮次  285  当前轮训练集损失 ： 0.3479552803333231\n",
      " 轮次  286  当前轮训练集损失 ： 0.34791691567567207\n",
      " 轮次  287  当前轮训练集损失 ： 0.3478788362767308\n",
      " 轮次  288  当前轮训练集损失 ： 0.3478410394016068\n",
      " 轮次  289  当前轮训练集损失 ： 0.3478035223480139\n",
      " 轮次  290  当前轮训练集损失 ： 0.347766282445808\n",
      " 轮次  291  当前轮训练集损失 ： 0.3477293170565307\n",
      " 轮次  292  当前轮训练集损失 ： 0.34769262357296044\n",
      " 轮次  293  当前轮训练集损失 ： 0.34765619941867115\n",
      " 轮次  294  当前轮训练集损失 ： 0.3476200420475978\n",
      " 轮次  295  当前轮训练集损失 ： 0.347584148943609\n",
      " 轮次  296  当前轮训练集损失 ： 0.34754851762008765\n",
      " 轮次  297  当前轮训练集损失 ： 0.3475131456195166\n",
      " 轮次  298  当前轮训练集损失 ： 0.3474780305130724\n",
      " 轮次  299  当前轮训练集损失 ： 0.34744316990022517\n",
      " 轮次  300  当前轮训练集损失 ： 0.3474085614083452\n",
      " 轮次  301  当前轮训练集损失 ： 0.34737420269231545\n",
      " 轮次  302  当前轮训练集损失 ： 0.34734009143415023\n",
      " 轮次  303  当前轮训练集损失 ： 0.347306225342621\n",
      " 轮次  304  当前轮训练集损失 ： 0.34727260215288697\n",
      " 轮次  305  当前轮训练集损失 ： 0.34723921962613163\n",
      " 轮次  306  当前轮训练集损失 ： 0.3472060755492065\n",
      " 轮次  307  当前轮训练集损失 ： 0.34717316773427903\n",
      " 轮次  308  当前轮训练集损失 ： 0.3471404940184863\n",
      " 轮次  309  当前轮训练集损失 ： 0.34710805226359503\n",
      " 轮次  310  当前轮训练集损失 ： 0.34707584035566624\n",
      " 轮次  311  当前轮训练集损失 ： 0.34704385620472555\n",
      " 轮次  312  当前轮训练集损失 ： 0.34701209774443814\n",
      " 轮次  313  当前轮训练集损失 ： 0.34698056293178964\n",
      " 轮次  314  当前轮训练集损失 ： 0.3469492497467713\n",
      " 轮次  315  当前轮训练集损失 ： 0.34691815619207045\n",
      " 轮次  316  当前轮训练集损失 ： 0.34688728029276555\n",
      " 轮次  317  当前轮训练集损失 ： 0.34685662009602647\n",
      " 轮次  318  当前轮训练集损失 ： 0.3468261736708184\n",
      " 轮次  319  当前轮训练集损失 ： 0.346795939107612\n",
      " 轮次  320  当前轮训练集损失 ： 0.346765914518096\n",
      " 轮次  321  当前轮训练集损失 ： 0.34673609803489613\n",
      " 轮次  322  当前轮训练集损失 ： 0.3467064878112971\n",
      " 轮次  323  当前轮训练集损失 ： 0.34667708202096925\n",
      " 轮次  324  当前轮训练集损失 ： 0.3466478788576998\n",
      " 轮次  325  当前轮训练集损失 ： 0.3466188765351273\n",
      " 轮次  326  当前轮训练集损失 ： 0.3465900732864813\n",
      " 轮次  327  当前轮训练集损失 ： 0.3465614673643247\n",
      " 轮次  328  当前轮训练集损失 ： 0.34653305704030124\n",
      " 轮次  329  当前轮训练集损失 ： 0.34650484060488584\n",
      " 轮次  330  当前轮训练集损失 ： 0.34647681636713945\n",
      " 轮次  331  当前轮训练集损失 ： 0.3464489826544669\n",
      " 轮次  332  当前轮训练集损失 ： 0.34642133781237916\n",
      " 轮次  333  当前轮训练集损失 ： 0.34639388020425876\n",
      " 轮次  334  当前轮训练集损失 ： 0.34636660821112847\n",
      " 轮次  335  当前轮训练集损失 ： 0.34633952023142417\n",
      " 轮次  336  当前轮训练集损失 ： 0.3463126146807702\n",
      " 轮次  337  当前轮训练集损失 ： 0.34628588999175886\n",
      " 轮次  338  当前轮训练集损失 ： 0.3462593446137333\n",
      " 轮次  339  当前轮训练集损失 ： 0.34623297701257216\n",
      " 轮次  340  当前轮训练集损失 ： 0.34620678567047963\n",
      " 轮次  341  当前轮训练集损失 ： 0.34618076908577655\n",
      " 轮次  342  当前轮训练集损失 ： 0.3461549257726961\n",
      " 轮次  343  当前轮训练集损失 ： 0.34612925426118146\n",
      " 轮次  344  当前轮训练集损失 ： 0.34610375309668734\n",
      " 轮次  345  当前轮训练集损失 ： 0.3460784208399837\n",
      " 轮次  346  当前轮训练集损失 ： 0.34605325606696236\n",
      " 轮次  347  当前轮训练集损失 ： 0.34602825736844733\n",
      " 轮次  348  当前轮训练集损失 ： 0.34600342335000706\n",
      " 轮次  349  当前轮训练集损失 ： 0.34597875263176875\n",
      " 轮次  350  当前轮训练集损失 ： 0.345954243848238\n",
      " 轮次  351  当前轮训练集损失 ： 0.34592989564811755\n",
      " 轮次  352  当前轮训练集损失 ： 0.34590570669413173\n",
      " 轮次  353  当前轮训练集损失 ： 0.34588167566285116\n",
      " 轮次  354  当前轮训练集损失 ： 0.3458578012445218\n",
      " 轮次  355  当前轮训练集损失 ： 0.3458340821428947\n",
      " 轮次  356  当前轮训练集损失 ： 0.3458105170750602\n",
      " 轮次  357  当前轮训练集损失 ： 0.34578710477128216\n",
      " 轮次  358  当前轮训练集损失 ： 0.34576384397483695\n",
      " 轮次  359  当前轮训练集损失 ： 0.345740733441853\n",
      " 轮次  360  当前轮训练集损失 ： 0.3457177719411534\n",
      " 轮次  361  当前轮训练集损失 ： 0.3456949582541001\n",
      " 轮次  362  当前轮训练集损失 ： 0.34567229117444176\n",
      " 轮次  363  当前轮训练集损失 ： 0.345649769508162\n",
      " 轮次  364  当前轮训练集损失 ： 0.34562739207333093\n",
      " 轮次  365  当前轮训练集损失 ： 0.3456051576999583\n",
      " 轮次  366  当前轮训练集损失 ： 0.3455830652298489\n",
      " 轮次  367  当前轮训练集损失 ： 0.34556111351645996\n",
      " 轮次  368  当前轮训练集损失 ： 0.34553930142476064\n",
      " 轮次  369  当前轮训练集损失 ： 0.3455176278310931\n",
      " 轮次  370  当前轮训练集损失 ： 0.3454960916230359\n",
      " 轮次  371  当前轮训练集损失 ： 0.34547469169926953\n",
      " 轮次  372  当前轮训练集损失 ： 0.3454534269694433\n",
      " 轮次  373  当前轮训练集损失 ： 0.34543229635404366\n",
      " 轮次  374  当前轮训练集损失 ： 0.34541129878426585\n",
      " 轮次  375  当前轮训练集损失 ： 0.3453904332018862\n",
      " 轮次  376  当前轮训练集损失 ： 0.34536969855913596\n",
      " 轮次  377  当前轮训练集损失 ： 0.3453490938185778\n",
      " 轮次  378  当前轮训练集损失 ： 0.34532861795298336\n",
      " 轮次  379  当前轮训练集损失 ： 0.3453082699452128\n",
      " 轮次  380  当前轮训练集损失 ： 0.3452880487880956\n",
      " 轮次  381  当前轮训练集损失 ： 0.34526795348431344\n",
      " 轮次  382  当前轮训练集损失 ： 0.34524798304628496\n",
      " 轮次  383  当前轮训练集损失 ： 0.3452281364960504\n",
      " 轮次  384  当前轮训练集损失 ： 0.34520841286516035\n",
      " 轮次  385  当前轮训练集损失 ： 0.34518881119456396\n",
      " 轮次  386  当前轮训练集损失 ： 0.3451693305344998\n",
      " 轮次  387  当前轮训练集损失 ： 0.3451499699443872\n",
      " 轮次  388  当前轮训练集损失 ： 0.3451307284927207\n",
      " 轮次  389  当前轮训练集损失 ： 0.34511160525696355\n",
      " 轮次  390  当前轮训练集损失 ： 0.3450925993234455\n",
      " 轮次  391  当前轮训练集损失 ： 0.34507370978725904\n",
      " 轮次  392  当前轮训练集损失 ： 0.3450549357521594\n",
      " 轮次  393  当前轮训练集损失 ： 0.34503627633046446\n",
      " 轮次  394  当前轮训练集损失 ： 0.3450177306429567\n",
      " 轮次  395  当前轮训练集损失 ： 0.34499929781878585\n",
      " 轮次  396  当前轮训练集损失 ： 0.3449809769953739\n",
      " 轮次  397  当前轮训练集损失 ： 0.34496276731832\n",
      " 轮次  398  当前轮训练集损失 ： 0.34494466794130757\n",
      " 轮次  399  当前轮训练集损失 ： 0.34492667802601246\n",
      " 轮次  400  当前轮训练集损失 ： 0.34490879674201225\n",
      " 轮次  401  当前轮训练集损失 ： 0.3448910232666964\n",
      " 轮次  402  当前轮训练集损失 ： 0.34487335678517833\n",
      " 轮次  403  当前轮训练集损失 ： 0.3448557964902082\n",
      " 轮次  404  当前轮训练集损失 ： 0.34483834158208654\n",
      " 轮次  405  当前轮训练集损失 ： 0.3448209912685796\n",
      " 轮次  406  当前轮训练集损失 ： 0.3448037447648358\n",
      " 轮次  407  当前轮训练集损失 ： 0.3447866012933024\n",
      " 轮次  408  当前轮训练集损失 ： 0.34476956008364457\n",
      " 轮次  409  当前轮训练集损失 ： 0.34475262037266446\n",
      " 轮次  410  当前轮训练集损失 ： 0.3447357814042217\n",
      " 轮次  411  当前轮训练集损失 ： 0.3447190424291548\n",
      " 轮次  412  当前轮训练集损失 ： 0.3447024027052043\n",
      " 轮次  413  当前轮训练集损失 ： 0.34468586149693536\n",
      " 轮次  414  当前轮训练集损失 ： 0.34466941807566315\n",
      " 轮次  415  当前轮训练集损失 ： 0.344653071719378\n",
      " 轮次  416  当前轮训练集损失 ： 0.3446368217126714\n",
      " 轮次  417  当前轮训练集损失 ： 0.3446206673466645\n",
      " 轮次  418  当前轮训练集损失 ： 0.34460460791893516\n",
      " 轮次  419  当前轮训练集损失 ： 0.344588642733448\n",
      " 轮次  420  当前轮训练集损失 ： 0.3445727711004846\n",
      " 轮次  421  当前轮训练集损失 ： 0.344556992336574\n",
      " 轮次  422  当前轮训练集损失 ： 0.3445413057644249\n",
      " 轮次  423  当前轮训练集损失 ： 0.3445257107128584\n",
      " 轮次  424  当前轮训练集损失 ： 0.34451020651674186\n",
      " 轮次  425  当前轮训练集损失 ： 0.3444947925169229\n",
      " 轮次  426  当前轮训练集损失 ： 0.34447946806016505\n",
      " 轮次  427  当前轮训练集损失 ： 0.3444642324990839\n",
      " 轮次  428  当前轮训练集损失 ： 0.3444490851920835\n",
      " 轮次  429  当前轮训练集损失 ： 0.34443402550329477\n",
      " 轮次  430  当前轮训练集损失 ： 0.3444190528025135\n",
      " 轮次  431  当前轮训练集损失 ： 0.3444041664651397\n",
      " 轮次  432  当前轮训练集损失 ： 0.3443893658721179\n",
      " 轮次  433  当前轮训练集损失 ： 0.3443746504098774\n",
      " 轮次  434  当前轮训练集损失 ： 0.34436001947027417\n",
      " 轮次  435  当前轮训练集损失 ： 0.34434547245053304\n",
      " 轮次  436  当前轮训练集损失 ： 0.3443310087531904\n",
      " 轮次  437  当前轮训练集损失 ： 0.34431662778603817\n",
      " 轮次  438  当前轮训练集损失 ： 0.34430232896206797\n",
      " 轮次  439  当前轮训练集损失 ： 0.34428811169941614\n",
      " 轮次  440  当前轮训练集损失 ： 0.3442739754213094\n",
      " 轮次  441  当前轮训练集损失 ： 0.34425991955601154\n",
      " 轮次  442  当前轮训练集损失 ： 0.3442459435367702\n",
      " 轮次  443  当前轮训练集损失 ： 0.34423204680176434\n",
      " 轮次  444  当前轮训练集损失 ： 0.3442182287940532\n",
      " 轮次  445  当前轮训练集损失 ： 0.3442044889615248\n",
      " 轮次  446  当前轮训练集损失 ： 0.34419082675684554\n",
      " 轮次  447  当前轮训练集损失 ： 0.3441772416374104\n",
      " 轮次  448  当前轮训练集损失 ： 0.3441637330652941\n",
      " 轮次  449  当前轮训练集损失 ： 0.34415030050720213\n",
      " 轮次  450  当前轮训练集损失 ： 0.3441369434344227\n",
      " 轮次  451  当前轮训练集损失 ： 0.34412366132277955\n",
      " 轮次  452  当前轮训练集损失 ： 0.3441104536525849\n",
      " 轮次  453  当前轮训练集损失 ： 0.34409731990859355\n",
      " 轮次  454  当前轮训练集损失 ： 0.34408425957995664\n",
      " 轮次  455  当前轮训练集损失 ： 0.3440712721601764\n",
      " 轮次  456  当前轮训练集损失 ： 0.34405835714706245\n",
      " 轮次  457  当前轮训练集损失 ： 0.34404551404268646\n",
      " 轮次  458  当前轮训练集损失 ： 0.34403274235333936\n",
      " 轮次  459  当前轮训练集损失 ： 0.34402004158948807\n",
      " 轮次  460  当前轮训练集损失 ： 0.3440074112657329\n",
      " 轮次  461  当前轮训练集损失 ： 0.3439948509007656\n",
      " 轮次  462  当前轮训练集损失 ： 0.3439823600173276\n",
      " 轮次  463  当前轮训练集损失 ： 0.34396993814216953\n",
      " 轮次  464  当前轮训练集损失 ： 0.3439575848060099\n",
      " 轮次  465  当前轮训练集损失 ： 0.34394529954349545\n",
      " 轮次  466  当前轮训练集损失 ： 0.34393308189316163\n",
      " 轮次  467  当前轮训练集损失 ： 0.34392093139739316\n",
      " 轮次  468  当前轮训练集损失 ： 0.34390884760238555\n",
      " 轮次  469  当前轮训练集损失 ： 0.34389683005810684\n",
      " 轮次  470  当前轮训练集损失 ： 0.34388487831825965\n",
      " 轮次  471  当前轮训练集损失 ： 0.34387299194024423\n",
      " 轮次  472  当前轮训练集损失 ： 0.3438611704851212\n",
      " 轮次  473  当前轮训练集损失 ： 0.3438494135175749\n",
      " 轮次  474  当前轮训练集损失 ： 0.34383772060587836\n",
      " 轮次  475  当前轮训练集损失 ： 0.34382609132185626\n",
      " 轮次  476  当前轮训练集损失 ： 0.34381452524085054\n",
      " 轮次  477  当前轮训练集损失 ： 0.3438030219416859\n",
      " 轮次  478  当前轮训练集损失 ： 0.3437915810066345\n",
      " 轮次  479  当前轮训练集损失 ： 0.34378020202138265\n",
      " 轮次  480  当前轮训练集损失 ： 0.34376888457499677\n",
      " 轮次  481  当前轮训练集损失 ： 0.34375762825989065\n",
      " 轮次  482  当前轮训练集损失 ： 0.3437464326717921\n",
      " 轮次  483  当前轮训练集损失 ： 0.3437352974097108\n",
      " 轮次  484  当前轮训练集损失 ： 0.34372422207590614\n",
      " 轮次  485  当前轮训练集损失 ： 0.3437132062758554\n",
      " 轮次  486  当前轮训练集损失 ： 0.34370224961822227\n",
      " 轮次  487  当前轮训练集损失 ： 0.34369135171482645\n",
      " 轮次  488  当前轮训练集损失 ： 0.3436805121806124\n",
      " 轮次  489  当前轮训练集损失 ： 0.343669730633619\n",
      " 轮次  490  当前轮训练集损失 ： 0.34365900669495\n",
      " 轮次  491  当前轮训练集损失 ： 0.3436483399887441\n",
      " 轮次  492  当前轮训练集损失 ： 0.3436377301421459\n",
      " 轮次  493  当前轮训练集损失 ： 0.3436271767852766\n",
      " 轮次  494  当前轮训练集损失 ： 0.3436166795512058\n",
      " 轮次  495  当前轮训练集损失 ： 0.34360623807592283\n",
      " 轮次  496  当前轮训练集损失 ： 0.34359585199830883\n",
      " 轮次  497  当前轮训练集损失 ： 0.3435855209601094\n",
      " 轮次  498  当前轮训练集损失 ： 0.34357524460590666\n",
      " 轮次  499  当前轮训练集损失 ： 0.3435650225830928\n",
      " 轮次  500  当前轮训练集损失 ： 0.34355485454184287\n",
      " 轮次  501  当前轮训练集损失 ： 0.3435447401350885\n",
      " 轮次  502  当前轮训练集损失 ： 0.34353467901849194\n",
      " 轮次  503  当前轮训练集损失 ： 0.3435246708504197\n",
      " 轮次  504  当前轮训练集损失 ： 0.34351471529191724\n",
      " 轮次  505  当前轮训练集损失 ： 0.3435048120066842\n",
      " 轮次  506  当前轮训练集损失 ： 0.34349496066104807\n",
      " 轮次  507  当前轮训练集损失 ： 0.34348516092394066\n",
      " 轮次  508  当前轮训练集损失 ： 0.3434754124668733\n",
      " 轮次  509  当前轮训练集损失 ： 0.3434657149639124\n",
      " 轮次  510  当前轮训练集损失 ： 0.3434560680916556\n",
      " 轮次  511  当前轮训练集损失 ： 0.3434464715292081\n",
      " 轮次  512  当前轮训练集损失 ： 0.3434369249581594\n",
      " 轮次  513  当前轮训练集损失 ： 0.3434274280625603\n",
      " 轮次  514  当前轮训练集损失 ： 0.34341798052889916\n",
      " 轮次  515  当前轮训练集损失 ： 0.3434085820460804\n",
      " 轮次  516  当前轮训练集损失 ： 0.34339923230540104\n",
      " 轮次  517  当前轮训练集损失 ： 0.3433899310005292\n",
      " 轮次  518  当前轮训练集损失 ： 0.3433806778274822\n",
      " 轮次  519  当前轮训练集损失 ： 0.3433714724846045\n",
      " 轮次  520  当前轮训练集损失 ： 0.343362314672546\n",
      " 轮次  521  当前轮训练集损失 ： 0.3433532040942422\n",
      " 轮次  522  当前轮训练集损失 ： 0.3433441404548912\n",
      " 轮次  523  当前轮训练集损失 ： 0.3433351234619347\n",
      " 轮次  524  当前轮训练集损失 ： 0.3433261528250364\n",
      " 轮次  525  当前轮训练集损失 ： 0.3433172282560622\n",
      " 轮次  526  当前轮训练集损失 ： 0.3433083494690598\n",
      " 轮次  527  当前轮训练集损失 ： 0.3432995161802391\n",
      " 轮次  528  当前轮训练集损失 ： 0.3432907281079521\n",
      " 轮次  529  当前轮训练集损失 ： 0.343281984972674\n",
      " 轮次  530  当前轮训练集损失 ： 0.34327328649698335\n",
      " 轮次  531  当前轮训练集损失 ： 0.3432646324055436\n",
      " 轮次  532  当前轮训练集损失 ： 0.34325602242508374\n",
      " 轮次  533  当前轮训练集损失 ： 0.34324745628438\n",
      " 轮次  534  当前轮训练集损失 ： 0.34323893371423725\n",
      " 轮次  535  当前轮训练集损失 ： 0.34323045444747036\n",
      " 轮次  536  当前轮训练集损失 ： 0.34322201821888715\n",
      " 轮次  537  当前轮训练集损失 ： 0.34321362476526945\n",
      " 轮次  538  当前轮训练集损失 ： 0.34320527382535626\n",
      " 轮次  539  当前轮训练集损失 ： 0.3431969651398254\n",
      " 轮次  540  当前轮训练集损失 ： 0.3431886984512769\n",
      " 轮次  541  当前轮训练集损失 ： 0.3431804735042158\n",
      " 轮次  542  当前轮训练集损失 ： 0.34317229004503463\n",
      " 轮次  543  当前轮训练集损失 ： 0.3431641478219973\n",
      " 轮次  544  当前轮训练集损失 ： 0.3431560465852222\n",
      " 轮次  545  当前轮训练集损失 ： 0.3431479860866655\n",
      " 轮次  546  当前轮训练集损失 ： 0.3431399660801059\n",
      " 轮次  547  当前轮训练集损失 ： 0.34313198632112674\n",
      " 轮次  548  当前轮训练集损失 ： 0.34312404656710227\n",
      " 轮次  549  当前轮训练集损失 ： 0.34311614657718004\n",
      " 轮次  550  当前轮训练集损失 ： 0.3431082861122665\n",
      " 轮次  551  当前轮训练集损失 ： 0.34310046493501073\n",
      " 轮次  552  当前轮训练集损失 ： 0.34309268280978983\n",
      " 轮次  553  当前轮训练集损失 ： 0.34308493950269375\n",
      " 轮次  554  当前轮训练集损失 ： 0.3430772347815096\n",
      " 轮次  555  当前轮训练集损失 ： 0.3430695684157075\n",
      " 轮次  556  当前轮训练集损失 ： 0.3430619401764262\n",
      " 轮次  557  当前轮训练集损失 ： 0.3430543498364577\n",
      " 轮次  558  当前轮训练集损失 ： 0.34304679717023345\n",
      " 轮次  559  当前轮训练集损失 ： 0.34303928195381017\n",
      " 轮次  560  当前轮训练集损失 ： 0.3430318039648555\n",
      " 轮次  561  当前轮训练集损失 ： 0.3430243629826344\n",
      " 轮次  562  当前轮训练集损失 ： 0.34301695878799493\n",
      " 轮次  563  当前轮训练集损失 ： 0.3430095911633553\n",
      " 轮次  564  当前轮训练集损失 ： 0.3430022598926894\n",
      " 轮次  565  当前轮训练集损失 ： 0.34299496476151436\n",
      " 轮次  566  当前轮训练集损失 ： 0.3429877055568771\n",
      " 轮次  567  当前轮训练集损失 ： 0.34298048206734044\n",
      " 轮次  568  当前轮训练集损失 ： 0.3429732940829713\n",
      " 轮次  569  当前轮训练集损失 ： 0.3429661413953271\n",
      " 轮次  570  当前轮训练集损失 ： 0.3429590237974434\n",
      " 轮次  571  当前轮训练集损失 ： 0.3429519410838212\n",
      " 轮次  572  当前轮训练集损失 ： 0.34294489305041415\n",
      " 轮次  573  当前轮训练集损失 ： 0.34293787949461685\n",
      " 轮次  574  当前轮训练集损失 ： 0.3429309002152524\n",
      " 轮次  575  当前轮训练集损失 ： 0.34292395501256007\n",
      " 轮次  576  当前轮训练集损失 ： 0.3429170436881837\n",
      " 轮次  577  当前轮训练集损失 ： 0.34291016604515945\n",
      " 轮次  578  当前轮训练集损失 ： 0.34290332188790446\n",
      " 轮次  579  当前轮训练集损失 ： 0.34289651102220503\n",
      " 轮次  580  当前轮训练集损失 ： 0.3428897332552052\n",
      " 轮次  581  当前轮训练集损失 ： 0.3428829883953954\n",
      " 轮次  582  当前轮训练集损失 ： 0.34287627625260075\n",
      " 轮次  583  当前轮训练集损失 ： 0.3428695966379708\n",
      " 轮次  584  当前轮训练集损失 ： 0.34286294936396755\n",
      " 轮次  585  当前轮训练集损失 ： 0.34285633424435497\n",
      " 轮次  586  当前轮训练集损失 ： 0.34284975109418825\n",
      " 轮次  587  当前轮训练集损失 ： 0.34284319972980265\n",
      " 轮次  588  当前轮训练集损失 ： 0.3428366799688035\n",
      " 轮次  589  当前轮训练集损失 ： 0.3428301916300549\n",
      " 轮次  590  当前轮训练集损失 ： 0.34282373453366993\n",
      " 轮次  591  当前轮训练集损失 ： 0.3428173085010002\n",
      " 轮次  592  当前轮训练集损失 ： 0.34281091335462544\n",
      " 轮次  593  当前轮训练集损失 ： 0.3428045489183434\n",
      " 轮次  594  当前轮训练集损失 ： 0.3427982150171602\n",
      " 轮次  595  当前轮训练集损失 ： 0.34279191147727983\n",
      " 轮次  596  当前轮训练集损失 ： 0.3427856381260949\n",
      " 轮次  597  当前轮训练集损失 ： 0.34277939479217645\n",
      " 轮次  598  当前轮训练集损失 ： 0.3427731813052646\n",
      " 轮次  599  当前轮训练集损失 ： 0.3427669974962588\n",
      " 轮次  600  当前轮训练集损失 ： 0.3427608431972086\n",
      " 轮次  601  当前轮训练集损失 ： 0.34275471824130416\n",
      " 轮次  602  当前轮训练集损失 ： 0.3427486224628668\n",
      " 轮次  603  当前轮训练集损失 ： 0.34274255569734025\n",
      " 轮次  604  当前轮训练集损失 ： 0.34273651778128106\n",
      " 轮次  605  当前轮训练集损失 ： 0.3427305085523501\n",
      " 轮次  606  当前轮训练集损失 ： 0.3427245278493032\n",
      " 轮次  607  当前轮训练集损失 ： 0.3427185755119824\n",
      " 轮次  608  当前轮训练集损失 ： 0.34271265138130735\n",
      " 轮次  609  当前轮训练集损失 ： 0.34270675529926686\n",
      " 轮次  610  当前轮训练集损失 ： 0.3427008871089094\n",
      " 轮次  611  当前轮训练集损失 ： 0.3426950466543357\n",
      " 轮次  612  当前轮训练集损失 ： 0.3426892337806897\n",
      " 轮次  613  当前轮训练集损失 ： 0.3426834483341501\n",
      " 轮次  614  当前轮训练集损失 ： 0.34267769016192273\n",
      " 轮次  615  当前轮训练集损失 ： 0.3426719591122315\n",
      " 轮次  616  当前轮训练集损失 ： 0.34266625503431103\n",
      " 轮次  617  当前轮训练集损失 ： 0.34266057777839815\n",
      " 轮次  618  当前轮训练集损失 ： 0.3426549271957242\n",
      " 轮次  619  当前轮训练集损失 ： 0.34264930313850694\n",
      " 轮次  620  当前轮训练集损失 ： 0.3426437054599428\n",
      " 轮次  621  当前轮训练集损失 ： 0.34263813401419935\n",
      " 轮次  622  当前轮训练集损失 ： 0.34263258865640717\n",
      " 轮次  623  当前轮训练集损失 ： 0.3426270692426526\n",
      " 轮次  624  当前轮训练集损失 ： 0.3426215756299704\n",
      " 轮次  625  当前轮训练集损失 ： 0.34261610767633544\n",
      " 轮次  626  当前轮训练集损失 ： 0.3426106652406566\n",
      " 轮次  627  当前轮训练集损失 ： 0.34260524818276816\n",
      " 轮次  628  当前轮训练集损失 ： 0.3425998563634234\n",
      " 轮次  629  当前轮训练集损失 ： 0.34259448964428707\n",
      " 轮次  630  当前轮训练集损失 ： 0.3425891478879282\n",
      " 轮次  631  当前轮训练集损失 ： 0.3425838309578136\n",
      " 轮次  632  当前轮训练集损失 ： 0.3425785387182998\n",
      " 轮次  633  当前轮训练集损失 ： 0.34257327103462726\n",
      " 轮次  634  当前轮训练集损失 ： 0.34256802777291295\n",
      " 轮次  635  当前轮训练集损失 ： 0.34256280880014334\n",
      " 轮次  636  当前轮训练集损失 ： 0.34255761398416823\n",
      " 轮次  637  当前轮训练集损失 ： 0.34255244319369343\n",
      " 轮次  638  当前轮训练集损失 ： 0.342547296298275\n",
      " 轮次  639  当前轮训练集损失 ： 0.3425421731683118\n",
      " 轮次  640  当前轮训练集损失 ： 0.3425370736750394\n",
      " 轮次  641  当前轮训练集损失 ： 0.3425319976905237\n",
      " 轮次  642  当前轮训练集损失 ： 0.34252694508765436\n",
      " 轮次  643  当前轮训练集损失 ： 0.34252191574013874\n",
      " 轮次  644  当前轮训练集损失 ： 0.3425169095224954\n",
      " 轮次  645  当前轮训练集损失 ： 0.3425119263100479\n",
      " 轮次  646  当前轮训练集损失 ： 0.3425069659789187\n",
      " 轮次  647  当前轮训练集损失 ： 0.3425020284060233\n",
      " 轮次  648  当前轮训练集损失 ： 0.3424971134690637\n",
      " 轮次  649  当前轮训练集损失 ： 0.34249222104652277\n",
      " 轮次  650  当前轮训练集损失 ： 0.3424873510176585\n",
      " 轮次  651  当前轮训练集损失 ： 0.3424825032624973\n",
      " 轮次  652  当前轮训练集损失 ： 0.3424776776618291\n",
      " 轮次  653  当前轮训练集损失 ： 0.3424728740972013\n",
      " 轮次  654  当前轮训练集损失 ： 0.34246809245091253\n",
      " 轮次  655  当前轮训练集损失 ： 0.34246333260600764\n",
      " 轮次  656  当前轮训练集损失 ： 0.3424585944462718\n",
      " 轮次  657  当前轮训练集损失 ： 0.34245387785622505\n",
      " 轮次  658  当前轮训练集损失 ： 0.3424491827211162\n",
      " 轮次  659  当前轮训练集损失 ： 0.3424445089269184\n",
      " 轮次  660  当前轮训练集损失 ： 0.3424398563603228\n",
      " 轮次  661  当前轮训练集损失 ： 0.3424352249087335\n",
      " 轮次  662  当前轮训练集损失 ： 0.34243061446026235\n",
      " 轮次  663  当前轮训练集损失 ： 0.34242602490372337\n",
      " 轮次  664  当前轮训练集损失 ： 0.3424214561286279\n",
      " 轮次  665  当前轮训练集损失 ： 0.3424169080251788\n",
      " 轮次  666  当前轮训练集损失 ： 0.3424123804842662\n",
      " 轮次  667  当前轮训练集损失 ： 0.3424078733974614\n",
      " 轮次  668  当前轮训练集损失 ： 0.3424033866570125\n",
      " 轮次  669  当前轮训练集损失 ： 0.3423989201558391\n",
      " 轮次  670  当前轮训练集损失 ： 0.3423944737875275\n",
      " 轮次  671  当前轮训练集损失 ： 0.34239004744632556\n",
      " 轮次  672  当前轮训练集损失 ： 0.34238564102713814\n",
      " 轮次  673  当前轮训练集损失 ： 0.3423812544255216\n",
      " 轮次  674  当前轮训练集损失 ： 0.34237688753768003\n",
      " 轮次  675  当前轮训练集损失 ： 0.34237254026045927\n",
      " 轮次  676  当前轮训练集损失 ： 0.3423682124913435\n",
      " 轮次  677  当前轮训练集损失 ： 0.3423639041284493\n",
      " 轮次  678  当前轮训练集损失 ： 0.3423596150705221\n",
      " 轮次  679  当前轮训练集损失 ： 0.34235534521693073\n",
      " 轮次  680  当前轮训练集损失 ： 0.3423510944676633\n",
      " 轮次  681  当前轮训练集损失 ： 0.34234686272332265\n",
      " 轮次  682  当前轮训练集损失 ： 0.34234264988512186\n",
      " 轮次  683  当前轮训练集损失 ： 0.34233845585487976\n",
      " 轮次  684  当前轮训练集损失 ： 0.3423342805350165\n",
      " 轮次  685  当前轮训练集损失 ： 0.34233012382854927\n",
      " 轮次  686  当前轮训练集损失 ： 0.3423259856390878\n",
      " 轮次  687  当前轮训练集损失 ： 0.3423218658708303\n",
      " 轮次  688  当前轮训练集损失 ： 0.3423177644285592\n",
      " 轮次  689  当前轮训练集损失 ： 0.3423136812176366\n",
      " 轮次  690  当前轮训练集损失 ： 0.3423096161440006\n",
      " 轮次  691  当前轮训练集损失 ： 0.3423055691141605\n",
      " 轮次  692  当前轮训练集损失 ： 0.34230154003519336\n",
      " 轮次  693  当前轮训练集损失 ： 0.3422975288147396\n",
      " 轮次  694  当前轮训练集损失 ： 0.3422935353609989\n",
      " 轮次  695  当前轮训练集损失 ： 0.3422895595827262\n",
      " 轮次  696  当前轮训练集损失 ： 0.3422856013892279\n",
      " 轮次  697  当前轮训练集损失 ： 0.3422816606903579\n",
      " 轮次  698  当前轮训练集损失 ： 0.3422777373965134\n",
      " 轮次  699  当前轮训练集损失 ： 0.3422738314186313\n",
      " 轮次  700  当前轮训练集损失 ： 0.34226994266818433\n",
      " 轮次  701  当前轮训练集损失 ： 0.3422660710571771\n",
      " 轮次  702  当前轮训练集损失 ： 0.3422622164981426\n",
      " 轮次  703  当前轮训练集损失 ： 0.3422583789041381\n",
      " 轮次  704  当前轮训练集损失 ： 0.3422545581887417\n",
      " 轮次  705  当前轮训练集损失 ： 0.3422507542660483\n",
      " 轮次  706  当前轮训练集损失 ： 0.3422469670506666\n",
      " 轮次  707  当前轮训练集损失 ： 0.34224319645771467\n",
      " 轮次  708  当前轮训练集损失 ： 0.3422394424028169\n",
      " 轮次  709  当前轮训练集损失 ： 0.3422357048021004\n",
      " 轮次  710  当前轮训练集损失 ： 0.3422319835721912\n",
      " 轮次  711  当前轮训练集损失 ： 0.3422282786302108\n",
      " 轮次  712  当前轮训练集损失 ： 0.3422245898937732\n",
      " 轮次  713  当前轮训练集损失 ： 0.3422209172809805\n",
      " 轮次  714  当前轮训练集损失 ： 0.34221726071042013\n",
      " 轮次  715  当前轮训练集损失 ： 0.3422136201011618\n",
      " 轮次  716  当前轮训练集损失 ： 0.34220999537275315\n",
      " 轮次  717  当前轮训练集损失 ： 0.3422063864452172\n",
      " 轮次  718  当前轮训练集损失 ： 0.34220279323904856\n",
      " 轮次  719  当前轮训练集损失 ： 0.3421992156752108\n",
      " 轮次  720  当前轮训练集损失 ： 0.3421956536751321\n",
      " 轮次  721  当前轮训练集损失 ： 0.34219210716070325\n",
      " 轮次  722  当前轮训练集损失 ： 0.34218857605427366\n",
      " 轮次  723  当前轮训练集损失 ： 0.34218506027864837\n",
      " 轮次  724  当前轮训练集损失 ： 0.34218155975708503\n",
      " 轮次  725  当前轮训练集损失 ： 0.3421780744132905\n",
      " 轮次  726  当前轮训练集损失 ： 0.3421746041714182\n",
      " 轮次  727  当前轮训练集损失 ： 0.34217114895606443\n",
      " 轮次  728  当前轮训练集损失 ： 0.34216770869226587\n",
      " 轮次  729  当前轮训练集损失 ： 0.34216428330549625\n",
      " 轮次  730  当前轮训练集损失 ： 0.3421608727216635\n",
      " 轮次  731  当前轮训练集损失 ： 0.3421574768671066\n",
      " 轮次  732  当前轮训练集损失 ： 0.34215409566859284\n",
      " 轮次  733  当前轮训练集损失 ： 0.34215072905331456\n",
      " 轮次  734  当前轮训练集损失 ： 0.34214737694888675\n",
      " 轮次  735  当前轮训练集损失 ： 0.3421440392833435\n",
      " 轮次  736  当前轮训练集损失 ： 0.34214071598513607\n",
      " 轮次  737  当前轮训练集损失 ： 0.34213740698312906\n",
      " 轮次  738  当前轮训练集损失 ： 0.3421341122065981\n",
      " 轮次  739  当前轮训练集损失 ： 0.34213083158522717\n",
      " 轮次  740  当前轮训练集损失 ： 0.34212756504910546\n",
      " 轮次  741  当前轮训练集损失 ： 0.34212431252872494\n",
      " 轮次  742  当前轮训练集损失 ： 0.34212107395497743\n",
      " 轮次  743  当前轮训练集损失 ： 0.342117849259152\n",
      " 轮次  744  当前轮训练集损失 ： 0.3421146383729324\n",
      " 轮次  745  当前轮训练集损失 ： 0.34211144122839404\n",
      " 轮次  746  当前轮训练集损失 ： 0.34210825775800163\n",
      " 轮次  747  当前轮训练集损失 ： 0.3421050878946067\n",
      " 轮次  748  当前轮训练集损失 ： 0.34210193157144453\n",
      " 轮次  749  当前轮训练集损失 ： 0.34209878872213184\n",
      " 轮次  750  当前轮训练集损失 ： 0.3420956592806645\n",
      " 轮次  751  当前轮训练集损失 ： 0.3420925431814147\n",
      " 轮次  752  当前轮训练集损失 ： 0.34208944035912814\n",
      " 轮次  753  当前轮训练集损失 ： 0.34208635074892213\n",
      " 轮次  754  当前轮训练集损失 ： 0.34208327428628255\n",
      " 轮次  755  当前轮训练集损失 ： 0.3420802109070622\n",
      " 轮次  756  当前轮训练集损失 ： 0.3420771605474773\n",
      " 轮次  757  当前轮训练集损失 ： 0.3420741231441057\n",
      " 轮次  758  当前轮训练集损失 ： 0.3420710986338845\n",
      " 轮次  759  当前轮训练集损失 ： 0.34206808695410756\n",
      " 轮次  760  当前轮训练集损失 ： 0.34206508804242314\n",
      " 轮次  761  当前轮训练集损失 ： 0.3420621018368313\n",
      " 轮次  762  当前轮训练集损失 ： 0.3420591282756819\n",
      " 轮次  763  当前轮训练集损失 ： 0.3420561672976723\n",
      " 轮次  764  当前轮训练集损失 ： 0.34205321884184486\n",
      " 轮次  765  当前轮训练集损失 ： 0.34205028284758476\n",
      " 轮次  766  当前轮训练集损失 ： 0.3420473592546178\n",
      " 轮次  767  当前轮训练集损失 ： 0.34204444800300804\n",
      " 轮次  768  当前轮训练集损失 ： 0.3420415490331554\n",
      " 轮次  769  当前轮训练集损失 ： 0.34203866228579444\n",
      " 轮次  770  当前轮训练集损失 ： 0.34203578770199033\n",
      " 轮次  771  当前轮训练集损失 ： 0.3420329252231387\n",
      " 轮次  772  当前轮训练集损失 ： 0.34203007479096204\n",
      " 轮次  773  当前轮训练集损失 ： 0.34202723634750837\n",
      " 轮次  774  当前轮训练集损失 ： 0.34202440983514865\n",
      " 轮次  775  当前轮训练集损失 ： 0.34202159519657477\n",
      " 轮次  776  当前轮训练集损失 ： 0.3420187923747976\n",
      " 轮次  777  当前轮训练集损失 ： 0.3420160013131449\n",
      " 轮次  778  当前轮训练集损失 ： 0.3420132219552593\n",
      " 轮次  779  当前轮训练集损失 ： 0.34201045424509585\n",
      " 轮次  780  当前轮训练集损失 ： 0.34200769812692056\n",
      " 轮次  781  当前轮训练集损失 ： 0.34200495354530813\n",
      " 轮次  782  当前轮训练集损失 ： 0.3420022204451398\n",
      " 轮次  783  当前轮训练集损失 ： 0.3419994987716015\n",
      " 轮次  784  当前轮训练集损失 ： 0.3419967884701823\n",
      " 轮次  785  当前轮训练集损失 ： 0.3419940894866715\n",
      " 轮次  786  当前轮训练集损失 ： 0.3419914017671574\n",
      " 轮次  787  当前轮训练集损失 ： 0.3419887252580257\n",
      " 轮次  788  当前轮训练集损失 ： 0.3419860599059565\n",
      " 轮次  789  当前轮训练集损失 ： 0.3419834056579234\n",
      " 轮次  790  当前轮训练集损失 ： 0.34198076246119097\n",
      " 轮次  791  当前轮训练集损失 ： 0.3419781302633136\n",
      " 轮次  792  当前轮训练集损失 ： 0.34197550901213264\n",
      " 轮次  793  当前轮训练集损失 ： 0.34197289865577546\n",
      " 轮次  794  当前轮训练集损失 ： 0.3419702991426533\n",
      " 轮次  795  当前轮训练集损失 ： 0.34196771042145935\n",
      " 轮次  796  当前轮训练集损失 ： 0.3419651324411671\n",
      " 轮次  797  当前轮训练集损失 ： 0.34196256515102846\n",
      " 轮次  798  当前轮训练集损失 ： 0.3419600085005721\n",
      " 轮次  799  当前轮训练集损失 ： 0.34195746243960157\n",
      " 轮次  800  当前轮训练集损失 ： 0.3419549269181936\n",
      " 轮次  801  当前轮训练集损失 ： 0.3419524018866965\n",
      " 轮次  802  当前轮训练集损失 ： 0.34194988729572817\n",
      " 轮次  803  当前轮训练集损失 ： 0.3419473830961746\n",
      " 轮次  804  当前轮训练集损失 ： 0.3419448892391882\n",
      " 轮次  805  当前轮训练集损失 ： 0.34194240567618583\n",
      " 轮次  806  当前轮训练集损失 ： 0.34193993235884773\n",
      " 轮次  807  当前轮训练集损失 ： 0.34193746923911494\n",
      " 轮次  808  当前轮训练集损失 ： 0.3419350162691887\n",
      " 轮次  809  当前轮训练集损失 ： 0.34193257340152783\n",
      " 轮次  810  当前轮训练集损失 ： 0.34193014058884796\n",
      " 轮次  811  当前轮训练集损失 ： 0.3419277177841194\n",
      " 轮次  812  当前轮训练集损失 ： 0.34192530494056567\n",
      " 轮次  813  当前轮训练集损失 ： 0.34192290201166187\n",
      " 轮次  814  当前轮训练集损失 ： 0.3419205089511333\n",
      " 轮次  815  当前轮训练集损失 ： 0.3419181257129537\n",
      " 轮次  816  当前轮训练集损失 ： 0.3419157522513439\n",
      " 轮次  817  当前轮训练集损失 ： 0.34191338852076986\n",
      " 轮次  818  当前轮训练集损失 ： 0.34191103447594157\n",
      " 轮次  819  当前轮训练集损失 ： 0.34190869007181146\n",
      " 轮次  820  当前轮训练集损失 ： 0.3419063552635727\n",
      " 轮次  821  当前轮训练集损失 ： 0.3419040300066579\n",
      " 轮次  822  当前轮训练集损失 ： 0.3419017142567375\n",
      " 轮次  823  当前轮训练集损失 ： 0.3418994079697184\n",
      " 轮次  824  当前轮训练集损失 ： 0.34189711110174237\n",
      " 轮次  825  当前轮训练集损失 ： 0.3418948236091847\n",
      " 轮次  826  当前轮训练集损失 ： 0.34189254544865283\n",
      " 轮次  827  当前轮训练集损失 ： 0.3418902765769845\n",
      " 轮次  828  当前轮训练集损失 ： 0.34188801695124693\n",
      " 轮次  829  当前轮训练集损失 ： 0.3418857665287351\n",
      " 轮次  830  当前轮训练集损失 ： 0.3418835252669702\n",
      " 轮次  831  当前轮训练集损失 ： 0.3418812931236986\n",
      " 轮次  832  当前轮训练集损失 ： 0.34187907005688994\n",
      " 轮次  833  当前轮训练集损失 ： 0.34187685602473666\n",
      " 轮次  834  当前轮训练集损失 ： 0.34187465098565145\n",
      " 轮次  835  当前轮训练集损失 ： 0.3418724548982669\n",
      " 轮次  836  当前轮训练集损失 ： 0.34187026772143364\n",
      " 轮次  837  当前轮训练集损失 ： 0.34186808941421915\n",
      " 轮次  838  当前轮训练集损失 ： 0.3418659199359064\n",
      " 轮次  839  当前轮训练集损失 ： 0.3418637592459927\n",
      " 轮次  840  当前轮训练集损失 ： 0.3418616073041881\n",
      " 轮次  841  当前轮训练集损失 ： 0.34185946407041423\n",
      " 轮次  842  当前轮训练集损失 ： 0.3418573295048031\n",
      " 轮次  843  当前轮训练集损失 ： 0.341855203567696\n",
      " 轮次  844  当前轮训练集损失 ： 0.34185308621964133\n",
      " 轮次  845  当前轮训练集损失 ： 0.3418509774213947\n",
      " 轮次  846  当前轮训练集损失 ： 0.3418488771339166\n",
      " 轮次  847  当前轮训练集损失 ： 0.3418467853183714\n",
      " 轮次  848  当前轮训练集损失 ： 0.3418447019361269\n",
      " 轮次  849  当前轮训练集损失 ： 0.34184262694875156\n",
      " 轮次  850  当前轮训练集损失 ： 0.3418405603180148\n",
      " 轮次  851  当前轮训练集损失 ： 0.34183850200588506\n",
      " 轮次  852  当前轮训练集损失 ： 0.3418364519745286\n",
      " 轮次  853  当前轮训练集损失 ： 0.34183441018630845\n",
      " 轮次  854  当前轮训练集损失 ： 0.34183237660378313\n",
      " 轮次  855  当前轮训练集损失 ： 0.34183035118970584\n",
      " 轮次  856  当前轮训练集损失 ： 0.34182833390702255\n",
      " 轮次  857  当前轮训练集损失 ： 0.3418263247188716\n",
      " 轮次  858  当前轮训练集损失 ： 0.3418243235885823\n",
      " 轮次  859  当前轮训练集损失 ： 0.34182233047967325\n",
      " 轮次  860  当前轮训练集损失 ： 0.34182034535585226\n",
      " 轮次  861  当前轮训练集损失 ： 0.34181836818101435\n",
      " 轮次  862  当前轮训练集损失 ： 0.341816398919241\n",
      " 轮次  863  当前轮训练集损失 ： 0.34181443753479884\n",
      " 轮次  864  当前轮训练集损失 ： 0.341812483992139\n",
      " 轮次  865  当前轮训练集损失 ： 0.34181053825589514\n",
      " 轮次  866  当前轮训练集损失 ： 0.34180860029088345\n",
      " 轮次  867  当前轮训练集损失 ： 0.3418066700621007\n",
      " 轮次  868  当前轮训练集损失 ： 0.3418047475347237\n",
      " 轮次  869  当前轮训练集损失 ： 0.34180283267410777\n",
      " 轮次  870  当前轮训练集损失 ： 0.3418009254457863\n",
      " 轮次  871  当前轮训练集损失 ： 0.34179902581546895\n",
      " 轮次  872  当前轮训练集损失 ： 0.34179713374904136\n",
      " 轮次  873  当前轮训练集损失 ： 0.3417952492125635\n",
      " 轮次  874  当前轮训练集损失 ： 0.34179337217226896\n",
      " 轮次  875  当前轮训练集损失 ： 0.34179150259456387\n",
      " 轮次  876  当前轮训练集损失 ： 0.3417896404460259\n",
      " 轮次  877  当前轮训练集损失 ： 0.3417877856934031\n",
      " 轮次  878  当前轮训练集损失 ： 0.3417859383036132\n",
      " 轮次  879  当前轮训练集损失 ： 0.3417840982437424\n",
      " 轮次  880  当前轮训练集损失 ： 0.34178226548104407\n",
      " 轮次  881  当前轮训练集损失 ： 0.3417804399829387\n",
      " 轮次  882  当前轮训练集损失 ： 0.34177862171701195\n",
      " 轮次  883  当前轮训练集损失 ： 0.3417768106510142\n",
      " 轮次  884  当前轮训练集损失 ： 0.34177500675285927\n",
      " 轮次  885  当前轮训练集损失 ： 0.34177320999062394\n",
      " 轮次  886  当前轮训练集损失 ： 0.3417714203325465\n",
      " 轮次  887  当前轮训练集损失 ： 0.34176963774702623\n",
      " 轮次  888  当前轮训练集损失 ： 0.341767862202622\n",
      " 轮次  889  当前轮训练集损失 ： 0.34176609366805183\n",
      " 轮次  890  当前轮训练集损失 ： 0.3417643321121915\n",
      " 轮次  891  当前轮训练集损失 ： 0.3417625775040741\n",
      " 轮次  892  当前轮训练集损失 ： 0.34176082981288874\n",
      " 轮次  893  当前轮训练集损失 ： 0.34175908900797974\n",
      " 轮次  894  当前轮训练集损失 ： 0.34175735505884586\n",
      " 轮次  895  当前轮训练集损失 ： 0.34175562793513964\n",
      " 轮次  896  当前轮训练集损失 ： 0.34175390760666535\n",
      " 轮次  897  当前轮训练集损失 ： 0.3417521940433799\n",
      " 轮次  898  当前轮训练集损失 ： 0.3417504872153904\n",
      " 轮次  899  当前轮训练集损失 ： 0.34174878709295425\n",
      " 轮次  900  当前轮训练集损失 ： 0.3417470936464777\n",
      " 轮次  901  当前轮训练集损失 ： 0.34174540684651544\n",
      " 轮次  902  当前轮训练集损失 ： 0.34174372666376923\n",
      " 轮次  903  当前轮训练集损失 ： 0.3417420530690878\n",
      " 轮次  904  当前轮训练集损失 ： 0.34174038603346524\n",
      " 轮次  905  当前轮训练集损失 ： 0.3417387255280407\n",
      " 轮次  906  当前轮训练集损失 ： 0.3417370715240969\n",
      " 轮次  907  当前轮训练集损失 ： 0.3417354239930604\n",
      " 轮次  908  当前轮训练集损失 ： 0.34173378290649975\n",
      " 轮次  909  当前轮训练集损失 ： 0.3417321482361251\n",
      " 轮次  910  当前轮训练集损失 ： 0.34173051995378745\n",
      " 轮次  911  当前轮训练集损失 ： 0.3417288980314778\n",
      " 轮次  912  当前轮训练集损失 ： 0.3417272824413261\n",
      " 轮次  913  当前轮训练集损失 ： 0.34172567315560093\n",
      " 轮次  914  当前轮训练集损失 ： 0.34172407014670797\n",
      " 轮次  915  当前轮训练集损失 ： 0.3417224733871904\n",
      " 轮次  916  当前轮训练集损失 ： 0.34172088284972685\n",
      " 轮次  917  当前轮训练集损失 ： 0.34171929850713145\n",
      " 轮次  918  当前轮训练集损失 ： 0.34171772033235265\n",
      " 轮次  919  当前轮训练集损失 ： 0.3417161482984728\n",
      " 轮次  920  当前轮训练集损失 ： 0.34171458237870694\n",
      " 轮次  921  当前轮训练集损失 ： 0.3417130225464029\n",
      " 轮次  922  当前轮训练集损失 ： 0.3417114687750391\n",
      " 轮次  923  当前轮训练集损失 ： 0.3417099210382254\n",
      " 轮次  924  当前轮训练集损失 ： 0.3417083793097013\n",
      " 轮次  925  当前轮训练集损失 ： 0.3417068435633357\n",
      " 轮次  926  当前轮训练集损失 ： 0.34170531377312574\n",
      " 轮次  927  当前轮训练集损失 ： 0.34170378991319683\n",
      " 轮次  928  当前轮训练集损失 ： 0.34170227195780106\n",
      " 轮次  929  当前轮训练集损失 ： 0.34170075988131704\n",
      " 轮次  930  当前轮训练集损失 ： 0.34169925365824905\n",
      " 轮次  931  当前轮训练集损失 ： 0.3416977532632261\n",
      " 轮次  932  当前轮训练集损失 ： 0.34169625867100184\n",
      " 轮次  933  当前轮训练集损失 ： 0.34169476985645314\n",
      " 轮次  934  当前轮训练集损失 ： 0.3416932867945799\n",
      " 轮次  935  当前轮训练集损失 ： 0.3416918094605039\n",
      " 轮次  936  当前轮训练集损失 ： 0.34169033782946917\n",
      " 轮次  937  当前轮训练集损失 ： 0.34168887187683966\n",
      " 轮次  938  当前轮训练集损失 ： 0.3416874115780999\n",
      " 轮次  939  当前轮训练集损失 ： 0.34168595690885395\n",
      " 轮次  940  当前轮训练集损失 ： 0.3416845078448244\n",
      " 轮次  941  当前轮训练集损失 ： 0.3416830643618522\n",
      " 轮次  942  当前轮训练集损失 ： 0.34168162643589595\n",
      " 轮次  943  当前轮训练集损失 ： 0.34168019404303057\n",
      " 轮次  944  当前轮训练集损失 ： 0.34167876715944756\n",
      " 轮次  945  当前轮训练集损失 ： 0.34167734576145403\n",
      " 轮次  946  当前轮训练集损失 ： 0.3416759298254716\n",
      " 轮次  947  当前轮训练集损失 ： 0.34167451932803666\n",
      " 轮次  948  当前轮训练集损失 ： 0.34167311424579877\n",
      " 轮次  949  当前轮训练集损失 ： 0.3416717145555209\n",
      " 轮次  950  当前轮训练集损失 ： 0.341670320234078\n",
      " 轮次  951  当前轮训练集损失 ： 0.34166893125845726\n",
      " 轮次  952  当前轮训练集损失 ： 0.3416675476057565\n",
      " 轮次  953  当前轮训练集损失 ： 0.34166616925318444\n",
      " 轮次  954  当前轮训练集损失 ： 0.3416647961780596\n",
      " 轮次  955  当前轮训练集损失 ： 0.3416634283578099\n",
      " 轮次  956  当前轮训练集损失 ： 0.3416620657699718\n",
      " 轮次  957  当前轮训练集损失 ： 0.34166070839219026\n",
      " 轮次  958  当前轮训练集损失 ： 0.3416593562022172\n",
      " 轮次  959  当前轮训练集损失 ： 0.34165800917791217\n",
      " 轮次  960  当前轮训练集损失 ： 0.34165666729724037\n",
      " 轮次  961  当前轮训练集损失 ： 0.34165533053827335\n",
      " 轮次  962  当前轮训练集损失 ： 0.34165399887918757\n",
      " 轮次  963  当前轮训练集损失 ： 0.34165267229826424\n",
      " 轮次  964  当前轮训练集损失 ： 0.34165135077388836\n",
      " 轮次  965  当前轮训练集损失 ： 0.34165003428454893\n",
      " 轮次  966  当前轮训练集损失 ： 0.3416487228088373\n",
      " 轮次  967  当前轮训练集损失 ： 0.3416474163254475\n",
      " 轮次  968  当前轮训练集损失 ： 0.34164611481317525\n",
      " 轮次  969  当前轮训练集损失 ： 0.34164481825091775\n",
      " 轮次  970  当前轮训练集损失 ： 0.3416435266176725\n",
      " 轮次  971  当前轮训练集损失 ： 0.34164223989253745\n",
      " 轮次  972  当前轮训练集损失 ： 0.3416409580547101\n",
      " 轮次  973  当前轮训练集损失 ： 0.34163968108348697\n",
      " 轮次  974  当前轮训练集损失 ： 0.34163840895826314\n",
      " 轮次  975  当前轮训练集损失 ： 0.34163714165853165\n",
      " 轮次  976  当前轮训练集损失 ： 0.3416358791638831\n",
      " 轮次  977  当前轮训练集损失 ： 0.34163462145400497\n",
      " 轮次  978  当前轮训练集损失 ： 0.3416333685086812\n",
      " 轮次  979  当前轮训练集损失 ： 0.3416321203077915\n",
      " 轮次  980  当前轮训练集损失 ： 0.3416308768313111\n",
      " 轮次  981  当前轮训练集损失 ： 0.34162963805931\n",
      " 轮次  982  当前轮训练集损失 ： 0.3416284039719528\n",
      " 轮次  983  当前轮训练集损失 ： 0.34162717454949765\n",
      " 轮次  984  当前轮训练集损失 ： 0.34162594977229616\n",
      " 轮次  985  当前轮训练集损失 ： 0.34162472962079266\n",
      " 轮次  986  当前轮训练集损失 ： 0.34162351407552405\n",
      " 轮次  987  当前轮训练集损失 ： 0.34162230311711905\n",
      " 轮次  988  当前轮训练集损失 ： 0.3416210967262977\n",
      " 轮次  989  当前轮训练集损失 ： 0.3416198948838707\n",
      " 轮次  990  当前轮训练集损失 ： 0.3416186975707394\n",
      " 轮次  991  当前轮训练集损失 ： 0.34161750476789504\n",
      " 轮次  992  当前轮训练集损失 ： 0.3416163164564181\n",
      " 轮次  993  当前轮训练集损失 ： 0.34161513261747817\n",
      " 轮次  994  当前轮训练集损失 ： 0.34161395323233323\n",
      " 轮次  995  当前轮训练集损失 ： 0.34161277828232933\n",
      " 轮次  996  当前轮训练集损失 ： 0.34161160774889987\n",
      " 轮次  997  当前轮训练集损失 ： 0.3416104416135654\n",
      " 轮次  998  当前轮训练集损失 ： 0.3416092798579333\n",
      " 轮次  999  当前轮训练集损失 ： 0.34160812246369676\n",
      " 轮次  1000  当前轮训练集损失 ： 0.34160696941263485\n",
      " 轮次  1001  当前轮训练集损失 ： 0.3416058206866117\n",
      " 轮次  1002  当前轮训练集损失 ： 0.34160467626757646\n",
      " 轮次  1003  当前轮训练集损失 ： 0.34160353613756245\n",
      " 轮次  1004  当前轮训练集损失 ： 0.3416024002786868\n",
      " 轮次  1005  当前轮训练集损失 ： 0.34160126867315027\n",
      " 轮次  1006  当前轮训练集损失 ： 0.34160014130323635\n",
      " 轮次  1007  当前轮训练集损失 ： 0.3415990181513115\n",
      " 轮次  1008  当前轮训练集损失 ： 0.34159789919982375\n",
      " 轮次  1009  当前轮训练集损失 ： 0.3415967844313033\n",
      " 轮次  1010  当前轮训练集损失 ： 0.3415956738283615\n",
      " 轮次  1011  当前轮训练集损失 ： 0.3415945673736902\n",
      " 轮次  1012  当前轮训练集损失 ： 0.3415934650500622\n",
      " 轮次  1013  当前轮训练集损失 ： 0.34159236684032984\n",
      " 轮次  1014  当前轮训练集损失 ： 0.3415912727274252\n",
      " 轮次  1015  当前轮训练集损失 ： 0.34159018269435937\n",
      " 轮次  1016  当前轮训练集损失 ： 0.34158909672422255\n",
      " 轮次  1017  当前轮训练集损失 ： 0.34158801480018275\n",
      " 轮次  1018  当前轮训练集损失 ： 0.34158693690548625\n",
      " 轮次  1019  当前轮训练集损失 ： 0.34158586302345667\n",
      " 轮次  1020  当前轮训练集损失 ： 0.3415847931374946\n",
      " 轮次  1021  当前轮训练集损失 ： 0.3415837272310777\n",
      " 轮次  1022  当前轮训练集损失 ： 0.3415826652877597\n",
      " 轮次  1023  当前轮训练集损失 ： 0.34158160729117004\n",
      " 轮次  1024  当前轮训练集损失 ： 0.34158055322501385\n",
      " 轮次  1025  当前轮训练集损失 ： 0.34157950307307144\n",
      " 轮次  1026  当前轮训练集损失 ： 0.34157845681919774\n",
      " 轮次  1027  当前轮训练集损失 ： 0.34157741444732187\n",
      " 轮次  1028  当前轮训练集损失 ： 0.3415763759414471\n",
      " 轮次  1029  当前轮训练集损失 ： 0.3415753412856501\n",
      " 轮次  1030  当前轮训练集损失 ： 0.3415743104640808\n",
      " 轮次  1031  当前轮训练集损失 ： 0.34157328346096183\n",
      " 轮次  1032  当前轮训练集损失 ： 0.34157226026058835\n",
      " 轮次  1033  当前轮训练集损失 ： 0.34157124084732754\n",
      " 轮次  1034  当前轮训练集损失 ： 0.341570225205618\n",
      " 轮次  1035  当前轮训练集损失 ： 0.34156921331997003\n",
      " 轮次  1036  当前轮训练集损失 ： 0.3415682051749645\n",
      " 轮次  1037  当前轮训练集损失 ： 0.3415672007552531\n",
      " 轮次  1038  当前轮训练集损失 ： 0.34156620004555743\n",
      " 轮次  1039  当前轮训练集损失 ： 0.3415652030306692\n",
      " 轮次  1040  当前轮训练集损失 ： 0.3415642096954494\n",
      " 轮次  1041  当前轮训练集损失 ： 0.3415632200248282\n",
      " 轮次  1042  当前轮训练集损失 ： 0.3415622340038045\n",
      " 轮次  1043  当前轮训练集损失 ： 0.3415612516174458\n",
      " 轮次  1044  当前轮训练集损失 ： 0.34156027285088736\n",
      " 轮次  1045  当前轮训练集损失 ： 0.3415592976893322\n",
      " 轮次  1046  当前轮训练集损失 ： 0.34155832611805076\n",
      " 轮次  1047  当前轮训练集损失 ： 0.34155735812238064\n",
      " 轮次  1048  当前轮训练集损失 ： 0.34155639368772583\n",
      " 轮次  1049  当前轮训练集损失 ： 0.3415554327995567\n",
      " 轮次  1050  当前轮训练集损失 ： 0.3415544754434098\n",
      " 轮次  1051  当前轮训练集损失 ： 0.34155352160488717\n",
      " 轮次  1052  当前轮训练集损失 ： 0.341552571269656\n",
      " 轮次  1053  当前轮训练集损失 ： 0.3415516244234487\n",
      " 轮次  1054  当前轮训练集损失 ： 0.3415506810520624\n",
      " 轮次  1055  当前轮训练集损失 ： 0.3415497411413584\n",
      " 轮次  1056  当前轮训练集损失 ： 0.3415488046772617\n",
      " 轮次  1057  当前轮训练集损失 ： 0.34154787164576145\n",
      " 轮次  1058  当前轮训练集损失 ： 0.34154694203291\n",
      " 轮次  1059  当前轮训练集损失 ： 0.34154601582482247\n",
      " 轮次  1060  当前轮训练集损失 ： 0.3415450930076769\n",
      " 轮次  1061  当前轮训练集损失 ： 0.3415441735677137\n",
      " 轮次  1062  当前轮训练集损失 ： 0.3415432574912353\n",
      " 轮次  1063  当前轮训练集损失 ： 0.3415423447646057\n",
      " 轮次  1064  当前轮训练集损失 ： 0.3415414353742507\n",
      " 轮次  1065  当前轮训练集损失 ： 0.3415405293066568\n",
      " 轮次  1066  当前轮训练集损失 ： 0.34153962654837167\n",
      " 轮次  1067  当前轮训练集损失 ： 0.3415387270860033\n",
      " 轮次  1068  当前轮训练集损失 ： 0.3415378309062198\n",
      " 轮次  1069  当前轮训练集损失 ： 0.3415369379957493\n",
      " 轮次  1070  当前轮训练集损失 ： 0.34153604834137963\n",
      " 轮次  1071  当前轮训练集损失 ： 0.3415351619299575\n",
      " 轮次  1072  当前轮训练集损失 ： 0.34153427874838915\n",
      " 轮次  1073  当前轮训练集损失 ： 0.3415333987836391\n",
      " 轮次  1074  当前轮训练集损失 ： 0.3415325220227303\n",
      " 轮次  1075  当前轮训练集损失 ： 0.34153164845274403\n",
      " 轮次  1076  当前轮训练集损失 ： 0.3415307780608192\n",
      " 轮次  1077  当前轮训练集损失 ： 0.3415299108341523\n",
      " 轮次  1078  当前轮训练集损失 ： 0.3415290467599969\n",
      " 轮次  1079  当前轮训练集损失 ： 0.3415281858256639\n",
      " 轮次  1080  当前轮训练集损失 ： 0.34152732801852037\n",
      " 轮次  1081  当前轮训练集损失 ： 0.34152647332599007\n",
      " 轮次  1082  当前轮训练集损失 ： 0.34152562173555284\n",
      " 轮次  1083  当前轮训练集损失 ： 0.341524773234744\n",
      " 轮次  1084  当前轮训练集损失 ： 0.3415239278111548\n",
      " 轮次  1085  当前轮训练集损失 ： 0.34152308545243165\n",
      " 轮次  1086  当前轮训练集损失 ： 0.34152224614627574\n",
      " 轮次  1087  当前轮训练集损失 ： 0.3415214098804432\n",
      " 轮次  1088  当前轮训练集损失 ： 0.3415205766427443\n",
      " 轮次  1089  当前轮训练集损失 ： 0.34151974642104393\n",
      " 轮次  1090  当前轮训练集损失 ： 0.34151891920326033\n",
      " 轮次  1091  当前轮训练集损失 ： 0.3415180949773657\n",
      " 轮次  1092  当前轮训练集损失 ： 0.3415172737313855\n",
      " 轮次  1093  当前轮训练集损失 ： 0.3415164554533982\n",
      " 轮次  1094  当前轮训练集损失 ： 0.34151564013153524\n",
      " 轮次  1095  当前轮训练集损失 ： 0.3415148277539805\n",
      " 轮次  1096  当前轮训练集损失 ： 0.3415140183089702\n",
      " 轮次  1097  当前轮训练集损失 ： 0.3415132117847926\n",
      " 轮次  1098  当前轮训练集损失 ： 0.34151240816978784\n",
      " 轮次  1099  当前轮训练集损失 ： 0.3415116074523474\n",
      " 轮次  1100  当前轮训练集损失 ： 0.34151080962091396\n",
      " 轮次  1101  当前轮训练集损失 ： 0.3415100146639816\n",
      " 轮次  1102  当前轮训练集损失 ： 0.3415092225700948\n",
      " 轮次  1103  当前轮训练集损失 ： 0.3415084333278487\n",
      " 轮次  1104  当前轮训练集损失 ： 0.34150764692588875\n",
      " 轮次  1105  当前轮训练集损失 ： 0.34150686335291014\n",
      " 轮次  1106  当前轮训练集损失 ： 0.3415060825976583\n",
      " 轮次  1107  当前轮训练集损失 ： 0.34150530464892764\n",
      " 轮次  1108  当前轮训练集损失 ： 0.3415045294955623\n",
      " 轮次  1109  当前轮训练集损失 ： 0.3415037571264553\n",
      " 轮次  1110  当前轮训练集损失 ： 0.3415029875305483\n",
      " 轮次  1111  当前轮训练集损失 ： 0.3415022206968316\n",
      " 轮次  1112  当前轮训练集损失 ： 0.34150145661434395\n",
      " 轮次  1113  当前轮训练集损失 ： 0.3415006952721721\n",
      " 轮次  1114  当前轮训练集损失 ： 0.3414999366594507\n",
      " 轮次  1115  当前轮训练集损失 ： 0.3414991807653616\n",
      " 轮次  1116  当前轮训练集损失 ： 0.3414984275791346\n",
      " 轮次  1117  当前轮训练集损失 ： 0.3414976770900464\n",
      " 轮次  1118  当前轮训练集损失 ： 0.3414969292874206\n",
      " 轮次  1119  当前轮训练集损失 ： 0.34149618416062755\n",
      " 轮次  1120  当前轮训练集损失 ： 0.3414954416990838\n",
      " 轮次  1121  当前轮训练集损失 ： 0.3414947018922524\n",
      " 轮次  1122  当前轮训练集损失 ： 0.3414939647296423\n",
      " 轮次  1123  当前轮训练集损失 ： 0.3414932302008082\n",
      " 轮次  1124  当前轮训练集损失 ： 0.34149249829535056\n",
      " 轮次  1125  当前轮训练集损失 ： 0.34149176900291495\n",
      " 轮次  1126  当前轮训练集损失 ： 0.34149104231319183\n",
      " 轮次  1127  当前轮训练集损失 ： 0.3414903182159171\n",
      " 轮次  1128  当前轮训练集损失 ： 0.341489596700871\n",
      " 轮次  1129  当前轮训练集损失 ： 0.3414888777578783\n",
      " 轮次  1130  当前轮训练集损失 ： 0.3414881613768078\n",
      " 轮次  1131  当前轮训练集损失 ： 0.34148744754757276\n",
      " 轮次  1132  当前轮训练集损失 ： 0.3414867362601299\n",
      " 轮次  1133  当前轮训练集损失 ： 0.34148602750447943\n",
      " 轮次  1134  当前轮训练集损失 ： 0.34148532127066544\n",
      " 轮次  1135  当前轮训练集损失 ： 0.3414846175487749\n",
      " 轮次  1136  当前轮训练集损失 ： 0.3414839163289374\n",
      " 轮次  1137  当前轮训练集损失 ： 0.341483217601326\n",
      " 轮次  1138  当前轮训练集损失 ： 0.34148252135615575\n",
      " 轮次  1139  当前轮训练集损失 ： 0.3414818275836843\n",
      " 轮次  1140  当前轮训练集损失 ： 0.3414811362742114\n",
      " 轮次  1141  当前轮训练集损失 ： 0.3414804474180787\n",
      " 轮次  1142  当前轮训练集损失 ： 0.3414797610056696\n",
      " 轮次  1143  当前轮训练集损失 ： 0.3414790770274091\n",
      " 轮次  1144  当前轮训练集损失 ： 0.3414783954737634\n",
      " 轮次  1145  当前轮训练集损失 ： 0.3414777163352399\n",
      " 轮次  1146  当前轮训练集损失 ： 0.341477039602387\n",
      " 轮次  1147  当前轮训练集损失 ： 0.3414763652657938\n",
      " 轮次  1148  当前轮训练集损失 ： 0.3414756933160898\n",
      " 轮次  1149  当前轮训练集损失 ： 0.34147502374394517\n",
      " 轮次  1150  当前轮训练集损失 ： 0.34147435654006997\n",
      " 轮次  1151  当前轮训练集损失 ： 0.34147369169521413\n",
      " 轮次  1152  当前轮训练集损失 ： 0.34147302920016775\n",
      " 轮次  1153  当前轮训练集损失 ： 0.3414723690457604\n",
      " 轮次  1154  当前轮训练集损失 ： 0.3414717112228605\n",
      " 轮次  1155  当前轮训练集损失 ： 0.3414710557223764\n",
      " 轮次  1156  当前轮训练集损失 ： 0.34147040253525496\n",
      " 轮次  1157  当前轮训练集损失 ： 0.3414697516524823\n",
      " 轮次  1158  当前轮训练集损失 ： 0.3414691030650829\n",
      " 轮次  1159  当前轮训练集损失 ： 0.3414684567641196\n",
      " 轮次  1160  当前轮训练集损失 ： 0.34146781274069365\n",
      " 轮次  1161  当前轮训练集损失 ： 0.3414671709859447\n",
      " 轮次  1162  当前轮训练集损失 ： 0.3414665314910497\n",
      " 轮次  1163  当前轮训练集损失 ： 0.34146589424722384\n",
      " 轮次  1164  当前轮训练集损失 ： 0.34146525924571963\n",
      " 轮次  1165  当前轮训练集损失 ： 0.34146462647782677\n",
      " 轮次  1166  当前轮训练集损失 ： 0.34146399593487275\n",
      " 轮次  1167  当前轮训练集损失 ： 0.3414633676082215\n",
      " 轮次  1168  当前轮训练集损失 ： 0.341462741489274\n",
      " 轮次  1169  当前轮训练集损失 ： 0.34146211756946776\n",
      " 轮次  1170  当前轮训练集损失 ： 0.34146149584027724\n",
      " 轮次  1171  当前轮训练集损失 ： 0.3414608762932126\n",
      " 轮次  1172  当前轮训练集损失 ： 0.34146025891982057\n",
      " 轮次  1173  当前轮训练集损失 ： 0.34145964371168364\n",
      " 轮次  1174  当前轮训练集损失 ： 0.3414590306604202\n",
      " 轮次  1175  当前轮训练集损失 ： 0.3414584197576844\n",
      " 轮次  1176  当前轮训练集损失 ： 0.34145781099516553\n",
      " 轮次  1177  当前轮训练集损失 ： 0.34145720436458854\n",
      " 轮次  1178  当前轮训练集损失 ： 0.34145659985771326\n",
      " 轮次  1179  当前轮训练集损失 ： 0.34145599746633454\n",
      " 轮次  1180  当前轮训练集损失 ： 0.3414553971822822\n",
      " 轮次  1181  当前轮训练集损失 ： 0.3414547989974205\n",
      " 轮次  1182  当前轮训练集损失 ： 0.34145420290364814\n",
      " 轮次  1183  当前轮训练集损失 ： 0.34145360889289833\n",
      " 轮次  1184  当前轮训练集损失 ： 0.3414530169571384\n",
      " 轮次  1185  当前轮训练集损失 ： 0.3414524270883695\n",
      " 轮次  1186  当前轮训练集损失 ： 0.3414518392786266\n",
      " 轮次  1187  当前轮训练集损失 ： 0.3414512535199789\n",
      " 轮次  1188  当前轮训练集损失 ： 0.3414506698045282\n",
      " 轮次  1189  当前轮训练集损失 ： 0.34145008812441036\n",
      " 轮次  1190  当前轮训练集损失 ： 0.34144950847179417\n",
      " 轮次  1191  当前轮训练集损失 ： 0.3414489308388815\n",
      " 轮次  1192  当前轮训练集损失 ： 0.34144835521790723\n",
      " 轮次  1193  当前轮训练集损失 ： 0.34144778160113864\n",
      " 轮次  1194  当前轮训练集损失 ： 0.3414472099808759\n",
      " 轮次  1195  当前轮训练集损失 ： 0.3414466403494517\n",
      " 轮次  1196  当前轮训练集损失 ： 0.3414460726992306\n",
      " 轮次  1197  当前轮训练集损失 ： 0.3414455070226096\n",
      " 轮次  1198  当前轮训练集损失 ： 0.3414449433120176\n",
      " 轮次  1199  当前轮训练集损失 ： 0.3414443815599153\n",
      " 轮次  1200  当前轮训练集损失 ： 0.34144382175879523\n",
      " 轮次  1201  当前轮训练集损失 ： 0.34144326390118107\n",
      " 轮次  1202  当前轮训练集损失 ： 0.3414427079796285\n",
      " 轮次  1203  当前轮训练集损失 ： 0.3414421539867238\n",
      " 轮次  1204  当前轮训练集损失 ： 0.3414416019150847\n",
      " 轮次  1205  当前轮训练集损失 ： 0.3414410517573599\n",
      " 轮次  1206  当前轮训练集损失 ： 0.3414405035062287\n",
      " 轮次  1207  当前轮训练集损失 ： 0.3414399571544013\n",
      " 轮次  1208  当前轮训练集损失 ： 0.34143941269461797\n",
      " 轮次  1209  当前轮训练集损失 ： 0.34143887011965013\n",
      " 轮次  1210  当前轮训练集损失 ： 0.34143832942229874\n",
      " 轮次  1211  当前轮训练集损失 ： 0.3414377905953952\n",
      " 轮次  1212  当前轮训练集损失 ： 0.3414372536318007\n",
      " 轮次  1213  当前轮训练集损失 ： 0.34143671852440627\n",
      " 轮次  1214  当前轮训练集损失 ： 0.3414361852661329\n",
      " 轮次  1215  当前轮训练集损失 ： 0.3414356538499306\n",
      " 轮次  1216  当前轮训练集损失 ： 0.34143512426877937\n",
      " 轮次  1217  当前轮训练集损失 ： 0.3414345965156879\n",
      " 轮次  1218  当前轮训练集损失 ： 0.3414340705836947\n",
      " 轮次  1219  当前轮训练集损失 ： 0.34143354646586654\n",
      " 轮次  1220  当前轮训练集损失 ： 0.3414330241552995\n",
      " 轮次  1221  当前轮训练集损失 ： 0.3414325036451184\n",
      " 轮次  1222  当前轮训练集损失 ： 0.3414319849284766\n",
      " 轮次  1223  当前轮训练集损失 ： 0.34143146799855567\n",
      " 轮次  1224  当前轮训练集损失 ： 0.341430952848566\n",
      " 轮次  1225  当前轮训练集损失 ： 0.34143043947174584\n",
      " 轮次  1226  当前轮训练集损失 ： 0.3414299278613616\n",
      " 轮次  1227  当前轮训练集损失 ： 0.34142941801070764\n",
      " 轮次  1228  当前轮训练集损失 ： 0.3414289099131062\n",
      " 轮次  1229  当前轮训练集损失 ： 0.34142840356190723\n",
      " 轮次  1230  当前轮训练集损失 ： 0.3414278989504881\n",
      " 轮次  1231  当前轮训练集损失 ： 0.3414273960722536\n",
      " 轮次  1232  当前轮训练集损失 ： 0.34142689492063627\n",
      " 轮次  1233  当前轮训练集损失 ： 0.3414263954890952\n",
      " 轮次  1234  当前轮训练集损失 ： 0.3414258977711171\n",
      " 轮次  1235  当前轮训练集损失 ： 0.34142540176021513\n",
      " 轮次  1236  当前轮训练集损失 ： 0.3414249074499298\n",
      " 轮次  1237  当前轮训练集损失 ： 0.3414244148338279\n",
      " 轮次  1238  当前轮训练集损失 ： 0.3414239239055029\n",
      " 轮次  1239  当前轮训练集损失 ： 0.3414234346585748\n",
      " 轮次  1240  当前轮训练集损失 ： 0.3414229470866898\n",
      " 轮次  1241  当前轮训练集损失 ： 0.34142246118352065\n",
      " 轮次  1242  当前轮训练集损失 ： 0.3414219769427656\n",
      " 轮次  1243  当前轮训练集损失 ： 0.34142149435814945\n",
      " 轮次  1244  当前轮训练集损失 ： 0.3414210134234226\n",
      " 轮次  1245  当前轮训练集损失 ： 0.34142053413236095\n",
      " 轮次  1246  当前轮训练集损失 ： 0.3414200564787666\n",
      " 轮次  1247  当前轮训练集损失 ： 0.34141958045646675\n",
      " 轮次  1248  当前轮训练集损失 ： 0.3414191060593139\n",
      " 轮次  1249  当前轮训练集损失 ： 0.3414186332811862\n",
      " 轮次  1250  当前轮训练集损失 ： 0.3414181621159866\n",
      " 轮次  1251  当前轮训练集损失 ： 0.3414176925576433\n",
      " 轮次  1252  当前轮训练集损失 ： 0.34141722460010926\n",
      " 轮次  1253  当前轮训练集损失 ： 0.34141675823736267\n",
      " 轮次  1254  当前轮训练集损失 ： 0.34141629346340563\n",
      " 轮次  1255  当前轮训练集损失 ： 0.34141583027226585\n",
      " 轮次  1256  当前轮训练集损失 ： 0.3414153686579946\n",
      " 轮次  1257  当前轮训练集损失 ： 0.3414149086146681\n",
      " 轮次  1258  当前轮训练集损失 ： 0.34141445013638666\n",
      " 轮次  1259  当前轮训练集损失 ： 0.3414139932172746\n",
      " 轮次  1260  当前轮训练集损失 ： 0.3414135378514807\n",
      " 轮次  1261  当前轮训练集损失 ： 0.34141308403317694\n",
      " 轮次  1262  当前轮训练集损失 ： 0.34141263175655984\n",
      " 轮次  1263  当前轮训练集损失 ： 0.3414121810158494\n",
      " 轮次  1264  当前轮训练集损失 ： 0.341411731805289\n",
      " 轮次  1265  当前轮训练集损失 ： 0.3414112841191459\n",
      " 轮次  1266  当前轮训练集损失 ： 0.3414108379517105\n",
      " 轮次  1267  当前轮训练集损失 ： 0.34141039329729667\n",
      " 轮次  1268  当前轮训练集损失 ： 0.3414099501502412\n",
      " 轮次  1269  当前轮训练集损失 ： 0.3414095085049043\n",
      " 轮次  1270  当前轮训练集损失 ： 0.3414090683556688\n",
      " 轮次  1271  当前轮训练集损失 ： 0.34140862969694086\n",
      " 轮次  1272  当前轮训练集损失 ： 0.34140819252314897\n",
      " 轮次  1273  当前轮训练集损失 ： 0.3414077568287445\n",
      " 轮次  1274  当前轮训练集损失 ： 0.3414073226082015\n",
      " 轮次  1275  当前轮训练集损失 ： 0.3414068898560162\n",
      " 轮次  1276  当前轮训练集损失 ： 0.3414064585667075\n",
      " 轮次  1277  当前轮训练集损失 ： 0.34140602873481624\n",
      " 轮次  1278  当前轮训练集损失 ： 0.34140560035490586\n",
      " 轮次  1279  当前轮训练集损失 ： 0.34140517342156146\n",
      " 轮次  1280  当前轮训练集损失 ： 0.34140474792939035\n",
      " 轮次  1281  当前轮训练集损失 ： 0.34140432387302155\n",
      " 轮次  1282  当前轮训练集损失 ： 0.3414039012471062\n",
      " 轮次  1283  当前轮训练集损失 ： 0.34140348004631665\n",
      " 轮次  1284  当前轮训练集损失 ： 0.341403060265347\n",
      " 轮次  1285  当前轮训练集损失 ： 0.341402641898913\n",
      " 轮次  1286  当前轮训练集损失 ： 0.34140222494175176\n",
      " 轮次  1287  当前轮训练集损失 ： 0.34140180938862147\n",
      " 轮次  1288  当前轮训练集损失 ： 0.34140139523430174\n",
      " 轮次  1289  当前轮训练集损失 ： 0.341400982473593\n",
      " 轮次  1290  当前轮训练集损失 ： 0.34140057110131716\n",
      " 轮次  1291  当前轮训练集损失 ： 0.34140016111231664\n",
      " 轮次  1292  当前轮训练集损失 ： 0.3413997525014547\n",
      " 轮次  1293  当前轮训练集损失 ： 0.3413993452636157\n",
      " 轮次  1294  当前轮训练集损失 ： 0.3413989393937042\n",
      " 轮次  1295  当前轮训练集损失 ： 0.3413985348866457\n",
      " 轮次  1296  当前轮训练集损失 ： 0.3413981317373857\n",
      " 轮次  1297  当前轮训练集损失 ： 0.3413977299408906\n",
      " 轮次  1298  当前轮训练集损失 ： 0.3413973294921466\n",
      " 轮次  1299  当前轮训练集损失 ： 0.34139693038616026\n",
      " 轮次  1300  当前轮训练集损失 ： 0.34139653261795855\n",
      " 轮次  1301  当前轮训练集损失 ： 0.3413961361825878\n",
      " 轮次  1302  当前轮训练集损失 ： 0.341395741075115\n",
      " 轮次  1303  当前轮训练集损失 ： 0.3413953472906263\n",
      " 轮次  1304  当前轮训练集损失 ： 0.3413949548242281\n",
      " 轮次  1305  当前轮训练集损失 ： 0.3413945636710461\n",
      " 轮次  1306  当前轮训练集损失 ： 0.3413941738262259\n",
      " 轮次  1307  当前轮训练集损失 ： 0.3413937852849323\n",
      " 轮次  1308  当前轮训练集损失 ： 0.3413933980423494\n",
      " 轮次  1309  当前轮训练集损失 ： 0.34139301209368134\n",
      " 轮次  1310  当前轮训练集损失 ： 0.34139262743415055\n",
      " 轮次  1311  当前轮训练集损失 ： 0.34139224405899904\n",
      " 轮次  1312  当前轮训练集损失 ： 0.3413918619634881\n",
      " 轮次  1313  当前轮训练集损失 ： 0.3413914811428975\n",
      " 轮次  1314  当前轮训练集损失 ： 0.3413911015925263\n",
      " 轮次  1315  当前轮训练集损失 ： 0.3413907233076922\n",
      " 轮次  1316  当前轮训练集损失 ： 0.3413903462837318\n",
      " 轮次  1317  当前轮训练集损失 ： 0.34138997051599984\n",
      " 轮次  1318  当前轮训练集损失 ： 0.3413895959998703\n",
      " 轮次  1319  当前轮训练集损失 ： 0.3413892227307352\n",
      " 轮次  1320  当前轮训练集损失 ： 0.3413888507040051\n",
      " 轮次  1321  当前轮训练集损失 ： 0.3413884799151088\n",
      " 轮次  1322  当前轮训练集损失 ： 0.34138811035949357\n",
      " 轮次  1323  当前轮训练集损失 ： 0.34138774203262456\n",
      " 轮次  1324  当前轮训练集损失 ： 0.3413873749299849\n",
      " 轮次  1325  当前轮训练集损失 ： 0.34138700904707625\n",
      " 轮次  1326  当前轮训练集损失 ： 0.3413866443794178\n",
      " 轮次  1327  当前轮训练集损失 ： 0.3413862809225466\n",
      " 轮次  1328  当前轮训练集损失 ： 0.3413859186720175\n",
      " 轮次  1329  当前轮训练集损失 ： 0.34138555762340317\n",
      " 轮次  1330  当前轮训练集损失 ： 0.34138519777229365\n",
      " 轮次  1331  当前轮训练集损失 ： 0.34138483911429657\n",
      " 轮次  1332  当前轮训练集损失 ： 0.34138448164503726\n",
      " 轮次  1333  当前轮训练集损失 ： 0.3413841253601583\n",
      " 轮次  1334  当前轮训练集损失 ： 0.34138377025531935\n",
      " 轮次  1335  当前轮训练集损失 ： 0.34138341632619756\n",
      " 轮次  1336  当前轮训练集损失 ： 0.3413830635684874\n",
      " 轮次  1337  当前轮训练集损失 ： 0.3413827119778999\n",
      " 轮次  1338  当前轮训练集损失 ： 0.3413823615501634\n",
      " 轮次  1339  当前轮训练集损失 ： 0.34138201228102333\n",
      " 轮次  1340  当前轮训练集损失 ： 0.3413816641662417\n",
      " 轮次  1341  当前轮训练集损失 ： 0.34138131720159737\n",
      " 轮次  1342  当前轮训练集损失 ： 0.34138097138288603\n",
      " 轮次  1343  当前轮训练集损失 ： 0.34138062670591995\n",
      " 轮次  1344  当前轮训练集损失 ： 0.3413802831665279\n",
      " 轮次  1345  当前轮训练集损失 ： 0.34137994076055517\n",
      " 轮次  1346  当前轮训练集损失 ： 0.34137959948386365\n",
      " 轮次  1347  当前轮训练集损失 ： 0.3413792593323313\n",
      " 轮次  1348  当前轮训练集损失 ： 0.3413789203018527\n",
      " 轮次  1349  当前轮训练集损失 ： 0.3413785823883383\n",
      " 轮次  1350  当前轮训练集损失 ： 0.34137824558771473\n",
      " 轮次  1351  当前轮训练集损失 ： 0.3413779098959251\n",
      " 轮次  1352  当前轮训练集损失 ： 0.34137757530892804\n",
      " 轮次  1353  当前轮训练集损失 ： 0.34137724182269835\n",
      " 轮次  1354  当前轮训练集损失 ： 0.3413769094332266\n",
      " 轮次  1355  当前轮训练集损失 ： 0.3413765781365195\n",
      " 轮次  1356  当前轮训练集损失 ： 0.3413762479285988\n",
      " 轮次  1357  当前轮训练集损失 ： 0.3413759188055025\n",
      " 轮次  1358  当前轮训练集损失 ： 0.34137559076328394\n",
      " 轮次  1359  当前轮训练集损失 ： 0.34137526379801225\n",
      " 轮次  1360  当前轮训练集损失 ： 0.3413749379057717\n",
      " 轮次  1361  当前轮训练集损失 ： 0.3413746130826621\n",
      " 轮次  1362  当前轮训练集损失 ： 0.3413742893247985\n",
      " 轮次  1363  当前轮训练集损失 ： 0.3413739666283114\n",
      " 轮次  1364  当前轮训练集损失 ： 0.3413736449893462\n",
      " 轮次  1365  当前轮训练集损失 ： 0.34137332440406387\n",
      " 轮次  1366  当前轮训练集损失 ： 0.34137300486864\n",
      " 轮次  1367  当前轮训练集损失 ： 0.3413726863792654\n",
      " 轮次  1368  当前轮训练集损失 ： 0.34137236893214584\n",
      " 轮次  1369  当前轮训练集损失 ： 0.34137205252350183\n",
      " 轮次  1370  当前轮训练集损失 ： 0.341371737149569\n",
      " 轮次  1371  当前轮训练集损失 ： 0.34137142280659705\n",
      " 轮次  1372  当前轮训练集损失 ： 0.3413711094908513\n",
      " 轮次  1373  当前轮训练集损失 ： 0.34137079719861096\n",
      " 轮次  1374  当前轮训练集损失 ： 0.34137048592617\n",
      " 轮次  1375  当前轮训练集损失 ： 0.3413701756698371\n",
      " 轮次  1376  当前轮训练集损失 ： 0.341369866425935\n",
      " 轮次  1377  当前轮训练集损失 ： 0.3413695581908011\n",
      " 轮次  1378  当前轮训练集损失 ： 0.3413692509607871\n",
      " 轮次  1379  当前轮训练集损失 ： 0.34136894473225876\n",
      " 轮次  1380  当前轮训练集损失 ： 0.3413686395015962\n",
      " 轮次  1381  当前轮训练集损失 ： 0.3413683352651936\n",
      " 轮次  1382  当前轮训练集损失 ： 0.34136803201945926\n",
      " 轮次  1383  当前轮训练集损失 ： 0.34136772976081536\n",
      " 轮次  1384  当前轮训练集损失 ： 0.34136742848569823\n",
      " 轮次  1385  当前轮训练集损失 ： 0.341367128190558\n",
      " 轮次  1386  当前轮训练集损失 ： 0.34136682887185826\n",
      " 轮次  1387  当前轮训练集损失 ： 0.34136653052607724\n",
      " 轮次  1388  当前轮训练集损失 ： 0.34136623314970593\n",
      " 轮次  1389  当前轮训练集损失 ： 0.34136593673924964\n",
      " 轮次  1390  当前轮训练集损失 ： 0.3413656412912271\n",
      " 轮次  1391  当前轮训练集损失 ： 0.3413653468021704\n",
      " 轮次  1392  当前轮训练集损失 ： 0.34136505326862515\n",
      " 轮次  1393  当前轮训练集损失 ： 0.34136476068715066\n",
      " 轮次  1394  当前轮训练集损失 ： 0.3413644690543194\n",
      " 轮次  1395  当前轮训练集损失 ： 0.34136417836671695\n",
      " 轮次  1396  当前轮训练集损失 ： 0.34136388862094263\n",
      " 轮次  1397  当前轮训练集损失 ： 0.3413635998136086\n",
      " 轮次  1398  当前轮训练集损失 ： 0.3413633119413403\n",
      " 轮次  1399  当前轮训练集损失 ： 0.341363025000776\n",
      " 轮次  1400  当前轮训练集损失 ： 0.3413627389885675\n",
      " 轮次  1401  当前轮训练集损失 ： 0.3413624539013792\n",
      " 轮次  1402  当前轮训练集损失 ： 0.34136216973588845\n",
      " 轮次  1403  当前轮训练集损失 ： 0.3413618864887856\n",
      " 轮次  1404  当前轮训练集损失 ： 0.34136160415677375\n",
      " 轮次  1405  当前轮训练集损失 ： 0.3413613227365687\n",
      " 轮次  1406  当前轮训练集损失 ： 0.3413610422248992\n",
      " 轮次  1407  当前轮训练集损失 ： 0.3413607626185061\n",
      " 轮次  1408  当前轮训练集损失 ： 0.3413604839141436\n",
      " 轮次  1409  当前轮训练集损失 ： 0.34136020610857776\n",
      " 轮次  1410  当前轮训练集损失 ： 0.3413599291985877\n",
      " 轮次  1411  当前轮训练集损失 ： 0.34135965318096456\n",
      " 轮次  1412  当前轮训练集损失 ： 0.34135937805251204\n",
      " 轮次  1413  当前轮训练集损失 ： 0.34135910381004614\n",
      " 轮次  1414  当前轮训练集损失 ： 0.34135883045039517\n",
      " 轮次  1415  当前轮训练集损失 ： 0.3413585579703997\n",
      " 轮次  1416  当前轮训练集损失 ： 0.34135828636691234\n",
      " 轮次  1417  当前轮训练集损失 ： 0.34135801563679813\n",
      " 轮次  1418  当前轮训练集损失 ： 0.3413577457769338\n",
      " 轮次  1419  当前轮训练集损失 ： 0.3413574767842084\n",
      " 轮次  1420  当前轮训练集损失 ： 0.341357208655523\n",
      " 轮次  1421  当前轮训练集损失 ： 0.34135694138779\n",
      " 轮次  1422  当前轮训练集损失 ： 0.34135667497793465\n",
      " 轮次  1423  当前轮训练集损失 ： 0.3413564094228933\n",
      " 轮次  1424  当前轮训练集损失 ： 0.34135614471961434\n",
      " 轮次  1425  当前轮训练集损失 ： 0.34135588086505775\n",
      " 轮次  1426  当前轮训练集损失 ： 0.34135561785619534\n",
      " 轮次  1427  当前轮训练集损失 ： 0.3413553556900105\n",
      " 轮次  1428  当前轮训练集损失 ： 0.34135509436349826\n",
      " 轮次  1429  当前轮训练集损失 ： 0.341354833873665\n",
      " 轮次  1430  当前轮训练集损失 ： 0.34135457421752885\n",
      " 轮次  1431  当前轮训练集损失 ： 0.3413543153921192\n",
      " 轮次  1432  当前轮训练集损失 ： 0.3413540573944768\n",
      " 轮次  1433  当前轮训练集损失 ： 0.34135380022165385\n",
      " 轮次  1434  当前轮训练集损失 ： 0.3413535438707138\n",
      " 轮次  1435  当前轮训练集损失 ： 0.34135328833873146\n",
      " 轮次  1436  当前轮训练集损失 ： 0.3413530336227926\n",
      " 轮次  1437  当前轮训练集损失 ： 0.3413527797199944\n",
      " 轮次  1438  当前轮训练集损失 ： 0.3413525266274449\n",
      " 轮次  1439  当前轮训练集损失 ： 0.3413522743422635\n",
      " 轮次  1440  当前轮训练集损失 ： 0.34135202286158045\n",
      " 轮次  1441  当前轮训练集损失 ： 0.34135177218253693\n",
      " 轮次  1442  当前轮训练集损失 ： 0.3413515223022851\n",
      " 轮次  1443  当前轮训练集损失 ： 0.3413512732179879\n",
      " 轮次  1444  当前轮训练集损失 ： 0.3413510249268194\n",
      " 轮次  1445  当前轮训练集损失 ： 0.3413507774259641\n",
      " 轮次  1446  当前轮训练集损失 ： 0.34135053071261756\n",
      " 轮次  1447  当前轮训练集损失 ： 0.3413502847839857\n",
      " 轮次  1448  当前轮训练集损失 ： 0.3413500396372855\n",
      " 轮次  1449  当前轮训练集损失 ： 0.3413497952697442\n",
      " 轮次  1450  当前轮训练集损失 ： 0.34134955167859965\n",
      " 轮次  1451  当前轮训练集损失 ： 0.3413493088611006\n",
      " 轮次  1452  当前轮训练集损失 ： 0.34134906681450583\n",
      " 轮次  1453  当前轮训练集损失 ： 0.34134882553608475\n",
      " 轮次  1454  当前轮训练集损失 ： 0.3413485850231173\n",
      " 轮次  1455  当前轮训练集损失 ： 0.34134834527289337\n",
      " 轮次  1456  当前轮训练集损失 ： 0.34134810628271356\n",
      " 轮次  1457  当前轮训练集损失 ： 0.34134786804988865\n",
      " 轮次  1458  当前轮训练集损失 ： 0.3413476305717394\n",
      " 轮次  1459  当前轮训练集损失 ： 0.34134739384559704\n",
      " 轮次  1460  当前轮训练集损失 ： 0.3413471578688028\n",
      " 轮次  1461  当前轮训练集损失 ： 0.34134692263870825\n",
      " 轮次  1462  当前轮训练集损失 ： 0.3413466881526745\n",
      " 轮次  1463  当前轮训练集损失 ： 0.34134645440807315\n",
      " 轮次  1464  当前轮训练集损失 ： 0.34134622140228565\n",
      " 轮次  1465  当前轮训练集损失 ： 0.34134598913270325\n",
      " 轮次  1466  当前轮训练集损失 ： 0.3413457575967272\n",
      " 轮次  1467  当前轮训练集损失 ： 0.3413455267917687\n",
      " 轮次  1468  当前轮训练集损失 ： 0.34134529671524855\n",
      " 轮次  1469  当前轮训练集损失 ： 0.3413450673645974\n",
      " 轮次  1470  当前轮训练集损失 ： 0.34134483873725574\n",
      " 轮次  1471  当前轮训练集损失 ： 0.34134461083067363\n",
      " 轮次  1472  当前轮训练集损失 ： 0.34134438364231084\n",
      " 轮次  1473  当前轮训练集损失 ： 0.3413441571696366\n",
      " 轮次  1474  当前轮训练集损失 ： 0.34134393141013003\n",
      " 轮次  1475  当前轮训练集损失 ： 0.3413437063612796\n",
      " 轮次  1476  当前轮训练集损失 ： 0.341343482020583\n",
      " 轮次  1477  当前轮训练集损失 ： 0.3413432583855479\n",
      " 轮次  1478  当前轮训练集损失 ： 0.3413430354536911\n",
      " 轮次  1479  当前轮训练集损失 ： 0.3413428132225388\n",
      " 轮次  1480  当前轮训练集损失 ： 0.34134259168962644\n",
      " 轮次  1481  当前轮训练集损失 ： 0.3413423708524989\n",
      " 轮次  1482  当前轮训练集损失 ： 0.3413421507087103\n",
      " 轮次  1483  当前轮训练集损失 ： 0.3413419312558241\n",
      " 轮次  1484  当前轮训练集损失 ： 0.34134171249141276\n",
      " 轮次  1485  当前轮训练集损失 ： 0.34134149441305794\n",
      " 轮次  1486  当前轮训练集损失 ： 0.34134127701835043\n",
      " 轮次  1487  当前轮训练集损失 ： 0.34134106030489003\n",
      " 轮次  1488  当前轮训练集损失 ： 0.3413408442702857\n",
      " 轮次  1489  当前轮训练集损失 ： 0.34134062891215544\n",
      " 轮次  1490  当前轮训练集损失 ： 0.341340414228126\n",
      " 轮次  1491  当前轮训练集损失 ： 0.3413402002158332\n",
      " 轮次  1492  当前轮训练集损失 ： 0.3413399868729217\n",
      " 轮次  1493  当前轮训练集损失 ： 0.34133977419704525\n",
      " 轮次  1494  当前轮训练集损失 ： 0.3413395621858661\n",
      " 轮次  1495  当前轮训练集损失 ： 0.3413393508370552\n",
      " 轮次  1496  当前轮训练集损失 ： 0.3413391401482927\n",
      " 轮次  1497  当前轮训练集损失 ： 0.3413389301172673\n",
      " 轮次  1498  当前轮训练集损失 ： 0.341338720741676\n",
      " 轮次  1499  当前轮训练集损失 ： 0.34133851201922494\n",
      " 轮次  1500  当前轮训练集损失 ： 0.34133830394762865\n",
      " 轮次  1501  当前轮训练集损失 ： 0.34133809652461006\n",
      " 轮次  1502  当前轮训练集损失 ： 0.34133788974790114\n",
      " 轮次  1503  当前轮训练集损失 ： 0.3413376836152419\n",
      " 轮次  1504  当前轮训练集损失 ： 0.341337478124381\n",
      " 轮次  1505  当前轮训练集损失 ： 0.3413372732730753\n",
      " 轮次  1506  当前轮训练集损失 ： 0.34133706905909056\n",
      " 轮次  1507  当前轮训练集损失 ： 0.34133686548020037\n",
      " 轮次  1508  当前轮训练集损失 ： 0.34133666253418715\n",
      " 轮次  1509  当前轮训练集损失 ： 0.341336460218841\n",
      " 轮次  1510  当前轮训练集损失 ： 0.3413362585319611\n",
      " 轮次  1511  当前轮训练集损失 ： 0.34133605747135404\n",
      " 轮次  1512  当前轮训练集损失 ： 0.3413358570348352\n",
      " 轮次  1513  当前轮训练集损失 ： 0.3413356572202277\n",
      " 轮次  1514  当前轮训练集损失 ： 0.34133545802536325\n",
      " 轮次  1515  当前轮训练集损失 ： 0.3413352594480811\n",
      " 轮次  1516  当前轮训练集损失 ： 0.3413350614862291\n",
      " 轮次  1517  当前轮训练集损失 ： 0.34133486413766295\n",
      " 轮次  1518  当前轮训练集损失 ： 0.34133466740024615\n",
      " 轮次  1519  当前轮训练集损失 ： 0.3413344712718504\n",
      " 轮次  1520  当前轮训练集损失 ： 0.3413342757503551\n",
      " 轮次  1521  当前轮训练集损失 ： 0.34133408083364797\n",
      " 轮次  1522  当前轮训练集损失 ： 0.3413338865196243\n",
      " 轮次  1523  当前轮训练集损失 ： 0.3413336928061873\n",
      " 轮次  1524  当前轮训练集损失 ： 0.3413334996912478\n",
      " 轮次  1525  当前轮训练集损失 ： 0.34133330717272486\n",
      " 轮次  1526  当前轮训练集损失 ： 0.3413331152485447\n",
      " 轮次  1527  当前轮训练集损失 ： 0.34133292391664183\n",
      " 轮次  1528  当前轮训练集损失 ： 0.34133273317495794\n",
      " 轮次  1529  当前轮训练集损失 ： 0.3413325430214428\n",
      " 轮次  1530  当前轮训练集损失 ： 0.3413323534540536\n",
      " 轮次  1531  当前轮训练集损失 ： 0.3413321644707551\n",
      " 轮次  1532  当前轮训练集损失 ： 0.3413319760695197\n",
      " 轮次  1533  当前轮训练集损失 ： 0.3413317882483272\n",
      " 轮次  1534  当前轮训练集损失 ： 0.3413316010051651\n",
      " 轮次  1535  当前轮训练集损失 ： 0.3413314143380283\n",
      " 轮次  1536  当前轮训练集损失 ： 0.3413312282449191\n",
      " 轮次  1537  当前轮训练集损失 ： 0.3413310427238472\n",
      " 轮次  1538  当前轮训练集损失 ： 0.3413308577728298\n",
      " 轮次  1539  当前轮训练集损失 ： 0.34133067338989126\n",
      " 轮次  1540  当前轮训练集损失 ： 0.34133048957306344\n",
      " 轮次  1541  当前轮训练集损失 ： 0.3413303063203854\n",
      " 轮次  1542  当前轮训练集损失 ： 0.3413301236299036\n",
      " 轮次  1543  当前轮训练集损失 ： 0.34132994149967155\n",
      " 轮次  1544  当前轮训练集损失 ： 0.34132975992775\n",
      " 轮次  1545  当前轮训练集损失 ： 0.341329578912207\n",
      " 轮次  1546  当前轮训练集损失 ： 0.34132939845111787\n",
      " 轮次  1547  当前轮训练集损失 ： 0.34132921854256454\n",
      " 轮次  1548  当前轮训练集损失 ： 0.34132903918463653\n",
      " 轮次  1549  当前轮训练集损失 ： 0.34132886037543014\n",
      " 轮次  1550  当前轮训练集损失 ： 0.34132868211304906\n",
      " 轮次  1551  当前轮训练集损失 ： 0.34132850439560347\n",
      " 轮次  1552  当前轮训练集损失 ： 0.34132832722121104\n",
      " 轮次  1553  当前轮训练集损失 ： 0.3413281505879961\n",
      " 轮次  1554  当前轮训练集损失 ： 0.34132797449409\n",
      " 轮次  1555  当前轮训练集损失 ： 0.3413277989376309\n",
      " 轮次  1556  当前轮训练集损失 ： 0.34132762391676386\n",
      " 轮次  1557  当前轮训练集损失 ： 0.341327449429641\n",
      " 轮次  1558  当前轮训练集损失 ： 0.3413272754744209\n",
      " 轮次  1559  当前轮训练集损失 ： 0.3413271020492692\n",
      " 轮次  1560  当前轮训练集损失 ： 0.3413269291523582\n",
      " 轮次  1561  当前轮训练集损失 ： 0.3413267567818669\n",
      " 轮次  1562  当前轮训练集损失 ： 0.3413265849359811\n",
      " 轮次  1563  当前轮训练集损失 ： 0.3413264136128932\n",
      " 轮次  1564  当前轮训练集损失 ： 0.34132624281080237\n",
      " 轮次  1565  当前轮训练集损失 ： 0.34132607252791414\n",
      " 轮次  1566  当前轮训练集损失 ： 0.34132590276244107\n",
      " 轮次  1567  当前轮训练集损失 ： 0.34132573351260204\n",
      " 轮次  1568  当前轮训练集损失 ： 0.34132556477662246\n",
      " 轮次  1569  当前轮训练集损失 ： 0.34132539655273425\n",
      " 轮次  1570  当前轮训练集损失 ： 0.3413252288391758\n",
      " 轮次  1571  当前轮训练集损失 ： 0.34132506163419235\n",
      " 轮次  1572  当前轮训练集损失 ： 0.3413248949360352\n",
      " 轮次  1573  当前轮训练集损失 ： 0.341324728742962\n",
      " 轮次  1574  当前轮训练集损失 ： 0.34132456305323716\n",
      " 轮次  1575  当前轮训练集损失 ： 0.3413243978651311\n",
      " 轮次  1576  当前轮训练集损失 ： 0.341324233176921\n",
      " 轮次  1577  当前轮训练集损失 ： 0.34132406898689\n",
      " 轮次  1578  当前轮训练集损失 ： 0.3413239052933276\n",
      " 轮次  1579  当前轮训练集损失 ： 0.34132374209452976\n",
      " 轮次  1580  当前轮训练集损失 ： 0.3413235793887985\n",
      " 轮次  1581  当前轮训练集损失 ： 0.3413234171744422\n",
      " 轮次  1582  当前轮训练集损失 ： 0.3413232554497752\n",
      " 轮次  1583  当前轮训练集损失 ： 0.3413230942131183\n",
      " 轮次  1584  当前轮训练集损失 ： 0.34132293346279824\n",
      " 轮次  1585  当前轮训练集损失 ： 0.3413227731971481\n",
      " 轮次  1586  当前轮训练集损失 ： 0.34132261341450687\n",
      " 轮次  1587  当前轮训练集损失 ： 0.34132245411321965\n",
      " 轮次  1588  当前轮训练集损失 ： 0.34132229529163766\n",
      " 轮次  1589  当前轮训练集损失 ： 0.341322136948118\n",
      " 轮次  1590  当前轮训练集损失 ： 0.34132197908102413\n",
      " 轮次  1591  当前轮训练集损失 ： 0.34132182168872505\n",
      " 轮次  1592  当前轮训练集损失 ： 0.3413216647695961\n",
      " 轮次  1593  当前轮训练集损失 ： 0.3413215083220182\n",
      " 轮次  1594  当前轮训练集损失 ： 0.3413213523443783\n",
      " 轮次  1595  当前轮训练集损失 ： 0.3413211968350697\n",
      " 轮次  1596  当前轮训练集损失 ： 0.3413210417924907\n",
      " 轮次  1597  当前轮训练集损失 ： 0.34132088721504605\n",
      " 轮次  1598  当前轮训练集损失 ： 0.3413207331011464\n",
      " 轮次  1599  当前轮训练集损失 ： 0.34132057944920774\n",
      " 轮次  1600  当前轮训练集损失 ： 0.3413204262576522\n",
      " 轮次  1601  当前轮训练集损失 ： 0.3413202735249074\n",
      " 轮次  1602  当前轮训练集损失 ： 0.34132012124940686\n",
      " 轮次  1603  当前轮训练集损失 ： 0.3413199694295899\n",
      " 轮次  1604  当前轮训练集损失 ： 0.34131981806390127\n",
      " 轮次  1605  当前轮训练集损失 ： 0.34131966715079154\n",
      " 轮次  1606  当前轮训练集损失 ： 0.3413195166887168\n",
      " 轮次  1607  当前轮训练集损失 ： 0.3413193666761391\n",
      " 轮次  1608  当前轮训练集损失 ： 0.3413192171115255\n",
      " 轮次  1609  当前轮训练集损失 ： 0.34131906799334927\n",
      " 轮次  1610  当前轮训练集损失 ： 0.3413189193200888\n",
      " 轮次  1611  当前轮训练集损失 ： 0.34131877109022796\n",
      " 轮次  1612  当前轮训练集损失 ： 0.3413186233022566\n",
      " 轮次  1613  当前轮训练集损失 ： 0.3413184759546697\n",
      " 轮次  1614  当前轮训练集损失 ： 0.3413183290459677\n",
      " 轮次  1615  当前轮训练集损失 ： 0.3413181825746565\n",
      " 轮次  1616  当前轮训练集损失 ： 0.34131803653924775\n",
      " 轮次  1617  当前轮训练集损失 ： 0.341317890938258\n",
      " 轮次  1618  当前轮训练集损失 ： 0.34131774577020957\n",
      " 轮次  1619  当前轮训练集损失 ： 0.34131760103362985\n",
      " 轮次  1620  当前轮训练集损失 ： 0.34131745672705177\n",
      " 轮次  1621  当前轮训练集损失 ： 0.3413173128490135\n",
      " 轮次  1622  当前轮训练集损失 ： 0.3413171693980588\n",
      " 轮次  1623  当前轮训练集损失 ： 0.34131702637273603\n",
      " 轮次  1624  当前轮训练集损失 ： 0.3413168837715994\n",
      " 轮次  1625  当前轮训练集损失 ： 0.3413167415932083\n",
      " 轮次  1626  当前轮训练集损失 ： 0.34131659983612705\n",
      " 轮次  1627  当前轮训练集损失 ： 0.34131645849892533\n",
      " 轮次  1628  当前轮训练集损失 ： 0.3413163175801781\n",
      " 轮次  1629  当前轮训练集损失 ： 0.34131617707846534\n",
      " 轮次  1630  当前轮训练集损失 ： 0.3413160369923722\n",
      " 轮次  1631  当前轮训练集损失 ： 0.341315897320489\n",
      " 轮次  1632  当前轮训练集损失 ： 0.34131575806141085\n",
      " 轮次  1633  当前轮训练集损失 ： 0.34131561921373865\n",
      " 轮次  1634  当前轮训练集损失 ： 0.3413154807760774\n",
      " 轮次  1635  当前轮训练集损失 ： 0.341315342747038\n",
      " 轮次  1636  当前轮训练集损失 ： 0.3413152051252359\n",
      " 轮次  1637  当前轮训练集损失 ： 0.3413150679092915\n",
      " 轮次  1638  当前轮训练集损失 ： 0.34131493109783073\n",
      " 轮次  1639  当前轮训练集损失 ： 0.34131479468948367\n",
      " 轮次  1640  当前轮训练集损失 ： 0.341314658682886\n",
      " 轮次  1641  当前轮训练集损失 ： 0.34131452307667787\n",
      " 轮次  1642  当前轮训练集损失 ： 0.3413143878695048\n",
      " 轮次  1643  当前轮训练集损失 ： 0.3413142530600168\n",
      " 轮次  1644  当前轮训练集损失 ： 0.34131411864686895\n",
      " 轮次  1645  当前轮训练集损失 ： 0.34131398462872115\n",
      " 轮次  1646  当前轮训练集损失 ： 0.3413138510042379\n",
      " 轮次  1647  当前轮训练集损失 ： 0.34131371777208874\n",
      " 轮次  1648  当前轮训练集损失 ： 0.3413135849309481\n",
      " 轮次  1649  当前轮训练集损失 ： 0.34131345247949507\n",
      " 轮次  1650  当前轮训练集损失 ： 0.3413133204164132\n",
      " 轮次  1651  当前轮训练集损失 ： 0.34131318874039135\n",
      " 轮次  1652  当前轮训练集损失 ： 0.34131305745012264\n",
      " 轮次  1653  当前轮训练集损失 ： 0.341312926544305\n",
      " 轮次  1654  当前轮训练集损失 ： 0.34131279602164116\n",
      " 轮次  1655  当前轮训练集损失 ： 0.3413126658808384\n",
      " 轮次  1656  当前轮训练集损失 ： 0.34131253612060863\n",
      " 轮次  1657  当前轮训练集损失 ： 0.3413124067396687\n",
      " 轮次  1658  当前轮训练集损失 ： 0.34131227773673944\n",
      " 轮次  1659  当前轮训练集损失 ： 0.3413121491105468\n",
      " 轮次  1660  当前轮训练集损失 ： 0.34131202085982126\n",
      " 轮次  1661  当前轮训练集损失 ： 0.3413118929832976\n",
      " 轮次  1662  当前轮训练集损失 ： 0.34131176547971526\n",
      " 轮次  1663  当前轮训练集损失 ： 0.34131163834781836\n",
      " 轮次  1664  当前轮训练集损失 ： 0.3413115115863553\n",
      " 轮次  1665  当前轮训练集损失 ： 0.341311385194079\n",
      " 轮次  1666  当前轮训练集损失 ： 0.341311259169747\n",
      " 轮次  1667  当前轮训练集损失 ： 0.3413111335121213\n",
      " 轮次  1668  当前轮训练集损失 ： 0.3413110082199679\n",
      " 轮次  1669  当前轮训练集损失 ： 0.3413108832920578\n",
      " 轮次  1670  当前轮训练集损失 ： 0.34131075872716615\n",
      " 轮次  1671  当前轮训练集损失 ： 0.34131063452407245\n",
      " 轮次  1672  当前轮训练集损失 ： 0.3413105106815606\n",
      " 轮次  1673  当前轮训练集损失 ： 0.341310387198419\n",
      " 轮次  1674  当前轮训练集损失 ： 0.34131026407344\n",
      " 轮次  1675  当前轮训练集损失 ： 0.34131014130542064\n",
      " 轮次  1676  当前轮训练集损失 ： 0.3413100188931622\n",
      " 轮次  1677  当前轮训练集损失 ： 0.34130989683547014\n",
      " 轮次  1678  当前轮训练集损失 ： 0.34130977513115407\n",
      " 轮次  1679  当前轮训练集损失 ： 0.3413096537790283\n",
      " 轮次  1680  当前轮训练集损失 ： 0.34130953277791104\n",
      " 轮次  1681  当前轮训练集损失 ： 0.3413094121266245\n",
      " 轮次  1682  当前轮训练集损失 ： 0.34130929182399566\n",
      " 轮次  1683  当前轮训练集损失 ： 0.34130917186885534\n",
      " 轮次  1684  当前轮训练集损失 ： 0.3413090522600385\n",
      " 轮次  1685  当前轮训练集损失 ： 0.3413089329963843\n",
      " 轮次  1686  当前轮训练集损失 ： 0.34130881407673636\n",
      " 轮次  1687  当前轮训练集损失 ： 0.34130869549994186\n",
      " 轮次  1688  当前轮训练集损失 ： 0.34130857726485264\n",
      " 轮次  1689  当前轮训练集损失 ： 0.34130845937032417\n",
      " 轮次  1690  当前轮训练集损失 ： 0.34130834181521624\n",
      " 轮次  1691  当前轮训练集损失 ： 0.34130822459839266\n",
      " 轮次  1692  当前轮训练集损失 ： 0.34130810771872133\n",
      " 轮次  1693  当前轮训练集损失 ： 0.3413079911750742\n",
      " 轮次  1694  当前轮训练集损失 ： 0.34130787496632725\n",
      " 轮次  1695  当前轮训练集损失 ： 0.34130775909136013\n",
      " 轮次  1696  当前轮训练集损失 ： 0.3413076435490569\n",
      " 轮次  1697  当前轮训练集损失 ： 0.34130752833830547\n",
      " 轮次  1698  当前轮训练集损失 ： 0.3413074134579976\n",
      " 轮次  1699  当前轮训练集损失 ： 0.3413072989070291\n",
      " 轮次  1700  当前轮训练集损失 ： 0.3413071846842996\n",
      " 轮次  1701  当前轮训练集损失 ： 0.34130707078871275\n",
      " 轮次  1702  当前轮训练集损失 ： 0.34130695721917603\n",
      " 轮次  1703  当前轮训练集损失 ： 0.34130684397460087\n",
      " 轮次  1704  当前轮训练集损失 ： 0.3413067310539025\n",
      " 轮次  1705  当前轮训练集损失 ： 0.341306618456\n",
      " 轮次  1706  当前轮训练集损失 ： 0.34130650617981634\n",
      " 轮次  1707  当前轮训练集损失 ： 0.3413063942242782\n",
      " 轮次  1708  当前轮训练集损失 ： 0.3413062825883164\n",
      " 轮次  1709  当前轮训练集损失 ： 0.341306171270865\n",
      " 轮次  1710  当前轮训练集损失 ： 0.34130606027086213\n",
      " 轮次  1711  当前轮训练集损失 ： 0.34130594958725\n",
      " 轮次  1712  当前轮训练集损失 ： 0.34130583921897417\n",
      " 轮次  1713  当前轮训练集损失 ： 0.341305729164984\n",
      " 轮次  1714  当前轮训练集损失 ： 0.34130561942423243\n",
      " 轮次  1715  当前轮训练集损失 ： 0.3413055099956766\n",
      " 轮次  1716  当前轮训练集损失 ： 0.34130540087827677\n",
      " 轮次  1717  当前轮训练集损失 ： 0.3413052920709974\n",
      " 轮次  1718  当前轮训练集损失 ： 0.34130518357280615\n",
      " 轮次  1719  当前轮训练集损失 ： 0.3413050753826747\n",
      " 轮次  1720  当前轮训练集损失 ： 0.34130496749957817\n",
      " 轮次  1721  当前轮训练集损失 ： 0.34130485992249526\n",
      " 轮次  1722  当前轮训练集损失 ： 0.34130475265040855\n",
      " 轮次  1723  当前轮训练集损失 ： 0.3413046456823039\n",
      " 轮次  1724  当前轮训练集损失 ： 0.34130453901717095\n",
      " 轮次  1725  当前轮训练集损失 ： 0.3413044326540028\n",
      " 轮次  1726  当前轮训练集损失 ： 0.3413043265917963\n",
      " 轮次  1727  当前轮训练集损失 ： 0.34130422082955164\n",
      " 轮次  1728  当前轮训练集损失 ： 0.3413041153662726\n",
      " 轮次  1729  当前轮训练集损失 ： 0.3413040102009666\n",
      " 轮次  1730  当前轮训练集损失 ： 0.34130390533264426\n",
      " 轮次  1731  当前轮训练集损失 ： 0.34130380076032\n",
      " 轮次  1732  当前轮训练集损失 ： 0.34130369648301173\n",
      " 轮次  1733  当前轮训练集损失 ： 0.3413035924997405\n",
      " 轮次  1734  当前轮训练集损失 ： 0.3413034888095311\n",
      " 轮次  1735  当前轮训练集损失 ： 0.34130338541141175\n",
      " 轮次  1736  当前轮训练集损失 ： 0.34130328230441387\n",
      " 轮次  1737  当前轮训练集损失 ： 0.3413031794875726\n",
      " 轮次  1738  当前轮训练集损失 ： 0.3413030769599262\n",
      " 轮次  1739  当前轮训练集损失 ： 0.3413029747205164\n",
      " 轮次  1740  当前轮训练集损失 ： 0.3413028727683887\n",
      " 轮次  1741  当前轮训练集损失 ： 0.3413027711025913\n",
      " 轮次  1742  当前轮训练集损失 ： 0.34130266972217593\n",
      " 轮次  1743  当前轮训练集损失 ： 0.3413025686261981\n",
      " 轮次  1744  当前轮训练集损失 ： 0.34130246781371626\n",
      " 轮次  1745  当前轮训练集损失 ： 0.34130236728379226\n",
      " 轮次  1746  当前轮训练集损失 ： 0.3413022670354911\n",
      " 轮次  1747  当前轮训练集损失 ： 0.34130216706788136\n",
      " 轮次  1748  当前轮训练集损失 ： 0.3413020673800347\n",
      " 轮次  1749  当前轮训练集损失 ： 0.3413019679710261\n",
      " 轮次  1750  当前轮训练集损失 ： 0.3413018688399337\n",
      " 轮次  1751  当前轮训练集损失 ： 0.34130176998583905\n",
      " 轮次  1752  当前轮训练集损失 ： 0.34130167140782675\n",
      " 轮次  1753  当前轮训练集损失 ： 0.3413015731049847\n",
      " 轮次  1754  当前轮训练集损失 ： 0.3413014750764043\n",
      " 轮次  1755  当前轮训练集损失 ： 0.34130137732117943\n",
      " 轮次  1756  当前轮训练集损失 ： 0.3413012798384078\n",
      " 轮次  1757  当前轮训练集损失 ： 0.34130118262718995\n",
      " 轮次  1758  当前轮训练集损失 ： 0.34130108568662965\n",
      " 轮次  1759  当前轮训练集损失 ： 0.341300989015834\n",
      " 轮次  1760  当前轮训练集损失 ： 0.3413008926139128\n",
      " 轮次  1761  当前轮训练集损失 ： 0.3413007964799796\n",
      " 轮次  1762  当前轮训练集损失 ： 0.3413007006131505\n",
      " 轮次  1763  当前轮训练集损失 ： 0.3413006050125448\n",
      " 轮次  1764  当前轮训练集损失 ： 0.3413005096772852\n",
      " 轮次  1765  当前轮训练集损失 ： 0.341300414606497\n",
      " 轮次  1766  当前轮训练集损失 ： 0.34130031979930914\n",
      " 轮次  1767  当前轮训练集损失 ： 0.34130022525485304\n",
      " 轮次  1768  当前轮训练集损失 ： 0.3413001309722635\n",
      " 轮次  1769  当前轮训练集损失 ： 0.34130003695067834\n",
      " 轮次  1770  当前轮训练集损失 ： 0.34129994318923795\n",
      " 轮次  1771  当前轮训练集损失 ： 0.34129984968708665\n",
      " 轮次  1772  当前轮训练集损失 ： 0.34129975644337085\n",
      " 轮次  1773  当前轮训练集损失 ： 0.34129966345724033\n",
      " 轮次  1774  当前轮训练集损失 ： 0.3412995707278479\n",
      " 轮次  1775  当前轮训练集损失 ： 0.34129947825434903\n",
      " 轮次  1776  当前轮训练集损失 ： 0.34129938603590265\n",
      " 轮次  1777  当前轮训练集损失 ： 0.34129929407167003\n",
      " 轮次  1778  当前轮训练集损失 ： 0.3412992023608159\n",
      " 轮次  1779  当前轮训练集损失 ： 0.3412991109025074\n",
      " 轮次  1780  当前轮训练集损失 ： 0.3412990196959153\n",
      " 轮次  1781  当前轮训练集损失 ： 0.34129892874021234\n",
      " 轮次  1782  当前轮训练集损失 ： 0.3412988380345749\n",
      " 轮次  1783  当前轮训练集损失 ： 0.34129874757818185\n",
      " 轮次  1784  当前轮训练集损失 ： 0.34129865737021503\n",
      " 轮次  1785  当前轮训练集损失 ： 0.34129856740985925\n",
      " 轮次  1786  当前轮训练集损失 ： 0.3412984776963019\n",
      " 轮次  1787  当前轮训练集损失 ： 0.34129838822873343\n",
      " 轮次  1788  当前轮训练集损失 ： 0.34129829900634706\n",
      " 轮次  1789  当前轮训练集损失 ： 0.34129821002833866\n",
      " 轮次  1790  当前轮训练集损失 ： 0.34129812129390724\n",
      " 轮次  1791  当前轮训练集损失 ： 0.3412980328022542\n",
      " 轮次  1792  当前轮训练集损失 ： 0.3412979445525841\n",
      " 轮次  1793  当前轮训练集损失 ： 0.34129785654410394\n",
      " 轮次  1794  当前轮训练集损失 ： 0.3412977687760235\n",
      " 轮次  1795  当前轮训练集损失 ： 0.3412976812475559\n",
      " 轮次  1796  当前轮训练集损失 ： 0.34129759395791626\n",
      " 轮次  1797  当前轮训练集损失 ： 0.34129750690632255\n",
      " 轮次  1798  当前轮训练集损失 ： 0.3412974200919959\n",
      " 轮次  1799  当前轮训练集损失 ： 0.34129733351415964\n",
      " 轮次  1800  当前轮训练集损失 ： 0.3412972471720403\n",
      " 轮次  1801  当前轮训练集损失 ： 0.3412971610648664\n",
      " 轮次  1802  当前轮训练集损失 ： 0.34129707519186997\n",
      " 轮次  1803  当前轮训练集损失 ： 0.3412969895522852\n",
      " 轮次  1804  当前轮训练集损失 ： 0.3412969041453489\n",
      " 轮次  1805  当前轮训练集损失 ： 0.3412968189703008\n",
      " 轮次  1806  当前轮训练集损失 ： 0.3412967340263832\n",
      " 轮次  1807  当前轮训练集损失 ： 0.34129664931284087\n",
      " 轮次  1808  当前轮训练集损失 ： 0.34129656482892134\n",
      " 轮次  1809  当前轮训练集损失 ： 0.34129648057387474\n",
      " 轮次  1810  当前轮训练集损失 ： 0.34129639654695376\n",
      " 轮次  1811  当前轮训练集损失 ： 0.3412963127474137\n",
      " 轮次  1812  当前轮训练集损失 ： 0.34129622917451247\n",
      " 轮次  1813  当前轮训练集损失 ： 0.3412961458275105\n",
      " 轮次  1814  当前轮训练集损失 ： 0.34129606270567087\n",
      " 轮次  1815  当前轮训练集损失 ： 0.341295979808259\n",
      " 轮次  1816  当前轮训练集损失 ： 0.3412958971345432\n",
      " 轮次  1817  当前轮训练集损失 ： 0.34129581468379405\n",
      " 轮次  1818  当前轮训练集损失 ： 0.3412957324552848\n",
      " 轮次  1819  当前轮训练集损失 ： 0.3412956504482911\n",
      " 轮次  1820  当前轮训练集损失 ： 0.34129556866209104\n",
      " 轮次  1821  当前轮训练集损失 ： 0.3412954870959655\n",
      " 轮次  1822  当前轮训练集损失 ： 0.34129540574919764\n",
      " 轮次  1823  当前轮训练集损失 ： 0.3412953246210731\n",
      " 轮次  1824  当前轮训练集损失 ： 0.3412952437108801\n",
      " 轮次  1825  当前轮训练集损失 ： 0.34129516301790913\n",
      " 轮次  1826  当前轮训练集损失 ： 0.34129508254145347\n",
      " 轮次  1827  当前轮训练集损失 ： 0.3412950022808083\n",
      " 轮次  1828  当前轮训练集损失 ： 0.34129492223527186\n",
      " 轮次  1829  当前轮训练集损失 ： 0.34129484240414437\n",
      " 轮次  1830  当前轮训练集损失 ： 0.34129476278672866\n",
      " 轮次  1831  当前轮训练集损失 ： 0.34129468338232993\n",
      " 轮次  1832  当前轮训练集损失 ： 0.3412946041902558\n",
      " 轮次  1833  当前轮训练集损失 ： 0.3412945252098161\n",
      " 轮次  1834  当前轮训练集损失 ： 0.3412944464403235\n",
      " 轮次  1835  当前轮训练集损失 ： 0.34129436788109246\n",
      " 轮次  1836  当前轮训练集损失 ： 0.3412942895314402\n",
      " 轮次  1837  当前轮训练集损失 ： 0.3412942113906862\n",
      " 轮次  1838  当前轮训练集损失 ： 0.34129413345815224\n",
      " 轮次  1839  当前轮训练集损失 ： 0.3412940557331625\n",
      " 轮次  1840  当前轮训练集损失 ： 0.34129397821504354\n",
      " 轮次  1841  当前轮训练集损失 ： 0.3412939009031239\n",
      " 轮次  1842  当前轮训练集损失 ： 0.34129382379673506\n",
      " 轮次  1843  当前轮训练集损失 ： 0.3412937468952101\n",
      " 轮次  1844  当前轮训练集损失 ： 0.3412936701978849\n",
      " 轮次  1845  当前轮训练集损失 ： 0.34129359370409745\n",
      " 轮次  1846  当前轮训练集损失 ： 0.3412935174131882\n",
      " 轮次  1847  当前轮训练集损失 ： 0.34129344132449935\n",
      " 轮次  1848  当前轮训练集损失 ： 0.341293365437376\n",
      " 轮次  1849  当前轮训练集损失 ： 0.3412932897511651\n",
      " 轮次  1850  当前轮训练集损失 ： 0.34129321426521614\n",
      " 轮次  1851  当前轮训练集损失 ： 0.34129313897888053\n",
      " 轮次  1852  当前轮训练集损失 ： 0.341293063891512\n",
      " 轮次  1853  当前轮训练集损失 ： 0.34129298900246674\n",
      " 轮次  1854  当前轮训练集损失 ： 0.341292914311103\n",
      " 轮次  1855  当前轮训练集损失 ： 0.341292839816781\n",
      " 轮次  1856  当前轮训练集损失 ： 0.3412927655188635\n",
      " 轮次  1857  当前轮训练集损失 ： 0.34129269141671537\n",
      " 轮次  1858  当前轮训练集损失 ： 0.3412926175097036\n",
      " 轮次  1859  当前轮训练集损失 ： 0.3412925437971973\n",
      " 轮次  1860  当前轮训练集损失 ： 0.3412924702785679\n",
      " 轮次  1861  当前轮训练集损失 ： 0.3412923969531889\n",
      " 轮次  1862  当前轮训练集损失 ： 0.34129232382043584\n",
      " 轮次  1863  当前轮训练集损失 ： 0.34129225087968673\n",
      " 轮次  1864  当前轮训练集损失 ： 0.34129217813032126\n",
      " 轮次  1865  当前轮训练集损失 ： 0.34129210557172174\n",
      " 轮次  1866  当前轮训练集损失 ： 0.34129203320327217\n",
      " 轮次  1867  当前轮训练集损失 ： 0.3412919610243591\n",
      " 轮次  1868  当前轮训练集损失 ： 0.3412918890343706\n",
      " 轮次  1869  当前轮训练集损失 ： 0.3412918172326973\n",
      " 轮次  1870  当前轮训练集损失 ： 0.3412917456187319\n",
      " 轮次  1871  当前轮训练集损失 ： 0.34129167419186884\n",
      " 轮次  1872  当前轮训练集损失 ： 0.3412916029515051\n",
      " 轮次  1873  当前轮训练集损失 ： 0.34129153189703937\n",
      " 轮次  1874  当前轮训练集损失 ： 0.34129146102787256\n",
      " 轮次  1875  当前轮训练集损失 ： 0.34129139034340755\n",
      " 轮次  1876  当前轮训练集损失 ： 0.3412913198430493\n",
      " 轮次  1877  当前轮训练集损失 ： 0.3412912495262049\n",
      " 轮次  1878  当前轮训练集损失 ： 0.3412911793922832\n",
      " 轮次  1879  当前轮训练集损失 ： 0.34129110944069546\n",
      " 轮次  1880  当前轮训练集损失 ： 0.34129103967085467\n",
      " 轮次  1881  当前轮训练集损失 ： 0.3412909700821759\n",
      " 轮次  1882  当前轮训练集损失 ： 0.3412909006740762\n",
      " 轮次  1883  当前轮训练集损失 ： 0.3412908314459747\n",
      " 轮次  1884  当前轮训练集损失 ： 0.3412907623972924\n",
      " 轮次  1885  当前轮训练集损失 ： 0.3412906935274523\n",
      " 轮次  1886  当前轮训练集损失 ： 0.3412906248358794\n",
      " 轮次  1887  当前轮训练集损失 ： 0.34129055632200084\n",
      " 轮次  1888  当前轮训练集损失 ： 0.3412904879852454\n",
      " 轮次  1889  当前轮训练集损失 ： 0.3412904198250441\n",
      " 轮次  1890  当前轮训练集损失 ： 0.3412903518408295\n",
      " 轮次  1891  当前轮训练集损失 ： 0.34129028403203654\n",
      " 轮次  1892  当前轮训练集损失 ： 0.341290216398102\n",
      " 轮次  1893  当前轮训练集损失 ： 0.3412901489384642\n",
      " 轮次  1894  当前轮训练集损失 ： 0.3412900816525639\n",
      " 轮次  1895  当前轮训练集损失 ： 0.34129001453984353\n",
      " 轮次  1896  当前轮训练集损失 ： 0.3412899475997473\n",
      " 轮次  1897  当前轮训练集损失 ： 0.34128988083172146\n",
      " 轮次  1898  当前轮训练集损失 ： 0.3412898142352142\n",
      " 轮次  1899  当前轮训练集损失 ： 0.3412897478096755\n",
      " 轮次  1900  当前轮训练集损失 ： 0.34128968155455724\n",
      " 轮次  1901  当前轮训练集损失 ： 0.34128961546931313\n",
      " 轮次  1902  当前轮训练集损失 ： 0.34128954955339885\n",
      " 轮次  1903  当前轮训练集损失 ： 0.34128948380627167\n",
      " 轮次  1904  当前轮训练集损失 ： 0.34128941822739106\n",
      " 轮次  1905  当前轮训练集损失 ： 0.3412893528162182\n",
      " 轮次  1906  当前轮训练集损失 ： 0.34128928757221577\n",
      " 轮次  1907  当前轮训练集损失 ： 0.34128922249484894\n",
      " 轮次  1908  当前轮训练集损失 ： 0.3412891575835843\n",
      " 轮次  1909  当前轮训练集损失 ： 0.34128909283789005\n",
      " 轮次  1910  当前轮训练集损失 ： 0.3412890282572365\n",
      " 轮次  1911  当前轮训练集损失 ： 0.3412889638410959\n",
      " 轮次  1912  当前轮训练集损失 ： 0.34128889958894204\n",
      " 轮次  1913  当前轮训练集损失 ： 0.34128883550025046\n",
      " 轮次  1914  当前轮训练集损失 ： 0.3412887715744985\n",
      " 轮次  1915  当前轮训练集损失 ： 0.3412887078111655\n",
      " 轮次  1916  当前轮训练集损失 ： 0.3412886442097325\n",
      " 轮次  1917  当前轮训练集损失 ： 0.34128858076968205\n",
      " 轮次  1918  当前轮训练集损失 ： 0.3412885174904987\n",
      " 轮次  1919  当前轮训练集损失 ： 0.3412884543716687\n",
      " 轮次  1920  当前轮训练集损失 ： 0.34128839141268\n",
      " 轮次  1921  当前轮训练集损失 ： 0.34128832861302233\n",
      " 轮次  1922  当前轮训练集损失 ： 0.34128826597218714\n",
      " 轮次  1923  当前轮训练集损失 ： 0.3412882034896678\n",
      " 轮次  1924  当前轮训练集损失 ： 0.34128814116495887\n",
      " 轮次  1925  当前轮训练集损失 ： 0.34128807899755714\n",
      " 轮次  1926  当前轮训练集损失 ： 0.3412880169869608\n",
      " 轮次  1927  当前轮训练集损失 ： 0.34128795513267013\n",
      " 轮次  1928  当前轮训练集损失 ： 0.34128789343418653\n",
      " 轮次  1929  当前轮训练集损失 ： 0.3412878318910135\n",
      " 轮次  1930  当前轮训练集损失 ： 0.34128777050265624\n",
      " 轮次  1931  当前轮训练集损失 ： 0.3412877092686215\n",
      " 轮次  1932  当前轮训练集损失 ： 0.34128764818841756\n",
      " 轮次  1933  当前轮训练集损失 ： 0.34128758726155467\n",
      " 轮次  1934  当前轮训练集损失 ： 0.34128752648754446\n",
      " 轮次  1935  当前轮训练集损失 ： 0.3412874658659005\n",
      " 轮次  1936  当前轮训练集损失 ： 0.34128740539613767\n",
      " 轮次  1937  当前轮训练集损失 ： 0.34128734507777275\n",
      " 轮次  1938  当前轮训练集损失 ： 0.3412872849103241\n",
      " 轮次  1939  当前轮训练集损失 ： 0.3412872248933117\n",
      " 轮次  1940  当前轮训练集损失 ： 0.341287165026257\n",
      " 轮次  1941  当前轮训练集损失 ： 0.3412871053086833\n",
      " 轮次  1942  当前轮训练集损失 ： 0.34128704574011537\n",
      " 轮次  1943  当前轮训练集损失 ： 0.34128698632007987\n",
      " 轮次  1944  当前轮训练集损失 ： 0.34128692704810437\n",
      " 轮次  1945  当前轮训练集损失 ： 0.3412868679237189\n",
      " 轮次  1946  当前轮训练集损失 ： 0.34128680894645447\n",
      " 轮次  1947  当前轮训练集损失 ： 0.341286750115844\n",
      " 轮次  1948  当前轮训练集损失 ： 0.3412866914314216\n",
      " 轮次  1949  当前轮训练集损失 ： 0.3412866328927235\n",
      " 轮次  1950  当前轮训练集损失 ： 0.3412865744992871\n",
      " 轮次  1951  当前轮训练集损失 ： 0.34128651625065154\n",
      " 轮次  1952  当前轮训练集损失 ： 0.34128645814635733\n",
      " 轮次  1953  当前轮训练集损失 ： 0.34128640018594664\n",
      " 轮次  1954  当前轮训练集损失 ： 0.3412863423689633\n",
      " 轮次  1955  当前轮训练集损失 ： 0.34128628469495254\n",
      " 轮次  1956  当前轮训练集损失 ： 0.34128622716346113\n",
      " 轮次  1957  当前轮训练集损失 ： 0.3412861697740374\n",
      " 轮次  1958  当前轮训练集损失 ： 0.34128611252623114\n",
      " 轮次  1959  当前轮训练集损失 ： 0.3412860554195938\n",
      " 轮次  1960  当前轮训练集损失 ： 0.3412859984536783\n",
      " 轮次  1961  当前轮训练集损失 ： 0.341285941628039\n",
      " 轮次  1962  当前轮训练集损失 ： 0.3412858849422316\n",
      " 轮次  1963  当前轮训练集损失 ： 0.3412858283958138\n",
      " 轮次  1964  当前轮训练集损失 ： 0.3412857719883441\n",
      " 轮次  1965  当前轮训练集损失 ： 0.3412857157193831\n",
      " 轮次  1966  当前轮训练集损失 ： 0.3412856595884926\n",
      " 轮次  1967  当前轮训练集损失 ： 0.3412856035952359\n",
      " 轮次  1968  当前轮训练集损失 ： 0.34128554773917785\n",
      " 轮次  1969  当前轮训练集损失 ： 0.3412854920198845\n",
      " 轮次  1970  当前轮训练集损失 ： 0.3412854364369238\n",
      " 轮次  1971  当前轮训练集损失 ： 0.3412853809898648\n",
      " 轮次  1972  当前轮训练集损失 ： 0.3412853256782781\n",
      " 轮次  1973  当前轮训练集损失 ： 0.3412852705017357\n",
      " 轮次  1974  当前轮训练集损失 ： 0.34128521545981116\n",
      " 轮次  1975  当前轮训练集损失 ： 0.34128516055207947\n",
      " 轮次  1976  当前轮训练集损失 ： 0.34128510577811677\n",
      " 轮次  1977  当前轮训练集损失 ： 0.3412850511375011\n",
      " 轮次  1978  当前轮训练集损失 ： 0.34128499662981154\n",
      " 轮次  1979  当前轮训练集损失 ： 0.34128494225462863\n",
      " 轮次  1980  当前轮训练集损失 ： 0.34128488801153445\n",
      " 轮次  1981  当前轮训练集损失 ： 0.34128483390011255\n",
      " 轮次  1982  当前轮训练集损失 ： 0.3412847799199475\n",
      " 轮次  1983  当前轮训练集损失 ： 0.34128472607062577\n",
      " 轮次  1984  当前轮训练集损失 ： 0.3412846723517347\n",
      " 轮次  1985  当前轮训练集损失 ： 0.3412846187628635\n",
      " 轮次  1986  当前轮训练集损失 ： 0.3412845653036024\n",
      " 轮次  1987  当前轮训练集损失 ： 0.34128451197354326\n",
      " 轮次  1988  当前轮训练集损失 ： 0.34128445877227903\n",
      " 轮次  1989  当前轮训练集损失 ： 0.3412844056994044\n",
      " 轮次  1990  当前轮训练集损失 ： 0.3412843527545149\n",
      " 轮次  1991  当前轮训练集损失 ： 0.3412842999372081\n",
      " 轮次  1992  当前轮训练集损失 ： 0.3412842472470822\n",
      " 轮次  1993  当前轮训练集损失 ： 0.34128419468373733\n",
      " 轮次  1994  当前轮训练集损失 ： 0.34128414224677456\n",
      " 轮次  1995  当前轮训练集损失 ： 0.34128408993579645\n",
      " 轮次  1996  当前轮训练集损失 ： 0.341284037750407\n",
      " 轮次  1997  当前轮训练集损失 ： 0.3412839856902113\n",
      " 轮次  1998  当前轮训练集损失 ： 0.34128393375481597\n",
      " 轮次  1999  当前轮训练集损失 ： 0.34128388194382886\n",
      " 轮次  2000  当前轮训练集损失 ： 0.34128383025685916\n",
      " 轮次  2001  当前轮训练集损失 ： 0.34128377869351734\n",
      " 轮次  2002  当前轮训练集损失 ： 0.3412837272534151\n",
      " 轮次  2003  当前轮训练集损失 ： 0.34128367593616554\n",
      " 轮次  2004  当前轮训练集损失 ： 0.3412836247413832\n",
      " 轮次  2005  当前轮训练集损失 ： 0.34128357366868345\n",
      " 轮次  2006  当前轮训练集损失 ： 0.34128352271768353\n",
      " 轮次  2007  当前轮训练集损失 ： 0.3412834718880014\n",
      " 轮次  2008  当前轮训练集损失 ： 0.34128342117925675\n",
      " 轮次  2009  当前轮训练集损失 ： 0.34128337059107033\n",
      " 轮次  2010  当前轮训练集损失 ： 0.34128332012306417\n",
      " 轮次  2011  当前轮训练集损失 ： 0.3412832697748616\n",
      " 轮次  2012  当前轮训练集损失 ： 0.341283219546087\n",
      " 轮次  2013  当前轮训练集损失 ： 0.3412831694363665\n",
      " 轮次  2014  当前轮训练集损失 ： 0.3412831194453268\n",
      " 轮次  2015  当前轮训练集损失 ： 0.34128306957259663\n",
      " 轮次  2016  当前轮训练集损失 ： 0.3412830198178052\n",
      " 轮次  2017  当前轮训练集损失 ： 0.34128297018058334\n",
      " 轮次  2018  当前轮训练集损失 ： 0.34128292066056326\n",
      " 轮次  2019  当前轮训练集损失 ： 0.34128287125737805\n",
      " 轮次  2020  当前轮训练集损失 ： 0.34128282197066223\n",
      " 轮次  2021  当前轮训练集损失 ： 0.34128277280005154\n",
      " 轮次  2022  当前轮训练集损失 ： 0.34128272374518265\n",
      " 轮次  2023  当前轮训练集损失 ： 0.3412826748056941\n",
      " 轮次  2024  当前轮训练集损失 ： 0.3412826259812247\n",
      " 轮次  2025  当前轮训练集损失 ： 0.34128257727141537\n",
      " 轮次  2026  当前轮训练集损失 ： 0.34128252867590775\n",
      " 轮次  2027  当前轮训练集损失 ： 0.3412824801943446\n",
      " 轮次  2028  当前轮训练集损失 ： 0.3412824318263701\n",
      " 轮次  2029  当前轮训练集损失 ： 0.34128238357162965\n",
      " 轮次  2030  当前轮训练集损失 ： 0.3412823354297696\n",
      " 轮次  2031  当前轮训练集损失 ： 0.34128228740043764\n",
      " 轮次  2032  当前轮训练集损失 ： 0.3412822394832825\n",
      " 轮次  2033  当前轮训练集损失 ： 0.34128219167795426\n",
      " 轮次  2034  当前轮训练集损失 ： 0.3412821439841042\n",
      " 轮次  2035  当前轮训练集损失 ： 0.34128209640138435\n",
      " 轮次  2036  当前轮训练集损失 ： 0.3412820489294483\n",
      " 轮次  2037  当前轮训练集损失 ： 0.34128200156795085\n",
      " 轮次  2038  当前轮训练集损失 ： 0.3412819543165475\n",
      " 轮次  2039  当前轮训练集损失 ： 0.3412819071748953\n",
      " 轮次  2040  当前轮训练集损失 ： 0.34128186014265216\n",
      " 轮次  2041  当前轮训练集损失 ： 0.34128181321947754\n",
      " 轮次  2042  当前轮训练集损失 ： 0.34128176640503155\n",
      " 轮次  2043  当前轮训练集损失 ： 0.3412817196989758\n",
      " 轮次  2044  当前轮训练集损失 ： 0.34128167310097274\n",
      " 轮次  2045  当前轮训练集损失 ： 0.3412816266106861\n",
      " 轮次  2046  当前轮训练集损失 ： 0.3412815802277808\n",
      " 轮次  2047  当前轮训练集损失 ： 0.3412815339519225\n",
      " 轮次  2048  当前轮训练集损失 ： 0.3412814877827785\n",
      " 轮次  2049  当前轮训练集损失 ： 0.3412814417200169\n",
      " 轮次  2050  当前轮训练集损失 ： 0.34128139576330685\n",
      " 轮次  2051  当前轮训练集损失 ： 0.3412813499123187\n",
      " 轮次  2052  当前轮训练集损失 ： 0.341281304166724\n",
      " 轮次  2053  当前轮训练集损失 ： 0.3412812585261952\n",
      " 轮次  2054  当前轮训练集损失 ： 0.3412812129904058\n",
      " 轮次  2055  当前轮训练集损失 ： 0.34128116755903076\n",
      " 轮次  2056  当前轮训练集损失 ： 0.34128112223174556\n",
      " 轮次  2057  当前轮训练集损失 ： 0.3412810770082272\n",
      " 轮次  2058  当前轮训练集损失 ： 0.3412810318881537\n",
      " 轮次  2059  当前轮训练集损失 ： 0.3412809868712038\n",
      " 轮次  2060  当前轮训练集损失 ： 0.34128094195705766\n",
      " 轮次  2061  当前轮训练集损失 ： 0.34128089714539644\n",
      " 轮次  2062  当前轮训练集损失 ： 0.34128085243590217\n",
      " 轮次  2063  当前轮训练集损失 ： 0.34128080782825837\n",
      " 轮次  2064  当前轮训练集损失 ： 0.34128076332214896\n",
      " 轮次  2065  当前轮训练集损失 ： 0.34128071891725925\n",
      " 轮次  2066  当前轮训练集损失 ： 0.34128067461327594\n",
      " 轮次  2067  当前轮训练集损失 ： 0.34128063040988604\n",
      " 轮次  2068  当前轮训练集损失 ： 0.3412805863067782\n",
      " 轮次  2069  当前轮训练集损失 ： 0.34128054230364185\n",
      " 轮次  2070  当前轮训练集损失 ： 0.34128049840016744\n",
      " 轮次  2071  当前轮训练集损失 ： 0.34128045459604645\n",
      " 轮次  2072  当前轮训练集损失 ： 0.3412804108909713\n",
      " 轮次  2073  当前轮训练集损失 ： 0.3412803672846358\n",
      " 轮次  2074  当前轮训练集损失 ： 0.3412803237767343\n",
      " 轮次  2075  当前轮训练集损失 ： 0.3412802803669625\n",
      " 轮次  2076  当前轮训练集损失 ： 0.34128023705501687\n",
      " 轮次  2077  当前轮训练集损失 ： 0.34128019384059494\n",
      " 轮次  2078  当前轮训练集损失 ： 0.34128015072339546\n",
      " 轮次  2079  当前轮训练集损失 ： 0.34128010770311773\n",
      " 轮次  2080  当前轮训练集损失 ： 0.3412800647794626\n",
      " 轮次  2081  当前轮训练集损失 ： 0.3412800219521314\n",
      " 轮次  2082  当前轮训练集损失 ： 0.34127997922082687\n",
      " 轮次  2083  当前轮训练集损失 ： 0.34127993658525224\n",
      " 轮次  2084  当前轮训练集损失 ： 0.3412798940451123\n",
      " 轮次  2085  当前轮训练集损失 ： 0.34127985160011237\n",
      " 轮次  2086  当前轮训练集损失 ： 0.3412798092499589\n",
      " 轮次  2087  当前轮训练集损失 ： 0.34127976699435925\n",
      " 轮次  2088  当前轮训练集损失 ： 0.3412797248330219\n",
      " 轮次  2089  当前轮训练集损失 ： 0.3412796827656561\n",
      " 轮次  2090  当前轮训练集损失 ： 0.3412796407919722\n",
      " 轮次  2091  当前轮训练集损失 ： 0.34127959891168147\n",
      " 轮次  2092  当前轮训练集损失 ： 0.341279557124496\n",
      " 轮次  2093  当前轮训练集损失 ： 0.3412795154301289\n",
      " 轮次  2094  当前轮训练集损失 ： 0.3412794738282945\n",
      " 轮次  2095  当前轮训练集损失 ： 0.3412794323187078\n",
      " 轮次  2096  当前轮训练集损失 ： 0.3412793909010845\n",
      " 轮次  2097  当前轮训练集损失 ： 0.3412793495751417\n",
      " 轮次  2098  当前轮训练集损失 ： 0.3412793083405973\n",
      " 轮次  2099  当前轮训练集损失 ： 0.34127926719716994\n",
      " 轮次  2100  当前轮训练集损失 ： 0.34127922614457945\n",
      " 轮次  2101  当前轮训练集损失 ： 0.34127918518254635\n",
      " 轮次  2102  当前轮训练集损失 ： 0.3412791443107921\n",
      " 轮次  2103  当前轮训练集损失 ： 0.3412791035290393\n",
      " 轮次  2104  当前轮训练集损失 ： 0.3412790628370113\n",
      " 轮次  2105  当前轮训练集损失 ： 0.34127902223443235\n",
      " 轮次  2106  当前轮训练集损失 ： 0.3412789817210276\n",
      " 轮次  2107  当前轮训练集损失 ： 0.34127894129652314\n",
      " 轮次  2108  当前轮训练集损失 ： 0.341278900960646\n",
      " 轮次  2109  当前轮训练集损失 ： 0.3412788607131241\n",
      " 轮次  2110  当前轮训练集损失 ： 0.341278820553686\n",
      " 轮次  2111  当前轮训练集损失 ： 0.34127878048206167\n",
      " 轮次  2112  当前轮训练集损失 ： 0.3412787404979815\n",
      " 轮次  2113  当前轮训练集损失 ： 0.341278700601177\n",
      " 轮次  2114  当前轮训练集损失 ： 0.34127866079138036\n",
      " 轮次  2115  当前轮训练集损失 ： 0.34127862106832496\n",
      " 轮次  2116  当前轮训练集损失 ： 0.3412785814317448\n",
      " 轮次  2117  当前轮训练集损失 ： 0.3412785418813748\n",
      " 轮次  2118  当前轮训练集损失 ： 0.34127850241695085\n",
      " 轮次  2119  当前轮训练集损失 ： 0.3412784630382095\n",
      " 轮次  2120  当前轮训练集损失 ： 0.34127842374488854\n",
      " 轮次  2121  当前轮训练集损失 ： 0.34127838453672615\n",
      " 轮次  2122  当前轮训练集损失 ： 0.3412783454134618\n",
      " 轮次  2123  当前轮训练集损失 ： 0.3412783063748354\n",
      " 轮次  2124  当前轮训练集损失 ： 0.3412782674205882\n",
      " 轮次  2125  当前轮训练集损失 ： 0.3412782285504617\n",
      " 轮次  2126  当前轮训练集损失 ： 0.34127818976419894\n",
      " 轮次  2127  当前轮训练集损失 ： 0.341278151061543\n",
      " 轮次  2128  当前轮训练集损失 ： 0.34127811244223866\n",
      " 轮次  2129  当前轮训练集损失 ： 0.34127807390603104\n",
      " 轮次  2130  当前轮训练集损失 ： 0.341278035452666\n",
      " 轮次  2131  当前轮训练集损失 ： 0.3412779970818903\n",
      " 轮次  2132  当前轮训练集损失 ： 0.341277958793452\n",
      " 轮次  2133  当前轮训练集损失 ： 0.3412779205870994\n",
      " 轮次  2134  当前轮训练集损失 ： 0.3412778824625818\n",
      " 轮次  2135  当前轮训练集损失 ： 0.3412778444196494\n",
      " 轮次  2136  当前轮训练集损失 ： 0.34127780645805333\n",
      " 轮次  2137  当前轮训练集损失 ： 0.3412777685775452\n",
      " 轮次  2138  当前轮训练集损失 ： 0.3412777307778777\n",
      " 轮次  2139  当前轮训练集损失 ： 0.34127769305880423\n",
      " 轮次  2140  当前轮训练集损失 ： 0.341277655420079\n",
      " 轮次  2141  当前轮训练集损失 ： 0.34127761786145694\n",
      " 轮次  2142  当前轮训练集损失 ： 0.341277580382694\n",
      " 轮次  2143  当前轮训练集损失 ： 0.3412775429835469\n",
      " 轮次  2144  当前轮训练集损失 ： 0.3412775056637728\n",
      " 轮次  2145  当前轮训练集损失 ： 0.34127746842313017\n",
      " 轮次  2146  当前轮训练集损失 ： 0.34127743126137783\n",
      " 轮次  2147  当前轮训练集损失 ： 0.34127739417827563\n",
      " 轮次  2148  当前轮训练集损失 ： 0.34127735717358426\n",
      " 轮次  2149  当前轮训练集损失 ： 0.3412773202470648\n",
      " 轮次  2150  当前轮训练集损失 ： 0.34127728339847957\n",
      " 轮次  2151  当前轮训练集损失 ： 0.3412772466275916\n",
      " 轮次  2152  当前轮训练集损失 ： 0.3412772099341642\n",
      " 轮次  2153  当前轮训练集损失 ： 0.34127717331796226\n",
      " 轮次  2154  当前轮训练集损失 ： 0.3412771367787508\n",
      " 轮次  2155  当前轮训练集损失 ： 0.3412771003162959\n",
      " 轮次  2156  当前轮训练集损失 ： 0.341277063930364\n",
      " 轮次  2157  当前轮训练集损失 ： 0.34127702762072304\n",
      " 轮次  2158  当前轮训练集损失 ： 0.34127699138714107\n",
      " 轮次  2159  当前轮训练集损失 ： 0.34127695522938717\n",
      " 轮次  2160  当前轮训练集损失 ： 0.34127691914723113\n",
      " 轮次  2161  当前轮训练集损失 ： 0.34127688314044347\n",
      " 轮次  2162  当前轮训练集损失 ： 0.3412768472087955\n",
      " 轮次  2163  当前轮训练集损失 ： 0.34127681135205906\n",
      " 轮次  2164  当前轮训练集损失 ： 0.34127677557000724\n",
      " 轮次  2165  当前轮训练集损失 ： 0.3412767398624134\n",
      " 轮次  2166  当前轮训练集损失 ： 0.34127670422905176\n",
      " 轮次  2167  当前轮训练集损失 ： 0.34127666866969736\n",
      " 轮次  2168  当前轮训练集损失 ： 0.3412766331841258\n",
      " 轮次  2169  当前轮训练集损失 ： 0.3412765977721137\n",
      " 轮次  2170  当前轮训练集损失 ： 0.3412765624334381\n",
      " 轮次  2171  当前轮训练集损失 ： 0.3412765271678769\n",
      " 轮次  2172  当前轮训练集损失 ： 0.3412764919752088\n",
      " 轮次  2173  当前轮训练集损失 ： 0.3412764568552131\n",
      " 轮次  2174  当前轮训练集损失 ： 0.3412764218076699\n",
      " 轮次  2175  当前轮训练集损失 ： 0.34127638683236\n",
      " 轮次  2176  当前轮训练集损失 ： 0.34127635192906475\n",
      " 轮次  2177  当前轮训练集损失 ： 0.34127631709756645\n",
      " 轮次  2178  当前轮训练集损失 ： 0.341276282337648\n",
      " 轮次  2179  当前轮训练集损失 ： 0.3412762476490929\n",
      " 轮次  2180  当前轮训练集损失 ： 0.3412762130316857\n",
      " 轮次  2181  当前轮训练集损失 ： 0.3412761784852113\n",
      " 轮次  2182  当前轮训练集损失 ： 0.34127614400945533\n",
      " 轮次  2183  当前轮训练集损失 ： 0.3412761096042043\n",
      " 轮次  2184  当前轮训练集损失 ： 0.3412760752692453\n",
      " 轮次  2185  当前轮训练集损失 ： 0.3412760410043661\n",
      " 轮次  2186  当前轮训练集损失 ： 0.34127600680935527\n",
      " 轮次  2187  当前轮训练集损失 ： 0.3412759726840019\n",
      " 轮次  2188  当前轮训练集损失 ： 0.34127593862809585\n",
      " 轮次  2189  当前轮训练集损失 ： 0.34127590464142776\n",
      " 轮次  2190  当前轮训练集损失 ： 0.3412758707237887\n",
      " 轮次  2191  当前轮训练集损失 ： 0.3412758368749706\n",
      " 轮次  2192  当前轮训练集损失 ： 0.34127580309476613\n",
      " 轮次  2193  当前轮训练集损失 ： 0.34127576938296844\n",
      " 轮次  2194  当前轮训练集损失 ： 0.3412757357393716\n",
      " 轮次  2195  当前轮训练集损失 ： 0.3412757021637699\n",
      " 轮次  2196  当前轮训练集损失 ： 0.34127566865595893\n",
      " 轮次  2197  当前轮训练集损失 ： 0.3412756352157343\n",
      " 轮次  2198  当前轮训练集损失 ： 0.34127560184289274\n",
      " 轮次  2199  当前轮训练集损失 ： 0.3412755685372316\n",
      " 轮次  2200  当前轮训练集损失 ： 0.34127553529854854\n",
      " 轮次  2201  当前轮训练集损失 ： 0.3412755021266422\n",
      " 轮次  2202  当前轮训练集损失 ： 0.3412754690213119\n",
      " 轮次  2203  当前轮训练集损失 ： 0.34127543598235727\n",
      " 轮次  2204  当前轮训练集损失 ： 0.341275403009579\n",
      " 轮次  2205  当前轮训练集损失 ： 0.3412753701027781\n",
      " 轮次  2206  当前轮训练集损失 ： 0.34127533726175646\n",
      " 轮次  2207  当前轮训练集损失 ： 0.3412753044863164\n",
      " 轮次  2208  当前轮训练集损失 ： 0.34127527177626094\n",
      " 轮次  2209  当前轮训练集损失 ： 0.341275239131394\n",
      " 轮次  2210  当前轮训练集损失 ： 0.3412752065515199\n",
      " 轮次  2211  当前轮训练集损失 ： 0.3412751740364434\n",
      " 轮次  2212  当前轮训练集损失 ： 0.34127514158597033\n",
      " 轮次  2213  当前轮训练集损失 ： 0.3412751091999068\n",
      " 轮次  2214  当前轮训练集损失 ： 0.34127507687805964\n",
      " 轮次  2215  当前轮训练集损失 ： 0.34127504462023645\n",
      " 轮次  2216  当前轮训练集损失 ： 0.34127501242624525\n",
      " 轮次  2217  当前轮训练集损失 ： 0.3412749802958948\n",
      " 轮次  2218  当前轮训练集损失 ： 0.34127494822899446\n",
      " 轮次  2219  当前轮训练集损失 ： 0.34127491622535416\n",
      " 轮次  2220  当前轮训练集损失 ： 0.3412748842847844\n",
      " 轮次  2221  当前轮训练集损失 ： 0.34127485240709654\n",
      " 轮次  2222  当前轮训练集损失 ： 0.34127482059210235\n",
      " 轮次  2223  当前轮训练集损失 ： 0.34127478883961415\n",
      " 轮次  2224  当前轮训练集损失 ： 0.3412747571494449\n",
      " 轮次  2225  当前轮训练集损失 ： 0.34127472552140825\n",
      " 轮次  2226  当前轮训练集损失 ： 0.34127469395531856\n",
      " 轮次  2227  当前轮训练集损失 ： 0.34127466245099053\n",
      " 轮次  2228  当前轮训练集损失 ： 0.34127463100823946\n",
      " 轮次  2229  当前轮训练集损失 ： 0.34127459962688156\n",
      " 轮次  2230  当前轮训练集损失 ： 0.34127456830673336\n",
      " 轮次  2231  当前轮训练集损失 ： 0.3412745370476121\n",
      " 轮次  2232  当前轮训练集损失 ： 0.3412745058493354\n",
      " 轮次  2233  当前轮训练集损失 ： 0.34127447471172173\n",
      " 轮次  2234  当前轮训练集损失 ： 0.3412744436345902\n",
      " 轮次  2235  当前轮训练集损失 ： 0.34127441261776015\n",
      " 轮次  2236  当前轮训练集损失 ： 0.3412743816610516\n",
      " 轮次  2237  当前轮训练集损失 ： 0.34127435076428553\n",
      " 轮次  2238  当前轮训练集损失 ： 0.34127431992728313\n",
      " 轮次  2239  当前轮训练集损失 ： 0.3412742891498662\n",
      " 轮次  2240  当前轮训练集损失 ： 0.3412742584318571\n",
      " 轮次  2241  当前轮训练集损失 ： 0.341274227773079\n",
      " 轮次  2242  当前轮训练集损失 ： 0.34127419717335533\n",
      " 轮次  2243  当前轮训练集损失 ： 0.3412741666325102\n",
      " 轮次  2244  当前轮训练集损失 ： 0.3412741361503685\n",
      " 轮次  2245  当前轮训练集损失 ： 0.34127410572675543\n",
      " 轮次  2246  当前轮训练集损失 ： 0.3412740753614966\n",
      " 轮次  2247  当前轮训练集损失 ： 0.3412740450544187\n",
      " 轮次  2248  当前轮训练集损失 ： 0.34127401480534847\n",
      " 轮次  2249  当前轮训练集损失 ： 0.3412739846141136\n",
      " 轮次  2250  当前轮训练集损失 ： 0.341273954480542\n",
      " 轮次  2251  当前轮训练集损失 ： 0.3412739244044624\n",
      " 轮次  2252  当前轮训练集损失 ： 0.3412738943857039\n",
      " 轮次  2253  当前轮训练集损失 ： 0.3412738644240962\n",
      " 轮次  2254  当前轮训练集损失 ： 0.34127383451946947\n",
      " 轮次  2255  当前轮训练集损失 ： 0.3412738046716547\n",
      " 轮次  2256  当前轮训练集损失 ： 0.34127377488048316\n",
      " 轮次  2257  当前轮训练集损失 ： 0.3412737451457868\n",
      " 轮次  2258  当前轮训练集损失 ： 0.34127371546739793\n",
      " 轮次  2259  当前轮训练集损失 ： 0.34127368584514967\n",
      " 轮次  2260  当前轮训练集损失 ： 0.34127365627887535\n",
      " 轮次  2261  当前轮训练集损失 ： 0.34127362676840906\n",
      " 轮次  2262  当前轮训练集损失 ： 0.34127359731358536\n",
      " 轮次  2263  当前轮训练集损失 ： 0.34127356791423963\n",
      " 轮次  2264  当前轮训练集损失 ： 0.3412735385702072\n",
      " 轮次  2265  当前轮训练集损失 ： 0.34127350928132416\n",
      " 轮次  2266  当前轮训练集损失 ： 0.3412734800474275\n",
      " 轮次  2267  当前轮训练集损失 ： 0.3412734508683542\n",
      " 轮次  2268  当前轮训练集损失 ： 0.3412734217439422\n",
      " 轮次  2269  当前轮训练集损失 ： 0.34127339267402956\n",
      " 轮次  2270  当前轮训练集损失 ： 0.34127336365845506\n",
      " 轮次  2271  当前轮训练集损失 ： 0.3412733346970581\n",
      " 轮次  2272  当前轮训练集损失 ： 0.34127330578967846\n",
      " 轮次  2273  当前轮训练集损失 ： 0.34127327693615644\n",
      " 轮次  2274  当前轮训练集损失 ： 0.3412732481363328\n",
      " 轮次  2275  当前轮训练集损失 ： 0.3412732193900489\n",
      " 轮次  2276  当前轮训练集损失 ： 0.34127319069714673\n",
      " 轮次  2277  当前轮训练集损失 ： 0.3412731620574684\n",
      " 轮次  2278  当前轮训练集损失 ： 0.341273133470857\n",
      " 轮次  2279  当前轮训练集损失 ： 0.3412731049371558\n",
      " 轮次  2280  当前轮训练集损失 ： 0.34127307645620863\n",
      " 轮次  2281  当前轮训练集损失 ： 0.34127304802785996\n",
      " 轮次  2282  当前轮训练集损失 ： 0.34127301965195456\n",
      " 轮次  2283  当前轮训练集损失 ： 0.3412729913283377\n",
      " 轮次  2284  当前轮训练集损失 ： 0.34127296305685545\n",
      " 轮次  2285  当前轮训练集损失 ： 0.34127293483735405\n",
      " 轮次  2286  当前轮训练集损失 ： 0.3412729066696802\n",
      " 轮次  2287  当前轮训练集损失 ： 0.34127287855368155\n",
      " 轮次  2288  当前轮训练集损失 ： 0.3412728504892056\n",
      " 轮次  2289  当前轮训练集损失 ： 0.34127282247610075\n",
      " 轮次  2290  当前轮训练集损失 ： 0.34127279451421594\n",
      " 轮次  2291  当前轮训练集损失 ： 0.3412727666034003\n",
      " 轮次  2292  当前轮训练集损失 ： 0.3412727387435035\n",
      " 轮次  2293  当前轮训练集损失 ： 0.341272710934376\n",
      " 轮次  2294  当前轮训练集损失 ： 0.3412726831758683\n",
      " 轮次  2295  当前轮训练集损失 ： 0.34127265546783164\n",
      " 轮次  2296  当前轮训练集损失 ： 0.34127262781011786\n",
      " 轮次  2297  当前轮训练集损失 ： 0.34127260020257866\n",
      " 轮次  2298  当前轮训练集损失 ： 0.3412725726450673\n",
      " 轮次  2299  当前轮训练集损失 ： 0.3412725451374363\n",
      " 轮次  2300  当前轮训练集损失 ： 0.3412725176795393\n",
      " 轮次  2301  当前轮训练集损失 ： 0.3412724902712307\n",
      " 轮次  2302  当前轮训练集损失 ： 0.3412724629123645\n",
      " 轮次  2303  当前轮训练集损失 ： 0.34127243560279596\n",
      " 轮次  2304  当前轮训练集损失 ： 0.34127240834238043\n",
      " 轮次  2305  当前轮训练集损失 ： 0.3412723811309736\n",
      " 轮次  2306  当前轮训练集损失 ： 0.3412723539684321\n",
      " 轮次  2307  当前轮训练集损失 ： 0.34127232685461245\n",
      " 轮次  2308  当前轮训练集损失 ： 0.34127229978937196\n",
      " 轮次  2309  当前轮训练集损失 ： 0.3412722727725685\n",
      " 轮次  2310  当前轮训练集损失 ： 0.34127224580405996\n",
      " 轮次  2311  当前轮训练集损失 ： 0.3412722188837051\n",
      " 轮次  2312  当前轮训练集损失 ： 0.341272192011363\n",
      " 轮次  2313  当前轮训练集损失 ： 0.3412721651868932\n",
      " 轮次  2314  当前轮训练集损失 ： 0.34127213841015547\n",
      " 轮次  2315  当前轮训练集损失 ： 0.3412721116810104\n",
      " 轮次  2316  当前轮训练集损失 ： 0.34127208499931855\n",
      " 轮次  2317  当前轮训练集损失 ： 0.34127205836494157\n",
      " 轮次  2318  当前轮训练集损失 ： 0.341272031777741\n",
      " 轮次  2319  当前轮训练集损失 ： 0.3412720052375791\n",
      " 轮次  2320  当前轮训练集损失 ： 0.34127197874431836\n",
      " 轮次  2321  当前轮训练集损失 ： 0.34127195229782187\n",
      " 轮次  2322  当前轮训练集损失 ： 0.34127192589795313\n",
      " 轮次  2323  当前轮训练集损失 ： 0.341271899544576\n",
      " 轮次  2324  当前轮训练集损失 ： 0.3412718732375548\n",
      " 轮次  2325  当前轮训练集损失 ： 0.3412718469767546\n",
      " 轮次  2326  当前轮训练集损失 ： 0.3412718207620402\n",
      " 轮次  2327  当前轮训练集损失 ： 0.34127179459327756\n",
      " 轮次  2328  当前轮训练集损失 ： 0.3412717684703324\n",
      " 轮次  2329  当前轮训练集损失 ： 0.3412717423930717\n",
      " 轮次  2330  当前轮训练集损失 ： 0.3412717163613619\n",
      " 轮次  2331  当前轮训练集损失 ： 0.3412716903750706\n",
      " 轮次  2332  当前轮训练集损失 ： 0.34127166443406554\n",
      " 轮次  2333  当前轮训练集损失 ： 0.34127163853821474\n",
      " 轮次  2334  当前轮训练集损失 ： 0.34127161268738704\n",
      " 轮次  2335  当前轮训练集损失 ： 0.3412715868814513\n",
      " 轮次  2336  当前轮训练集损失 ： 0.34127156112027696\n",
      " 轮次  2337  当前轮训练集损失 ： 0.34127153540373384\n",
      " 轮次  2338  当前轮训练集损失 ： 0.34127150973169235\n",
      " 轮次  2339  当前轮训练集损失 ： 0.34127148410402297\n",
      " 轮次  2340  当前轮训练集损失 ： 0.3412714585205969\n",
      " 轮次  2341  当前轮训练集损失 ： 0.3412714329812856\n",
      " 轮次  2342  当前轮训练集损失 ： 0.341271407485961\n",
      " 轮次  2343  当前轮训练集损失 ： 0.3412713820344953\n",
      " 轮次  2344  当前轮训练集损失 ： 0.3412713566267614\n",
      " 轮次  2345  当前轮训练集损失 ： 0.34127133126263226\n",
      " 轮次  2346  当前轮训练集损失 ： 0.3412713059419814\n",
      " 轮次  2347  当前轮训练集损失 ： 0.3412712806646828\n",
      " 轮次  2348  当前轮训练集损失 ： 0.34127125543061076\n",
      " 轮次  2349  当前轮训练集损失 ： 0.3412712302396399\n",
      " 轮次  2350  当前轮训练集损失 ： 0.34127120509164555\n",
      " 轮次  2351  当前轮训练集损失 ： 0.34127117998650297\n",
      " 轮次  2352  当前轮训练集损失 ： 0.34127115492408816\n",
      " 轮次  2353  当前轮训练集损失 ： 0.34127112990427755\n",
      " 轮次  2354  当前轮训练集损失 ： 0.3412711049269477\n",
      " 轮次  2355  当前轮训练集损失 ： 0.3412710799919756\n",
      " 轮次  2356  当前轮训练集损失 ： 0.3412710550992389\n",
      " 轮次  2357  当前轮训练集损失 ： 0.3412710302486153\n",
      " 轮次  2358  当前轮训练集损失 ： 0.3412710054399832\n",
      " 轮次  2359  当前轮训练集损失 ： 0.3412709806732211\n",
      " 轮次  2360  当前轮训练集损失 ： 0.34127095594820794\n",
      " 轮次  2361  当前轮训练集损失 ： 0.34127093126482333\n",
      " 轮次  2362  当前轮训练集损失 ： 0.34127090662294696\n",
      " 轮次  2363  当前轮训练集损失 ： 0.3412708820224589\n",
      " 轮次  2364  当前轮训练集损失 ： 0.3412708574632398\n",
      " 轮次  2365  当前轮训练集损失 ： 0.34127083294517047\n",
      " 轮次  2366  当前轮训练集损失 ： 0.34127080846813224\n",
      " 轮次  2367  当前轮训练集损失 ： 0.34127078403200684\n",
      " 轮次  2368  当前轮训练集损失 ： 0.34127075963667625\n",
      " 轮次  2369  当前轮训练集损失 ： 0.34127073528202273\n",
      " 轮次  2370  当前轮训练集损失 ： 0.3412707109679294\n",
      " 轮次  2371  当前轮训练集损失 ： 0.3412706866942791\n",
      " 轮次  2372  当前轮训练集损失 ： 0.34127066246095566\n",
      " 轮次  2373  当前轮训练集损失 ： 0.3412706382678426\n",
      " 轮次  2374  当前轮训练集损失 ： 0.34127061411482434\n",
      " 轮次  2375  当前轮训练集损失 ： 0.3412705900017857\n",
      " 轮次  2376  当前轮训练集损失 ： 0.34127056592861144\n",
      " 轮次  2377  当前轮训练集损失 ： 0.3412705418951869\n",
      " 轮次  2378  当前轮训练集损失 ： 0.3412705179013978\n",
      " 轮次  2379  当前轮训练集损失 ： 0.3412704939471304\n",
      " 轮次  2380  当前轮训练集损失 ： 0.34127047003227096\n",
      " 轮次  2381  当前轮训练集损失 ： 0.3412704461567064\n",
      " 轮次  2382  当前轮训练集损失 ： 0.3412704223203235\n",
      " 轮次  2383  当前轮训练集损失 ： 0.34127039852301033\n",
      " 轮次  2384  当前轮训练集损失 ： 0.3412703747646542\n",
      " 轮次  2385  当前轮训练集损失 ： 0.3412703510451437\n",
      " 轮次  2386  当前轮训练集损失 ： 0.34127032736436724\n",
      " 轮次  2387  当前轮训练集损失 ： 0.34127030372221384\n",
      " 轮次  2388  当前轮训练集损失 ： 0.3412702801185726\n",
      " 轮次  2389  当前轮训练集损失 ： 0.34127025655333326\n",
      " 轮次  2390  当前轮训练集损失 ： 0.34127023302638576\n",
      " 轮次  2391  当前轮训练集损失 ： 0.3412702095376204\n",
      " 轮次  2392  当前轮训练集损失 ： 0.3412701860869277\n",
      " 轮次  2393  当前轮训练集损失 ： 0.3412701626741989\n",
      " 轮次  2394  当前轮训练集损失 ： 0.3412701392993252\n",
      " 轮次  2395  当前轮训练集损失 ： 0.3412701159621982\n",
      " 轮次  2396  当前轮训练集损失 ： 0.3412700926627103\n",
      " 轮次  2397  当前轮训练集损失 ： 0.3412700694007534\n",
      " 轮次  2398  当前轮训练集损失 ： 0.34127004617622037\n",
      " 轮次  2399  当前轮训练集损失 ： 0.34127002298900433\n",
      " 轮次  2400  当前轮训练集损失 ： 0.3412699998389986\n",
      " 轮次  2401  当前轮训练集损失 ： 0.3412699767260968\n",
      " 轮次  2402  当前轮训练集损失 ： 0.3412699536501932\n",
      " 轮次  2403  当前轮训练集损失 ： 0.34126993061118205\n",
      " 轮次  2404  当前轮训练集损失 ： 0.3412699076089579\n",
      " 轮次  2405  当前轮训练集损失 ： 0.3412698846434161\n",
      " 轮次  2406  当前轮训练集损失 ： 0.34126986171445195\n",
      " 轮次  2407  当前轮训练集损失 ： 0.341269838821961\n",
      " 轮次  2408  当前轮训练集损失 ： 0.34126981596583933\n",
      " 轮次  2409  当前轮训练集损失 ： 0.3412697931459834\n",
      " 轮次  2410  当前轮训练集损失 ： 0.3412697703622898\n",
      " 轮次  2411  当前轮训练集损失 ： 0.3412697476146555\n",
      " 轮次  2412  当前轮训练集损失 ： 0.3412697249029779\n",
      " 轮次  2413  当前轮训练集损失 ： 0.34126970222715475\n",
      " 轮次  2414  当前轮训练集损失 ： 0.34126967958708376\n",
      " 轮次  2415  当前轮训练集损失 ： 0.3412696569826635\n",
      " 轮次  2416  当前轮训练集损失 ： 0.3412696344137923\n",
      " 轮次  2417  当前轮训练集损失 ： 0.3412696118803694\n",
      " 轮次  2418  当前轮训练集损失 ： 0.34126958938229385\n",
      " 轮次  2419  当前轮训练集损失 ： 0.3412695669194653\n",
      " 轮次  2420  当前轮训练集损失 ： 0.34126954449178354\n",
      " 轮次  2421  当前轮训练集损失 ： 0.3412695220991488\n",
      " 轮次  2422  当前轮训练集损失 ： 0.3412694997414617\n",
      " 轮次  2423  当前轮训练集损失 ： 0.34126947741862285\n",
      " 轮次  2424  当前轮训练集损失 ： 0.34126945513053353\n",
      " 轮次  2425  当前轮训练集损失 ： 0.34126943287709516\n",
      " 轮次  2426  当前轮训练集损失 ： 0.3412694106582095\n",
      " 轮次  2427  当前轮训练集损失 ： 0.3412693884737785\n",
      " 轮次  2428  当前轮训练集损失 ： 0.34126936632370464\n",
      " 轮次  2429  当前轮训练集损失 ： 0.3412693442078905\n",
      " 轮次  2430  当前轮训练集损失 ： 0.34126932212623917\n",
      " 轮次  2431  当前轮训练集损失 ： 0.3412693000786538\n",
      " 轮次  2432  当前轮训练集损失 ： 0.34126927806503793\n",
      " 轮次  2433  当前轮训练集损失 ： 0.3412692560852958\n",
      " 轮次  2434  当前轮训练集损失 ： 0.34126923413933125\n",
      " 轮次  2435  当前轮训练集损失 ： 0.34126921222704876\n",
      " 轮次  2436  当前轮训练集损失 ： 0.34126919034835324\n",
      " 轮次  2437  当前轮训练集损失 ： 0.34126916850314976\n",
      " 轮次  2438  当前轮训练集损失 ： 0.3412691466913436\n",
      " 轮次  2439  当前轮训练集损失 ： 0.3412691249128405\n",
      " 轮次  2440  当前轮训练集损失 ： 0.34126910316754655\n",
      " 轮次  2441  当前轮训练集损失 ： 0.3412690814553678\n",
      " 轮次  2442  当前轮训练集损失 ： 0.34126905977621097\n",
      " 轮次  2443  当前轮训练集损失 ： 0.3412690381299828\n",
      " 轮次  2444  当前轮训练集损失 ： 0.3412690165165904\n",
      " 轮次  2445  当前轮训练集损失 ： 0.3412689949359413\n",
      " 轮次  2446  当前轮训练集损失 ： 0.34126897338794326\n",
      " 轮次  2447  当前轮训练集损失 ： 0.3412689518725041\n",
      " 轮次  2448  当前轮训练集损失 ： 0.34126893038953227\n",
      " 轮次  2449  当前轮训练集损失 ： 0.3412689089389363\n",
      " 轮次  2450  当前轮训练集损失 ： 0.34126888752062506\n",
      " 轮次  2451  当前轮训练集损失 ： 0.34126886613450785\n",
      " 轮次  2452  当前轮训练集损失 ： 0.3412688447804938\n",
      " 轮次  2453  当前轮训练集损失 ： 0.3412688234584928\n",
      " 轮次  2454  当前轮训练集损失 ： 0.3412688021684149\n",
      " 轮次  2455  当前轮训练集损失 ： 0.3412687809101702\n",
      " 轮次  2456  当前轮训练集损失 ： 0.34126875968366943\n",
      " 轮次  2457  当前轮训练集损失 ： 0.34126873848882344\n",
      " 轮次  2458  当前轮训练集损失 ： 0.34126871732554326\n",
      " 轮次  2459  当前轮训练集损失 ： 0.3412686961937402\n",
      " 轮次  2460  当前轮训练集损失 ： 0.34126867509332615\n",
      " 轮次  2461  当前轮训练集损失 ： 0.3412686540242129\n",
      " 轮次  2462  当前轮训练集损失 ： 0.34126863298631266\n",
      " 轮次  2463  当前轮训练集损失 ： 0.34126861197953806\n",
      " 轮次  2464  当前轮训练集损失 ： 0.34126859100380175\n",
      " 轮次  2465  当前轮训练集损失 ： 0.3412685700590169\n",
      " 轮次  2466  当前轮训练集损失 ： 0.3412685491450966\n",
      " 轮次  2467  当前轮训练集损失 ： 0.3412685282619546\n",
      " 轮次  2468  当前轮训练集损失 ： 0.34126850740950465\n",
      " 轮次  2469  当前轮训练集损失 ： 0.3412684865876611\n",
      " 轮次  2470  当前轮训练集损失 ： 0.34126846579633796\n",
      " 轮次  2471  当前轮训练集损失 ： 0.3412684450354502\n",
      " 轮次  2472  当前轮训练集损失 ： 0.34126842430491255\n",
      " 轮次  2473  当前轮训练集损失 ： 0.3412684036046403\n",
      " 轮次  2474  当前轮训练集损失 ： 0.3412683829345488\n",
      " 轮次  2475  当前轮训练集损失 ： 0.34126836229455376\n",
      " 轮次  2476  当前轮训练集损失 ： 0.3412683416845715\n",
      " 轮次  2477  当前轮训练集损失 ： 0.34126832110451766\n",
      " 轮次  2478  当前轮训练集损失 ： 0.3412683005543092\n",
      " 轮次  2479  当前轮训练集损失 ： 0.34126828003386267\n",
      " 轮次  2480  当前轮训练集损失 ： 0.34126825954309503\n",
      " 轮次  2481  当前轮训练集损失 ： 0.34126823908192383\n",
      " 轮次  2482  当前轮训练集损失 ： 0.34126821865026624\n",
      " 轮次  2483  当前轮训练集损失 ： 0.3412681982480402\n",
      " 轮次  2484  当前轮训练集损失 ： 0.3412681778751639\n",
      " 轮次  2485  当前轮训练集损失 ： 0.3412681575315554\n",
      " 轮次  2486  当前轮训练集损失 ： 0.34126813721713345\n",
      " 轮次  2487  当前轮训练集损失 ： 0.3412681169318167\n",
      " 轮次  2488  当前轮训练集损失 ： 0.3412680966755241\n",
      " 轮次  2489  当前轮训练集损失 ： 0.34126807644817536\n",
      " 轮次  2490  当前轮训练集损失 ： 0.3412680562496896\n",
      " 轮次  2491  当前轮训练集损失 ： 0.3412680360799869\n",
      " 轮次  2492  当前轮训练集损失 ： 0.34126801593898715\n",
      " 轮次  2493  当前轮训练集损失 ： 0.34126799582661077\n",
      " 轮次  2494  当前轮训练集损失 ： 0.3412679757427782\n",
      " 轮次  2495  当前轮训练集损失 ： 0.34126795568741036\n",
      " 轮次  2496  当前轮训练集损失 ： 0.34126793566042823\n",
      " 轮次  2497  当前轮训练集损失 ： 0.341267915661753\n",
      " 轮次  2498  当前轮训练集损失 ： 0.3412678956913065\n",
      " 轮次  2499  当前轮训练集损失 ： 0.34126787574901024\n",
      " 轮次  2500  当前轮训练集损失 ： 0.34126785583478614\n",
      " 轮次  2501  当前轮训练集损失 ： 0.3412678359485568\n",
      " 轮次  2502  当前轮训练集损失 ： 0.3412678160902444\n",
      " 轮次  2503  当前轮训练集损失 ： 0.341267796259772\n",
      " 轮次  2504  当前轮训练集损失 ： 0.34126777645706224\n",
      " 轮次  2505  当前轮训练集损失 ： 0.3412677566820385\n",
      " 轮次  2506  当前轮训练集损失 ： 0.3412677369346243\n",
      " 轮次  2507  当前轮训练集损失 ： 0.34126771721474325\n",
      " 轮次  2508  当前轮训练集损失 ： 0.3412676975223193\n",
      " 轮次  2509  当前轮训练集损失 ： 0.3412676778572766\n",
      " 轮次  2510  当前轮训练集损失 ： 0.34126765821953947\n",
      " 轮次  2511  当前轮训练集损失 ： 0.3412676386090327\n",
      " 轮次  2512  当前轮训练集损失 ： 0.3412676190256812\n",
      " 轮次  2513  当前轮训练集损失 ： 0.3412675994694099\n",
      " 轮次  2514  当前轮训练集损失 ： 0.3412675799401441\n",
      " 轮次  2515  当前轮训练集损失 ： 0.3412675604378095\n",
      " 轮次  2516  当前轮训练集损失 ： 0.34126754096233186\n",
      " 轮次  2517  当前轮训练集损失 ： 0.34126752151363715\n",
      " 轮次  2518  当前轮训练集损失 ： 0.3412675020916517\n",
      " 轮次  2519  当前轮训练集损失 ： 0.3412674826963019\n",
      " 轮次  2520  当前轮训练集损失 ： 0.3412674633275146\n",
      " 轮次  2521  当前轮训练集损失 ： 0.3412674439852165\n",
      " 轮次  2522  当前轮训练集损失 ： 0.34126742466933496\n",
      " 轮次  2523  当前轮训练集损失 ： 0.3412674053797974\n",
      " 轮次  2524  当前轮训练集损失 ： 0.3412673861165312\n",
      " 轮次  2525  当前轮训练集损失 ： 0.3412673668794643\n",
      " 轮次  2526  当前轮训练集损失 ： 0.3412673476685248\n",
      " 轮次  2527  当前轮训练集损失 ： 0.34126732848364094\n",
      " 轮次  2528  当前轮训练集损失 ： 0.3412673093247413\n",
      " 轮次  2529  当前轮训练集损失 ： 0.34126729019175456\n",
      " 轮次  2530  当前轮训练集损失 ： 0.3412672710846096\n",
      " 轮次  2531  当前轮训练集损失 ： 0.34126725200323543\n",
      " 轮次  2532  当前轮训练集损失 ： 0.3412672329475617\n",
      " 轮次  2533  当前轮训练集损失 ： 0.341267213917518\n",
      " 轮次  2534  当前轮训练集损失 ： 0.341267194913034\n",
      " 轮次  2535  当前轮训练集损失 ： 0.34126717593403977\n",
      " 轮次  2536  当前轮训练集损失 ： 0.34126715698046556\n",
      " 轮次  2537  当前轮训练集损失 ： 0.34126713805224174\n",
      " 轮次  2538  当前轮训练集损失 ： 0.3412671191492993\n",
      " 轮次  2539  当前轮训练集损失 ： 0.34126710027156876\n",
      " 轮次  2540  当前轮训练集损失 ： 0.3412670814189814\n",
      " 轮次  2541  当前轮训练集损失 ： 0.34126706259146855\n",
      " 轮次  2542  当前轮训练集损失 ： 0.3412670437889616\n",
      " 轮次  2543  当前轮训练集损失 ： 0.3412670250113924\n",
      " 轮次  2544  当前轮训练集损失 ： 0.34126700625869283\n",
      " 轮次  2545  当前轮训练集损失 ： 0.3412669875307951\n",
      " 轮次  2546  当前轮训练集损失 ： 0.34126696882763163\n",
      " 轮次  2547  当前轮训练集损失 ： 0.3412669501491349\n",
      " 轮次  2548  当前轮训练集损失 ： 0.3412669314952376\n",
      " 轮次  2549  当前轮训练集损失 ： 0.3412669128658731\n",
      " 轮次  2550  当前轮训练集损失 ： 0.34126689426097395\n",
      " 轮次  2551  当前轮训练集损失 ： 0.34126687568047426\n",
      " 轮次  2552  当前轮训练集损失 ： 0.3412668571243072\n",
      " 轮次  2553  当前轮训练集损失 ： 0.3412668385924066\n",
      " 轮次  2554  当前轮训练集损失 ： 0.34126682008470655\n",
      " 轮次  2555  当前轮训练集损失 ： 0.34126680160114137\n",
      " 轮次  2556  当前轮训练集损失 ： 0.3412667831416453\n",
      " 轮次  2557  当前轮训练集损失 ： 0.3412667647061531\n",
      " 轮次  2558  当前轮训练集损失 ： 0.3412667462945995\n",
      " 轮次  2559  当前轮训练集损失 ： 0.34126672790691953\n",
      " 轮次  2560  当前轮训练集损失 ： 0.34126670954304844\n",
      " 轮次  2561  当前轮训练集损失 ： 0.3412666912029216\n",
      " 轮次  2562  当前轮训练集损失 ： 0.3412666728864747\n",
      " 轮次  2563  当前轮训练集损失 ： 0.3412666545936435\n",
      " 轮次  2564  当前轮训练集损失 ： 0.3412666363243641\n",
      " 轮次  2565  当前轮训练集损失 ： 0.34126661807857256\n",
      " 轮次  2566  当前轮训练集损失 ： 0.34126659985620544\n",
      " 轮次  2567  当前轮训练集损失 ： 0.3412665816571992\n",
      " 轮次  2568  当前轮训练集损失 ： 0.34126656348149076\n",
      " 轮次  2569  当前轮训练集损失 ： 0.34126654532901696\n",
      " 轮次  2570  当前轮训练集损失 ： 0.3412665271997152\n",
      " 轮次  2571  当前轮训练集损失 ： 0.34126650909352274\n",
      " 轮次  2572  当前轮训练集损失 ： 0.341266491010377\n",
      " 轮次  2573  当前轮训练集损失 ： 0.34126647295021595\n",
      " 轮次  2574  当前轮训练集损失 ： 0.3412664549129775\n",
      " 轮次  2575  当前轮训练集损失 ： 0.34126643689859976\n",
      " 轮次  2576  当前轮训练集损失 ： 0.34126641890702114\n",
      " 轮次  2577  当前轮训练集损失 ： 0.34126640093817995\n",
      " 轮次  2578  当前轮训练集损失 ： 0.341266382992015\n",
      " 轮次  2579  当前轮训练集损失 ： 0.3412663650684653\n",
      " 轮次  2580  当前轮训练集损失 ： 0.34126634716746995\n",
      " 轮次  2581  当前轮训练集损失 ： 0.34126632928896794\n",
      " 轮次  2582  当前轮训练集损失 ： 0.34126631143289893\n",
      " 轮次  2583  当前轮训练集损失 ： 0.3412662935992026\n",
      " 轮次  2584  当前轮训练集损失 ： 0.3412662757878186\n",
      " 轮次  2585  当前轮训练集损失 ： 0.34126625799868715\n",
      " 轮次  2586  当前轮训练集损失 ： 0.34126624023174834\n",
      " 轮次  2587  当前轮训练集损失 ： 0.3412662224869425\n",
      " 轮次  2588  当前轮训练集损失 ： 0.34126620476421027\n",
      " 轮次  2589  当前轮训练集损失 ： 0.3412661870634923\n",
      " 轮次  2590  当前轮训练集损失 ： 0.34126616938472965\n",
      " 轮次  2591  当前轮训练集损失 ： 0.34126615172786334\n",
      " 轮次  2592  当前轮训练集损失 ： 0.34126613409283474\n",
      " 轮次  2593  当前轮训练集损失 ： 0.341266116479585\n",
      " 轮次  2594  当前轮训练集损失 ： 0.34126609888805615\n",
      " 轮次  2595  当前轮训练集损失 ： 0.3412660813181898\n",
      " 轮次  2596  当前轮训练集损失 ： 0.3412660637699281\n",
      " 轮次  2597  当前轮训练集损失 ： 0.34126604624321305\n",
      " 轮次  2598  当前轮训练集损失 ： 0.34126602873798706\n",
      " 轮次  2599  当前轮训练集损失 ： 0.3412660112541927\n",
      " 轮次  2600  当前轮训练集损失 ： 0.3412659937917727\n",
      " 轮次  2601  当前轮训练集损失 ： 0.3412659763506698\n",
      " 轮次  2602  当前轮训练集损失 ： 0.3412659589308272\n",
      " 轮次  2603  当前轮训练集损失 ： 0.34126594153218814\n",
      " 轮次  2604  当前轮训练集损失 ： 0.3412659241546958\n",
      " 轮次  2605  当前轮训练集损失 ： 0.341265906798294\n",
      " 轮次  2606  当前轮训练集损失 ： 0.3412658894629265\n",
      " 轮次  2607  当前轮训练集损失 ： 0.34126587214853693\n",
      " 轮次  2608  当前轮训练集损失 ： 0.3412658548550697\n",
      " 轮次  2609  当前轮训练集损失 ： 0.34126583758246887\n",
      " 轮次  2610  当前轮训练集损失 ： 0.3412658203306789\n",
      " 轮次  2611  当前轮训练集损失 ： 0.34126580309964444\n",
      " 轮次  2612  当前轮训练集损失 ： 0.3412657858893103\n",
      " 轮次  2613  当前轮训练集损失 ： 0.3412657686996213\n",
      " 轮次  2614  当前轮训练集损失 ： 0.34126575153052263\n",
      " 轮次  2615  当前轮训练集损失 ： 0.34126573438195945\n",
      " 轮次  2616  当前轮训练集损失 ： 0.34126571725387744\n",
      " 轮次  2617  当前轮训练集损失 ： 0.3412657001462218\n",
      " 轮次  2618  当前轮训练集损失 ： 0.34126568305893873\n",
      " 轮次  2619  当前轮训练集损失 ： 0.34126566599197394\n",
      " 轮次  2620  当前轮训练集损失 ： 0.34126564894527356\n",
      " 轮次  2621  当前轮训练集损失 ： 0.34126563191878384\n",
      " 轮次  2622  当前轮训练集损失 ： 0.34126561491245133\n",
      " 轮次  2623  当前轮训练集损失 ： 0.34126559792622235\n",
      " 轮次  2624  当前轮训练集损失 ： 0.341265580960044\n",
      " 轮次  2625  当前轮训练集损失 ： 0.34126556401386293\n",
      " 轮次  2626  当前轮训练集损失 ： 0.34126554708762646\n",
      " 轮次  2627  当前轮训练集损失 ： 0.34126553018128153\n",
      " 轮次  2628  当前轮训练集损失 ： 0.3412655132947757\n",
      " 轮次  2629  当前轮训练集损失 ： 0.3412654964280565\n",
      " 轮次  2630  当前轮训练集损失 ： 0.3412654795810716\n",
      " 轮次  2631  当前轮训练集损失 ： 0.34126546275376907\n",
      " 轮次  2632  当前轮训练集损失 ： 0.34126544594609687\n",
      " 轮次  2633  当前轮训练集损失 ： 0.34126542915800306\n",
      " 轮次  2634  当前轮训练集损失 ： 0.34126541238943614\n",
      " 轮次  2635  当前轮训练集损失 ： 0.3412653956403445\n",
      " 轮次  2636  当前轮训练集损失 ： 0.3412653789106768\n",
      " 轮次  2637  当前轮训练集损失 ： 0.341265362200382\n",
      " 轮次  2638  当前轮训练集损失 ： 0.341265345509409\n",
      " 轮次  2639  当前轮训练集损失 ： 0.341265328837707\n",
      " 轮次  2640  当前轮训练集损失 ： 0.34126531218522504\n",
      " 轮次  2641  当前轮训练集损失 ： 0.3412652955519129\n",
      " 轮次  2642  当前轮训练集损失 ： 0.34126527893771996\n",
      " 轮次  2643  当前轮训练集损失 ： 0.3412652623425959\n",
      " 轮次  2644  当前轮训练集损失 ： 0.341265245766491\n",
      " 轮次  2645  当前轮训练集损失 ： 0.3412652292093547\n",
      " 轮次  2646  当前轮训练集损失 ： 0.34126521267113774\n",
      " 轮次  2647  当前轮训练集损失 ： 0.3412651961517903\n",
      " 轮次  2648  当前轮训练集损失 ： 0.34126517965126285\n",
      " 轮次  2649  当前轮训练集损失 ： 0.34126516316950606\n",
      " 轮次  2650  当前轮训练集损失 ： 0.34126514670647073\n",
      " 轮次  2651  当前轮训练集损失 ： 0.3412651302621078\n",
      " 轮次  2652  当前轮训练集损失 ： 0.3412651138363684\n",
      " 轮次  2653  当前轮训练集损失 ： 0.34126509742920375\n",
      " 轮次  2654  当前轮训练集损失 ： 0.3412650810405653\n",
      " 轮次  2655  当前轮训练集损失 ： 0.34126506467040457\n",
      " 轮次  2656  当前轮训练集损失 ： 0.3412650483186732\n",
      " 轮次  2657  当前轮训练集损失 ： 0.3412650319853231\n",
      " 轮次  2658  当前轮训练集损失 ： 0.3412650156703062\n",
      " 轮次  2659  当前轮训练集损失 ： 0.34126499937357485\n",
      " 轮次  2660  当前轮训练集损失 ： 0.3412649830950808\n",
      " 轮次  2661  当前轮训练集损失 ： 0.34126496683477703\n",
      " 轮次  2662  当前轮训练集损失 ： 0.3412649505926158\n",
      " 轮次  2663  当前轮训练集损失 ： 0.34126493436854993\n",
      " 轮次  2664  当前轮训练集损失 ： 0.3412649181625322\n",
      " 轮次  2665  当前轮训练集损失 ： 0.34126490197451564\n",
      " 轮次  2666  当前轮训练集损失 ： 0.34126488580445336\n",
      " 轮次  2667  当前轮训练集损失 ： 0.34126486965229863\n",
      " 轮次  2668  当前轮训练集损失 ： 0.341264853518005\n",
      " 轮次  2669  当前轮训练集损失 ： 0.34126483740152574\n",
      " 轮次  2670  当前轮训练集损失 ： 0.3412648213028149\n",
      " 轮次  2671  当前轮训练集损失 ： 0.341264805221826\n",
      " 轮次  2672  当前轮训练集损失 ： 0.3412647891585132\n",
      " 轮次  2673  当前轮训练集损失 ： 0.3412647731128307\n",
      " 轮次  2674  当前轮训练集损失 ： 0.34126475708473264\n",
      " 轮次  2675  当前轮训练集损失 ： 0.3412647410741732\n",
      " 轮次  2676  当前轮训练集损失 ： 0.34126472508110733\n",
      " 轮次  2677  当前轮训练集损失 ： 0.34126470910548945\n",
      " 轮次  2678  当前轮训练集损失 ： 0.3412646931472743\n",
      " 轮次  2679  当前轮训练集损失 ： 0.34126467720641696\n",
      " 轮次  2680  当前轮训练集损失 ： 0.34126466128287264\n",
      " 轮次  2681  当前轮训练集损失 ： 0.34126464537659623\n",
      " 轮次  2682  当前轮训练集损失 ： 0.34126462948754327\n",
      " 轮次  2683  当前轮训练集损失 ： 0.34126461361566923\n",
      " 轮次  2684  当前轮训练集损失 ： 0.34126459776092977\n",
      " 轮次  2685  当前轮训练集损失 ： 0.3412645819232805\n",
      " 轮次  2686  当前轮训练集损失 ： 0.34126456610267747\n",
      " 轮次  2687  当前轮训练集损失 ： 0.34126455029907665\n",
      " 轮次  2688  当前轮训练集损失 ： 0.3412645345124341\n",
      " 轮次  2689  当前轮训练集损失 ： 0.34126451874270614\n",
      " 轮次  2690  当前轮训练集损失 ： 0.3412645029898492\n",
      " 轮次  2691  当前轮训练集损失 ： 0.34126448725381986\n",
      " 轮次  2692  当前轮训练集损失 ： 0.3412644715345749\n",
      " 轮次  2693  当前轮训练集损失 ： 0.3412644558320708\n",
      " 轮次  2694  当前轮训练集损失 ： 0.34126444014626484\n",
      " 轮次  2695  当前轮训练集损失 ： 0.34126442447711386\n",
      " 轮次  2696  当前轮训练集损失 ： 0.3412644088245752\n",
      " 轮次  2697  当前轮训练集损失 ： 0.3412643931886061\n",
      " 轮次  2698  当前轮训练集损失 ： 0.34126437756916406\n",
      " 轮次  2699  当前轮训练集损失 ： 0.3412643619662067\n",
      " 轮次  2700  当前轮训练集损失 ： 0.34126434637969155\n",
      " 轮次  2701  当前轮训练集损失 ： 0.34126433080957663\n",
      " 轮次  2702  当前轮训练集损失 ： 0.3412643152558198\n",
      " 轮次  2703  当前轮训练集损失 ： 0.34126429971837935\n",
      " 轮次  2704  当前轮训练集损失 ： 0.3412642841972133\n",
      " 轮次  2705  当前轮训练集损失 ： 0.34126426869227994\n",
      " 轮次  2706  当前轮训练集损失 ： 0.3412642532035378\n",
      " 轮次  2707  当前轮训练集损失 ： 0.3412642377309457\n",
      " 轮次  2708  当前轮训练集损失 ： 0.341264222274462\n",
      " 轮次  2709  当前轮训练集损失 ： 0.34126420683404574\n",
      " 轮次  2710  当前轮训练集损失 ： 0.3412641914096559\n",
      " 轮次  2711  当前轮训练集损失 ： 0.3412641760012514\n",
      " 轮次  2712  当前轮训练集损失 ： 0.34126416060879167\n",
      " 轮次  2713  当前轮训练集损失 ： 0.34126414523223586\n",
      " 轮次  2714  当前轮训练集损失 ： 0.3412641298715436\n",
      " 轮次  2715  当前轮训练集损失 ： 0.3412641145266744\n",
      " 轮次  2716  当前轮训练集损失 ： 0.3412640991975877\n",
      " 轮次  2717  当前轮训练集损失 ： 0.34126408388424373\n",
      " 轮次  2718  当前轮训练集损失 ： 0.34126406858660213\n",
      " 轮次  2719  当前轮训练集损失 ： 0.34126405330462317\n",
      " 轮次  2720  当前轮训练集损失 ： 0.341264038038267\n",
      " 轮次  2721  当前轮训练集损失 ： 0.34126402278749374\n",
      " 轮次  2722  当前轮训练集损失 ： 0.3412640075522639\n",
      " 轮次  2723  当前轮训练集损失 ： 0.34126399233253807\n",
      " 轮次  2724  当前轮训练集损失 ： 0.34126397712827694\n",
      " 轮次  2725  当前轮训练集损失 ： 0.3412639619394411\n",
      " 轮次  2726  当前轮训练集损失 ： 0.3412639467659916\n",
      " 轮次  2727  当前轮训练集损失 ： 0.3412639316078895\n",
      " 轮次  2728  当前轮训练集损失 ： 0.3412639164650957\n",
      " 轮次  2729  当前轮训练集损失 ： 0.3412639013375717\n",
      " 轮次  2730  当前轮训练集损失 ： 0.3412638862252786\n",
      " 轮次  2731  当前轮训练集损失 ： 0.34126387112817796\n",
      " 轮次  2732  当前轮训练集损失 ： 0.3412638560462315\n",
      " 轮次  2733  当前轮训练集损失 ： 0.34126384097940093\n",
      " 轮次  2734  当前轮训练集损失 ： 0.34126382592764776\n",
      " 轮次  2735  当前轮训练集损失 ： 0.34126381089093427\n",
      " 轮次  2736  当前轮训练集损失 ： 0.34126379586922234\n",
      " 轮次  2737  当前轮训练集损失 ： 0.3412637808624741\n",
      " 轮次  2738  当前轮训练集损失 ： 0.34126376587065194\n",
      " 轮次  2739  当前轮训练集损失 ： 0.3412637508937181\n",
      " 轮次  2740  当前轮训练集损失 ： 0.3412637359316353\n",
      " 轮次  2741  当前轮训练集损失 ： 0.34126372098436586\n",
      " 轮次  2742  当前轮训练集损失 ： 0.3412637060518728\n",
      " 轮次  2743  当前轮训练集损失 ： 0.3412636911341187\n",
      " 轮次  2744  当前轮训练集损失 ： 0.3412636762310665\n",
      " 轮次  2745  当前轮训练集损失 ： 0.34126366134267955\n",
      " 轮次  2746  当前轮训练集损失 ： 0.3412636464689208\n",
      " 轮次  2747  当前轮训练集损失 ： 0.34126363160975354\n",
      " 轮次  2748  当前轮训练集损失 ： 0.34126361676514116\n",
      " 轮次  2749  当前轮训练集损失 ： 0.3412636019350473\n",
      " 轮次  2750  当前轮训练集损失 ： 0.34126358711943516\n",
      " 轮次  2751  当前轮训练集损失 ： 0.34126357231826887\n",
      " 轮次  2752  当前轮训练集损失 ： 0.3412635575315121\n",
      " 轮次  2753  当前轮训练集损失 ： 0.3412635427591288\n",
      " 轮次  2754  当前轮训练集损失 ： 0.34126352800108284\n",
      " 轮次  2755  当前轮训练集损失 ： 0.34126351325733867\n",
      " 轮次  2756  当前轮训练集损失 ： 0.34126349852786036\n",
      " 轮次  2757  当前轮训练集损失 ： 0.34126348381261223\n",
      " 轮次  2758  当前轮训练集损失 ： 0.3412634691115588\n",
      " 轮次  2759  当前轮训练集损失 ： 0.34126345442466466\n",
      " 轮次  2760  当前轮训练集损失 ： 0.34126343975189444\n",
      " 轮次  2761  当前轮训练集损失 ： 0.34126342509321295\n",
      " 轮次  2762  当前轮训练集损失 ： 0.34126341044858505\n",
      " 轮次  2763  当前轮训练集损失 ： 0.3412633958179757\n",
      " 轮次  2764  当前轮训练集损失 ： 0.3412633812013502\n",
      " 轮次  2765  当前轮训练集损失 ： 0.3412633665986735\n",
      " 轮次  2766  当前轮训练集损失 ： 0.3412633520099108\n",
      " 轮次  2767  当前轮训练集损失 ： 0.34126333743502796\n",
      " 轮次  2768  当前轮训练集损失 ： 0.34126332287399\n",
      " 轮次  2769  当前轮训练集损失 ： 0.341263308326763\n",
      " 轮次  2770  当前轮训练集损失 ： 0.3412632937933122\n",
      " 轮次  2771  当前轮训练集损失 ： 0.3412632792736037\n",
      " 轮次  2772  当前轮训练集损失 ： 0.34126326476760344\n",
      " 轮次  2773  当前轮训练集损失 ： 0.34126325027527715\n",
      " 轮次  2774  当前轮训练集损失 ： 0.3412632357965913\n",
      " 轮次  2775  当前轮训练集损失 ： 0.341263221331512\n",
      " 轮次  2776  当前轮训练集损失 ： 0.3412632068800055\n",
      " 轮次  2777  当前轮训练集损失 ： 0.34126319244203807\n",
      " 轮次  2778  当前轮训练集损失 ： 0.34126317801757655\n",
      " 轮次  2779  当前轮训练集损失 ： 0.3412631636065873\n",
      " 轮次  2780  当前轮训练集损失 ： 0.34126314920903733\n",
      " 轮次  2781  当前轮训练集损失 ： 0.34126313482489323\n",
      " 轮次  2782  当前轮训练集损失 ： 0.3412631204541219\n",
      " 轮次  2783  当前轮训练集损失 ： 0.3412631060966904\n",
      " 轮次  2784  当前轮训练集损失 ： 0.3412630917525659\n",
      " 轮次  2785  当前轮训练集损失 ： 0.34126307742171563\n",
      " 轮次  2786  当前轮训练集损失 ： 0.34126306310410687\n",
      " 轮次  2787  当前轮训练集损失 ： 0.34126304879970687\n",
      " 轮次  2788  当前轮训练集损失 ： 0.3412630345084832\n",
      " 轮次  2789  当前轮训练集损失 ： 0.3412630202304036\n",
      " 轮次  2790  当前轮训练集损失 ： 0.34126300596543574\n",
      " 轮次  2791  当前轮训练集损失 ： 0.34126299171354724\n",
      " 轮次  2792  当前轮训练集损失 ： 0.3412629774747061\n",
      " 轮次  2793  当前轮训练集损失 ： 0.3412629632488803\n",
      " 轮次  2794  当前轮训练集损失 ： 0.3412629490360378\n",
      " 轮次  2795  当前轮训练集损失 ： 0.34126293483614706\n",
      " 轮次  2796  当前轮训练集损失 ： 0.3412629206491761\n",
      " 轮次  2797  当前轮训练集损失 ： 0.3412629064750933\n",
      " 轮次  2798  当前轮训练集损失 ： 0.34126289231386703\n",
      " 轮次  2799  当前轮训练集损失 ： 0.3412628781654661\n",
      " 轮次  2800  当前轮训练集损失 ： 0.3412628640298589\n",
      " 轮次  2801  当前轮训练集损失 ： 0.3412628499070142\n",
      " 轮次  2802  当前轮训练集损失 ： 0.341262835796901\n",
      " 轮次  2803  当前轮训练集损失 ： 0.3412628216994881\n",
      " 轮次  2804  当前轮训练集损失 ： 0.34126280761474437\n",
      " 轮次  2805  当前轮训练集损失 ： 0.34126279354263916\n",
      " 轮次  2806  当前轮训练集损失 ： 0.34126277948314143\n",
      " 轮次  2807  当前轮训练集损失 ： 0.3412627654362207\n",
      " 轮次  2808  当前轮训练集损失 ： 0.34126275140184614\n",
      " 轮次  2809  当前轮训练集损失 ： 0.3412627373799873\n",
      " 轮次  2810  当前轮训练集损失 ： 0.34126272337061364\n",
      " 轮次  2811  当前轮训练集损失 ： 0.341262709373695\n",
      " 轮次  2812  当前轮训练集损失 ： 0.3412626953892009\n",
      " 轮次  2813  当前轮训练集损失 ： 0.3412626814171013\n",
      " 轮次  2814  当前轮训练集损失 ： 0.34126266745736616\n",
      " 轮次  2815  当前轮训练集损失 ： 0.34126265350996515\n",
      " 轮次  2816  当前轮训练集损失 ： 0.3412626395748688\n",
      " 轮次  2817  当前轮训练集损失 ： 0.341262625652047\n",
      " 轮次  2818  当前轮训练集损失 ： 0.3412626117414702\n",
      " 轮次  2819  当前轮训练集损失 ： 0.34126259784310853\n",
      " 轮次  2820  当前轮训练集损失 ： 0.34126258395693265\n",
      " 轮次  2821  当前轮训练集损失 ： 0.3412625700829129\n",
      " 轮次  2822  当前轮训练集损失 ： 0.34126255622102003\n",
      " 轮次  2823  当前轮训练集损失 ： 0.3412625423712247\n",
      " 轮次  2824  当前轮训练集损失 ： 0.3412625285334977\n",
      " 轮次  2825  当前轮训练集损失 ： 0.3412625147078099\n",
      " 轮次  2826  当前轮训练集损失 ： 0.34126250089413224\n",
      " 轮次  2827  当前轮训练集损失 ： 0.3412624870924359\n",
      " 轮次  2828  当前轮训练集损失 ： 0.34126247330269177\n",
      " 轮次  2829  当前轮训练集损失 ： 0.3412624595248712\n",
      " 轮次  2830  当前轮训练集损失 ： 0.3412624457589455\n",
      " 轮次  2831  当前轮训练集损失 ： 0.34126243200488604\n",
      " 轮次  2832  当前轮训练集损失 ： 0.34126241826266435\n",
      " 轮次  2833  当前轮训练集损失 ： 0.3412624045322518\n",
      " 轮次  2834  当前轮训练集损失 ： 0.34126239081362025\n",
      " 轮次  2835  当前轮训练集损失 ： 0.34126237710674123\n",
      " 轮次  2836  当前轮训练集损失 ： 0.3412623634115867\n",
      " 轮次  2837  当前轮训练集损失 ： 0.34126234972812847\n",
      " 轮次  2838  当前轮训练集损失 ： 0.34126233605633854\n",
      " 轮次  2839  当前轮训练集损失 ： 0.3412623223961889\n",
      " 轮次  2840  当前轮训练集损失 ： 0.3412623087476518\n",
      " 轮次  2841  当前轮训练集损失 ： 0.3412622951106993\n",
      " 轮次  2842  当前轮训练集损失 ： 0.3412622814853038\n",
      " 轮次  2843  当前轮训练集损失 ： 0.3412622678714377\n",
      " 轮次  2844  当前轮训练集损失 ： 0.34126225426907336\n",
      " 轮次  2845  当前轮训练集损失 ： 0.3412622406781835\n",
      " 轮次  2846  当前轮训练集损失 ： 0.3412622270987405\n",
      " 轮次  2847  当前轮训练集损失 ： 0.34126221353071723\n",
      " 轮次  2848  当前轮训练集损失 ： 0.34126219997408636\n",
      " 轮次  2849  当前轮训练集损失 ： 0.3412621864288209\n",
      " 轮次  2850  当前轮训练集损失 ： 0.3412621728948937\n",
      " 轮次  2851  当前轮训练集损失 ： 0.3412621593722779\n",
      " 轮次  2852  当前轮训练集损失 ： 0.3412621458609463\n",
      " 轮次  2853  当前轮训练集损失 ： 0.3412621323608725\n",
      " 轮次  2854  当前轮训练集损失 ： 0.3412621188720293\n",
      " 轮次  2855  当前轮训练集损失 ： 0.3412621053943904\n",
      " 轮次  2856  当前轮训练集损失 ： 0.34126209192792917\n",
      " 轮次  2857  当前轮训练集损失 ： 0.34126207847261886\n",
      " 轮次  2858  当前轮训练集损失 ： 0.34126206502843326\n",
      " 轮次  2859  当前轮训练集损失 ： 0.341262051595346\n",
      " 轮次  2860  当前轮训练集损失 ： 0.34126203817333073\n",
      " 轮次  2861  当前轮训练集损失 ： 0.34126202476236134\n",
      " 轮次  2862  当前轮训练集损失 ： 0.3412620113624117\n",
      " 轮次  2863  当前轮训练集损失 ： 0.3412619979734556\n",
      " 轮次  2864  当前轮训练集损失 ： 0.34126198459546725\n",
      " 轮次  2865  当前轮训练集损失 ： 0.34126197122842067\n",
      " 轮次  2866  当前轮训练集损失 ： 0.34126195787229013\n",
      " 轮次  2867  当前轮训练集损失 ： 0.34126194452704983\n",
      " 轮次  2868  当前轮训练集损失 ： 0.34126193119267406\n",
      " 轮次  2869  当前轮训练集损失 ： 0.3412619178691374\n",
      " 轮次  2870  当前轮训练集损失 ： 0.34126190455641414\n",
      " 轮次  2871  当前轮训练集损失 ： 0.3412618912544789\n",
      " 轮次  2872  当前轮训练集损失 ： 0.34126187796330637\n",
      " 轮次  2873  当前轮训练集损失 ： 0.34126186468287123\n",
      " 轮次  2874  当前轮训练集损失 ： 0.34126185141314824\n",
      " 轮次  2875  当前轮训练集损失 ： 0.3412618381541124\n",
      " 轮次  2876  当前轮训练集损失 ： 0.34126182490573825\n",
      " 轮次  2877  当前轮训练集损失 ： 0.3412618116680012\n",
      " 轮次  2878  当前轮训练集损失 ： 0.34126179844087623\n",
      " 轮次  2879  当前轮训练集损失 ： 0.3412617852243383\n",
      " 轮次  2880  当前轮训练集损失 ： 0.3412617720183629\n",
      " 轮次  2881  当前轮训练集损失 ： 0.34126175882292514\n",
      " 轮次  2882  当前轮训练集损失 ： 0.34126174563800044\n",
      " 轮次  2883  当前轮训练集损失 ： 0.3412617324635643\n",
      " 轮次  2884  当前轮训练集损失 ： 0.34126171929959204\n",
      " 轮次  2885  当前轮训练集损失 ： 0.3412617061460594\n",
      " 轮次  2886  当前轮训练集损失 ： 0.34126169300294207\n",
      " 轮次  2887  当前轮训练集损失 ： 0.34126167987021566\n",
      " 轮次  2888  当前轮训练集损失 ： 0.34126166674785596\n",
      " 轮次  2889  当前轮训练集损失 ： 0.341261653635839\n",
      " 轮次  2890  当前轮训练集损失 ： 0.34126164053414043\n",
      " 轮次  2891  当前轮训练集损失 ： 0.34126162744273647\n",
      " 轮次  2892  当前轮训练集损失 ： 0.34126161436160324\n",
      " 轮次  2893  当前轮训练集损失 ： 0.3412616012907167\n",
      " 轮次  2894  当前轮训练集损失 ： 0.3412615882300532\n",
      " 轮次  2895  当前轮训练集损失 ： 0.3412615751795889\n",
      " 轮次  2896  当前轮训练集损失 ： 0.34126156213930026\n",
      " 轮次  2897  当前轮训练集损失 ： 0.3412615491091637\n",
      " 轮次  2898  当前轮训练集损失 ： 0.34126153608915555\n",
      " 轮次  2899  当前轮训练集损失 ： 0.34126152307925267\n",
      " 轮次  2900  当前轮训练集损失 ： 0.34126151007943145\n",
      " 轮次  2901  当前轮训练集损失 ： 0.34126149708966863\n",
      " 轮次  2902  当前轮训练集损失 ： 0.34126148410994095\n",
      " 轮次  2903  当前轮训练集损失 ： 0.3412614711402253\n",
      " 轮次  2904  当前轮训练集损失 ： 0.34126145818049863\n",
      " 轮次  2905  当前轮训练集损失 ： 0.34126144523073787\n",
      " 轮次  2906  当前轮训练集损失 ： 0.3412614322909198\n",
      " 轮次  2907  当前轮训练集损失 ： 0.34126141936102206\n",
      " 轮次  2908  当前轮训练集损失 ： 0.34126140644102126\n",
      " 轮次  2909  当前轮训练集损失 ： 0.34126139353089485\n",
      " 轮次  2910  当前轮训练集损失 ： 0.3412613806306203\n",
      " 轮次  2911  当前轮训练集损失 ： 0.3412613677401746\n",
      " 轮次  2912  当前轮训练集损失 ： 0.3412613548595355\n",
      " 轮次  2913  当前轮训练集损失 ： 0.3412613419886804\n",
      " 轮次  2914  当前轮训练集损失 ： 0.34126132912758683\n",
      " 轮次  2915  当前轮训练集损失 ： 0.3412613162762323\n",
      " 轮次  2916  当前轮训练集损失 ： 0.3412613034345948\n",
      " 轮次  2917  当前轮训练集损失 ： 0.34126129060265187\n",
      " 轮次  2918  当前轮训练集损失 ： 0.3412612777803813\n",
      " 轮次  2919  当前轮训练集损失 ： 0.34126126496776105\n",
      " 轮次  2920  当前轮训练集损失 ： 0.34126125216476916\n",
      " 轮次  2921  当前轮训练集损失 ： 0.34126123937138353\n",
      " 轮次  2922  当前轮训练集损失 ： 0.3412612265875822\n",
      " 轮次  2923  当前轮训练集损失 ： 0.3412612138133435\n",
      " 轮次  2924  当前轮训练集损失 ： 0.34126120104864527\n",
      " 轮次  2925  当前轮训练集损失 ： 0.3412611882934661\n",
      " 轮次  2926  当前轮训练集损失 ： 0.34126117554778423\n",
      " 轮次  2927  当前轮训练集损失 ： 0.3412611628115781\n",
      " 轮次  2928  当前轮训练集损失 ： 0.341261150084826\n",
      " 轮次  2929  当前轮训练集损失 ： 0.3412611373675065\n",
      " 轮次  2930  当前轮训练集损失 ： 0.3412611246595984\n",
      " 轮次  2931  当前轮训练集损失 ： 0.3412611119610801\n",
      " 轮次  2932  当前轮训练集损失 ： 0.34126109927193027\n",
      " 轮次  2933  当前轮训练集损失 ： 0.3412610865921279\n",
      " 轮次  2934  当前轮训练集损失 ： 0.3412610739216515\n",
      " 轮次  2935  当前轮训练集损失 ： 0.34126106126048034\n",
      " 轮次  2936  当前轮训练集损失 ： 0.3412610486085931\n",
      " 轮次  2937  当前轮训练集损失 ： 0.3412610359659689\n",
      " 轮次  2938  当前轮训练集损失 ： 0.34126102333258673\n",
      " 轮次  2939  当前轮训练集损失 ： 0.34126101070842574\n",
      " 轮次  2940  当前轮训练集损失 ： 0.3412609980934653\n",
      " 轮次  2941  当前轮训练集损失 ： 0.3412609854876842\n",
      " 轮次  2942  当前轮训练集损失 ： 0.3412609728910622\n",
      " 轮次  2943  当前轮训练集损失 ： 0.3412609603035786\n",
      " 轮次  2944  当前轮训练集损失 ： 0.3412609477252126\n",
      " 轮次  2945  当前轮训练集损失 ： 0.3412609351559439\n",
      " 轮次  2946  当前轮训练集损失 ： 0.3412609225957521\n",
      " 轮次  2947  当前轮训练集损失 ： 0.34126091004461656\n",
      " 轮次  2948  当前轮训练集损失 ： 0.3412608975025171\n",
      " 轮次  2949  当前轮训练集损失 ： 0.3412608849694334\n",
      " 轮次  2950  当前轮训练集损失 ： 0.34126087244534525\n",
      " 轮次  2951  当前轮训练集损失 ： 0.3412608599302326\n",
      " 轮次  2952  当前轮训练集损失 ： 0.34126084742407514\n",
      " 轮次  2953  当前轮训练集损失 ： 0.341260834926853\n",
      " 轮次  2954  当前轮训练集损失 ： 0.34126082243854616\n",
      " 轮次  2955  当前轮训练集损失 ： 0.3412608099591346\n",
      " 轮次  2956  当前轮训练集损失 ： 0.34126079748859856\n",
      " 轮次  2957  当前轮训练集损失 ： 0.3412607850269182\n",
      " 轮次  2958  当前轮训练集损失 ： 0.3412607725740737\n",
      " 轮次  2959  当前轮训练集损失 ： 0.34126076013004547\n",
      " 轮次  2960  当前轮训练集损失 ： 0.34126074769481374\n",
      " 轮次  2961  当前轮训练集损失 ： 0.34126073526835904\n",
      " 轮次  2962  当前轮训练集损失 ： 0.3412607228506617\n",
      " 轮次  2963  当前轮训练集损失 ： 0.3412607104417025\n",
      " 轮次  2964  当前轮训练集损失 ： 0.34126069804146175\n",
      " 轮次  2965  当前轮训练集损失 ： 0.3412606856499202\n",
      " 轮次  2966  当前轮训练集损失 ： 0.34126067326705856\n",
      " 轮次  2967  当前轮训练集损失 ： 0.3412606608928576\n",
      " 轮次  2968  当前轮训练集损失 ： 0.34126064852729804\n",
      " 轮次  2969  当前轮训练集损失 ： 0.34126063617036084\n",
      " 轮次  2970  当前轮训练集损失 ： 0.3412606238220269\n",
      " 轮次  2971  当前轮训练集损失 ： 0.3412606114822771\n",
      " 轮次  2972  当前轮训练集损失 ： 0.3412605991510924\n",
      " 轮次  2973  当前轮训练集损失 ： 0.3412605868284542\n",
      " 轮次  2974  当前轮训练集损失 ： 0.3412605745143434\n",
      " 轮次  2975  当前轮训练集损失 ： 0.3412605622087411\n",
      " 轮次  2976  当前轮训练集损失 ： 0.34126054991162874\n",
      " 轮次  2977  当前轮训练集损失 ： 0.3412605376229875\n",
      " 轮次  2978  当前轮训练集损失 ： 0.34126052534279877\n",
      " 轮次  2979  当前轮训练集损失 ： 0.341260513071044\n",
      " 轮次  2980  当前轮训练集损失 ： 0.34126050080770454\n",
      " 轮次  2981  当前轮训练集损失 ： 0.341260488552762\n",
      " 轮次  2982  当前轮训练集损失 ： 0.3412604763061978\n",
      " 轮次  2983  当前轮训练集损失 ： 0.3412604640679938\n",
      " 轮次  2984  当前轮训练集损失 ： 0.3412604518381315\n",
      " 轮次  2985  当前轮训练集损失 ： 0.3412604396165926\n",
      " 轮次  2986  当前轮训练集损失 ： 0.3412604274033588\n",
      " 轮次  2987  当前轮训练集损失 ： 0.3412604151984122\n",
      " 轮次  2988  当前轮训练集损失 ： 0.34126040300173444\n",
      " 轮次  2989  当前轮训练集损失 ： 0.3412603908133076\n",
      " 轮次  2990  当前轮训练集损失 ： 0.3412603786331134\n",
      " 轮次  2991  当前轮训练集损失 ： 0.3412603664611342\n",
      " 轮次  2992  当前轮训练集损失 ： 0.34126035429735196\n",
      " 轮次  2993  当前轮训练集损失 ： 0.34126034214174866\n",
      " 轮次  2994  当前轮训练集损失 ： 0.3412603299943068\n",
      " 轮次  2995  当前轮训练集损失 ： 0.34126031785500827\n",
      " 轮次  2996  当前轮训练集损失 ： 0.34126030572383564\n",
      " 轮次  2997  当前轮训练集损失 ： 0.34126029360077104\n",
      " 轮次  2998  当前轮训练集损失 ： 0.34126028148579696\n",
      " 轮次  2999  当前轮训练集损失 ： 0.3412602693788959\n",
      " 轮次  3000  当前轮训练集损失 ： 0.34126025728005016\n",
      "训练最终损失: 0.34126025728005016\n",
      "逻辑回归训练的准确率为:  86.78%\n"
     ]
    }
   ],
   "source": [
    "loss_history, weight_history, bias_histoay = \\\n",
    "    logistic_regression(X_train, y_train, weight, bias, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e5456852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:17:36.700991Z",
     "start_time": "2024-11-19T09:17:36.570888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "loss_history_test = np.zeros(iterations) # 训练集有归一化\n",
    "for i in range(iterations):\n",
    "    loss_history_test[i] = loss_function(X_test,y_test,weight_history[i],bias_histoay[i])\n",
    "index = np.arange(0,iterations,1)\n",
    "plt.figure(figsize=(12,6), dpi=100)\n",
    "plt.plot(index, loss_history, c='blue', linestyle='dotted',label='Trainning Loss')\n",
    "plt.plot(index, loss_history_test, c='red', label='Test Loss')\n",
    "\n",
    "plt.xlabel('Number of Iteratrion')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba2abf",
   "metadata": {},
   "source": [
    "### sklean实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "a4c8de3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:27:22.321134Z",
     "start_time": "2024-11-19T09:27:22.310076Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8852459016393442"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "3987e46f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:28:09.954113Z",
     "start_time": "2024-11-19T09:28:09.945073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |\n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |\n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |\n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |\n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
      " |      Specify the norm of the penalty:\n",
      " |\n",
      " |      - `None`: no penalty is added;\n",
      " |      - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      " |      - `'l1'`: add a L1 penalty term;\n",
      " |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      " |\n",
      " |      .. warning::\n",
      " |         Some penalties may not work with some solvers. See the parameter\n",
      " |         `solver` below, to know the compatibility between the penalty and\n",
      " |         solver.\n",
      " |\n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |\n",
      " |  dual : bool, default=False\n",
      " |      Dual (constrained) or primal (regularized, see also\n",
      " |      :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n",
      " |      is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |\n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |\n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |\n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |\n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |\n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |\n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |\n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |\n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |\n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |\n",
      " |  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
      " |\n",
      " |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      " |      To choose a solver, you might want to consider the following aspects:\n",
      " |\n",
      " |          - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      " |            and 'saga' are faster for large ones;\n",
      " |          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      " |            'lbfgs' handle multinomial loss;\n",
      " |          - 'liblinear' is limited to one-versus-rest schemes.\n",
      " |          - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
      " |            especially with one-hot encoded categorical features with rare\n",
      " |            categories. Note that it is limited to binary classification and the\n",
      " |            one-versus-rest reduction for multiclass classification. Be aware that\n",
      " |            the memory usage of this solver has a quadratic dependency on\n",
      " |            `n_features` because it explicitly computes the Hessian matrix.\n",
      " |\n",
      " |      .. warning::\n",
      " |         The choice of the algorithm depends on the penalty chosen.\n",
      " |         Supported penalties by solver:\n",
      " |\n",
      " |         - 'lbfgs'           -   ['l2', None]\n",
      " |         - 'liblinear'       -   ['l1', 'l2']\n",
      " |         - 'newton-cg'       -   ['l2', None]\n",
      " |         - 'newton-cholesky' -   ['l2', None]\n",
      " |         - 'sag'             -   ['l2', None]\n",
      " |         - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
      " |\n",
      " |      .. note::\n",
      " |         'sag' and 'saga' fast convergence is only guaranteed on features\n",
      " |         with approximately the same scale. You can preprocess the data with\n",
      " |         a scaler from :mod:`sklearn.preprocessing`.\n",
      " |\n",
      " |      .. seealso::\n",
      " |         Refer to the User Guide for more information regarding\n",
      " |         :class:`LogisticRegression` and more specifically the\n",
      " |         :ref:`Table <Logistic_regression>`\n",
      " |         summarizing solver/penalty supports.\n",
      " |\n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |      .. versionadded:: 1.2\n",
      " |         newton-cholesky solver.\n",
      " |\n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |\n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |\n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |\n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |\n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |\n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |\n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |\n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |\n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |\n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |\n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |\n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |\n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |\n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |\n",
      " |      .. versionadded:: 0.24\n",
      " |\n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |\n",
      " |      .. versionadded:: 1.0\n",
      " |\n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |\n",
      " |      .. versionchanged:: 0.20\n",
      " |\n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log_loss\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |\n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |\n",
      " |  References\n",
      " |  ----------\n",
      " |\n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |\n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |\n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |\n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |          :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
      " |          for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
      " |\n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |\n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |\n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |\n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |\n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |\n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |\n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |\n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e. calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |\n",
      " |  set_fit_request(self: sklearn.linear_model._logistic.LogisticRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegression from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |\n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      The options for each parameter are:\n",
      " |\n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |\n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |\n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |\n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |\n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |\n",
      " |  set_score_request(self: sklearn.linear_model._logistic.LogisticRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegression from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |\n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      The options for each parameter are:\n",
      " |\n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |\n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |\n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |\n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |\n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |\n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |\n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the confidence scores.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      " |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      " |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      " |          this class would be predicted.\n",
      " |\n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the predictions.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,)\n",
      " |          Vector containing the class labels for each sample.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |\n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |\n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |\n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |\n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |\n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |\n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |\n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |\n",
      " |      The ``intercept_`` member is not converted.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |\n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  __init_subclass__(**kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83ce58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
